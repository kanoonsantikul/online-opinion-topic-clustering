{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus\n",
    "\n",
    "from model.new_sdc import NewSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 350\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TrueCoffee'\n",
    "\n",
    "corpus, labels = load_corpus('../data/' + file_name + '.txt')\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/tokenized/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 948 words\n",
      "filter frequent words: 370 words\n",
      "filter letter words: 368 words\n",
      "filter stop words: 219 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.extend(['นี้'])\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(idx_corpus)]\n",
    "model = Doc2Vec(tagged_corpus, vector_size=average_doc_size, window=4, min_count=2, epochs=100)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "paragraph_vectors = [model.infer_vector(doc) for doc in idx_corpus]\n",
    "paragraph_vectors = pandas.DataFrame(paragraph_vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(corpus, weight):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if weight == 'normal':\n",
    "        weight_corpus = bow_corpus\n",
    "    elif weight == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        weight_corpus = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(weight_corpus):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if weight == 'normal' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(onehot_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(onehot_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(onehot_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.32\n",
    "expand_rate = 0.05\n",
    "\n",
    "onehot_corpus = get_onehot(idx_corpus, 'normal')\n",
    "# onehot_corpus = get_onehot(filtered_corpus, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.1837864203253\n",
      "[ 30  10  30   9  12  12  25  22  10  10  25 149   4   2]\n"
     ]
    }
   ],
   "source": [
    "max_compactness = 0\n",
    "epoch = 15\n",
    "for i in range(epoch):\n",
    "#     model = NewSDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps)\n",
    "    \n",
    "    marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "    model = KMeans(n_clusters=14).fit(onehot_corpus)\n",
    "    _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(onehot_corpus, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "num_cluster = len(label_count)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None \n",
    "prev_label_count = None\n",
    "while True:\n",
    "    model = NewSDC()\n",
    "    predicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(onehot_corpus, result)\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    if numpy.array_equal(label_count, prev_label_count):\n",
    "        break\n",
    "    prev_label_count = label_count\n",
    "    centroids = centroids[1:]\n",
    "    \n",
    "    print(compactness)\n",
    "    print(label_count)\n",
    "num_cluster = len(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "11 1 0.30535198048644013\n",
      "10 9 0.8783020838381587\n",
      "10 3 0.34298458536886484\n",
      "3 2 0.33847700234811184\n",
      "[0, 1, 2, 2, 4, 5, 6, 7, 8, 2, 2, 1, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "result.to_csv('../data/results/k-mean/' + file_name + '.csv')\n",
    "\n",
    "# result = pandas.read_csv('../data/results/k-mean/' + file_name + '.csv')\n",
    "\n",
    "count = 0\n",
    "for index, value in result.iterrows():\n",
    "    if value['marks'] == -1:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe75fef9c50422488221bdaa5117744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 4, 5, 6, 7, 8, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ไม่ ชอบ การ แบ่ง ชนชั้น ในร้านกาแฟ ผู้ถือบัตรอีกแบบ นั่งได้แค่ตรงนี้ ผู้ถือบัตรอีกแบบนั่งตรงนี้ จ่ายราคากาแฟก็เท่ากันไหม\n",
      "57 เป็นร้านที่แดกกาแฟร้อนทีไร แม่งต้องล้วกลิ้นไหม้เป็นแผลตลอด น้ำร้อนเอ็งจะร้อนไปไหนวะ สงสัยแม่งเอามาจากบ่อในนรกชัวร์ๆ ถ้าเอ็งร้อนแบบนี้\n",
      "58 ทานที่มหาวิทยาลัยเป็นมุมกาแฟรสชาติอร่อยมาก กาแฟดีมาก ราคาดีรับได้ แต่ถ้าเป็นร้านในห้างราคา over เกินไป\n",
      "90 กาแฟพอใช้ได้นะ แต่ราคาก็ยังสูงกว่าร้านอื่นๆอยุ่ ลงราคามาหน่อย หรือจัดโทรโมชั่นบ้าง\n",
      "99 ลองเปลี่ยนไปขายอย่างอื่นที่ไม่ใช่กาแฟดีกว่า\n",
      "101 กาแฟแม่งแรงสัสมึงจะดีดไปไหน\n",
      "114 กาแฟจืดสัดๆ เปรี้ยวด้วย ใช้โรบัสต้า ใช่มั้ยมึง\n",
      "128 ต่อไปคงมี ดีแท็ก/เอไอเอส กาแฟ คอยดู\n",
      "131 ไม่เคยแดก\n",
      "136 จ่ายค่าโท...ด้วย...หอมกลิ่นกาแฟ..ด้วย.. ไม่ได้ตังค์นางหร้อก\n",
      "149 เหมือนประชด เคยได้โปรฟรีเป็นกาแฟร้อนตอนหน้าร้อน\n",
      "152 นี่กาแฟหรือน้ำล้าง..(มือ)\n",
      "154 กาแฟไม่อร่อยเลย ราคาถูกยังไงก็ไม่คุ้ม\n",
      "159 อีนี่ก็แอบแพงไป..แดกค่าเนตไม่พอ ยังมากดราคากาแฟอีก 😡\n",
      "169 ไม่เคยแดก\n",
      "202 ถ้ามีส่วนลด กาแฟก็โอเคอยู่นะ\n",
      "215 เลิกใช้กาแฟเพื่อน มาใช้กาแฟคุณภาพได้แล้วครับ\n",
      "229 เรื่องกาแฟ\n",
      "247 ขอกาแฟสดเถอะขอร้องง\n",
      "261 นี่กาแฟหรือน้ำเปล่า?..🤔\n",
      "271 กาแฟที่สาขาสยามดิส ผ่าน\n",
      "273 เหนื่อยกาแฟ รสชาติ เหมือนของเหลือ\n",
      "278 จืดอ่า ทรูเนตกากประหยัดกาแฟ คอนเซป\n",
      "285 กาแฟไม่ได้เรื่องเลย\n",
      "288 นี่กาแฟหรือน้ำล้างตีน..\n",
      "290 แพง กาแฟจืด\n",
      "295 กาแฟก็จืด\n",
      "297 กาแฟแรงมาก\n",
      "326 กาแฟเหมือนกินน้ำเปล่ากลิ่นกาแฟค่ะ\n",
      "339 กาแฟมึงกากมากกกกกกกกกกกกกกกกกก\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7fb981be9d4f9f83a7840f21b3d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ไม่ ชอบ การ แบ่ง ชนชั้น ในร้านกาแฟ ผู้ถือบัตรอีกแบบ นั่งได้แค่ตรงนี้ ผู้ถือบัตรอีกแบบนั่งตรงนี้ จ่ายราคากาแฟก็เท่ากันไหม\n",
      "57 เป็นร้านที่แดกกาแฟร้อนทีไร แม่งต้องล้วกลิ้นไหม้เป็นแผลตลอด น้ำร้อนเอ็งจะร้อนไปไหนวะ สงสัยแม่งเอามาจากบ่อในนรกชัวร์ๆ ถ้าเอ็งร้อนแบบนี้\n",
      "58 ทานที่มหาวิทยาลัยเป็นมุมกาแฟรสชาติอร่อยมาก กาแฟดีมาก ราคาดีรับได้ แต่ถ้าเป็นร้านในห้างราคา over เกินไป\n",
      "90 กาแฟพอใช้ได้นะ แต่ราคาก็ยังสูงกว่าร้านอื่นๆอยุ่ ลงราคามาหน่อย หรือจัดโทรโมชั่นบ้าง\n",
      "99 ลองเปลี่ยนไปขายอย่างอื่นที่ไม่ใช่กาแฟดีกว่า\n",
      "101 กาแฟแม่งแรงสัสมึงจะดีดไปไหน\n",
      "114 กาแฟจืดสัดๆ เปรี้ยวด้วย ใช้โรบัสต้า ใช่มั้ยมึง\n",
      "128 ต่อไปคงมี ดีแท็ก/เอไอเอส กาแฟ คอยดู\n",
      "131 ไม่เคยแดก\n",
      "136 จ่ายค่าโท...ด้วย...หอมกลิ่นกาแฟ..ด้วย.. ไม่ได้ตังค์นางหร้อก\n",
      "149 เหมือนประชด เคยได้โปรฟรีเป็นกาแฟร้อนตอนหน้าร้อน\n",
      "152 นี่กาแฟหรือน้ำล้าง..(มือ)\n",
      "154 กาแฟไม่อร่อยเลย ราคาถูกยังไงก็ไม่คุ้ม\n",
      "159 อีนี่ก็แอบแพงไป..แดกค่าเนตไม่พอ ยังมากดราคากาแฟอีก 😡\n",
      "169 ไม่เคยแดก\n",
      "202 ถ้ามีส่วนลด กาแฟก็โอเคอยู่นะ\n",
      "215 เลิกใช้กาแฟเพื่อน มาใช้กาแฟคุณภาพได้แล้วครับ\n",
      "229 เรื่องกาแฟ\n",
      "247 ขอกาแฟสดเถอะขอร้องง\n",
      "261 นี่กาแฟหรือน้ำเปล่า?..🤔\n",
      "271 กาแฟที่สาขาสยามดิส ผ่าน\n",
      "273 เหนื่อยกาแฟ รสชาติ เหมือนของเหลือ\n",
      "278 จืดอ่า ทรูเนตกากประหยัดกาแฟ คอนเซป\n",
      "285 กาแฟไม่ได้เรื่องเลย\n",
      "288 นี่กาแฟหรือน้ำล้างตีน..\n",
      "290 แพง กาแฟจืด\n",
      "295 กาแฟก็จืด\n",
      "297 กาแฟแรงมาก\n",
      "326 กาแฟเหมือนกินน้ำเปล่ากลิ่นกาแฟค่ะ\n",
      "339 กาแฟมึงกากมากกกกกกกกกกกกกกกกกก\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea77617795440595ab24342c415756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['ชอบ', 'ร้าน', 'กาแฟ', 'บัตร', 'นั่ง', 'บัตร', 'นั่ง', 'จ่าย', 'ราคา', 'กาแฟ', 'ไหม']\n",
      "57 ['ร้าน', 'แดก', 'กาแฟ', 'ร้อน', 'แม่ง', 'น้ำ', 'ร้อน', 'ร้อน', 'แม่ง', 'ร้อน']\n",
      "58 ['ทาน', 'กาแฟ', 'รสชาติ', 'อร่อย', 'กาแฟ', 'ดี', 'ราคา', 'ดี', 'ร้าน', 'ห้าง', 'ราคา']\n",
      "90 ['กาแฟ', 'ราคา', 'ร้าน']\n",
      "99 ['ลอง', 'ขาย', 'กาแฟ', 'ดี']\n",
      "101 ['กาแฟ', 'แม่ง', 'แรง']\n",
      "114 ['กาแฟ', 'จืด', 'เปรี้ยว']\n",
      "128 ['ดี', 'กาแฟ', 'ดู']\n",
      "131 ['แดก']\n",
      "136 ['จ่าย', 'ค่า', 'หอม', 'กลิ่น', 'กาแฟ']\n",
      "149 ['เหมือน', 'กาแฟ', 'ร้อน', 'ตอน', 'หน้า', 'ร้อน']\n",
      "152 ['กาแฟ', 'น้ำ', 'ล้าง', 'มือ']\n",
      "154 ['กาแฟ', 'อร่อย', 'ราคา', 'คุ้ม']\n",
      "159 ['แดก', 'ค่า', 'เนต', 'กด', 'ราคา', 'กาแฟ']\n",
      "169 ['แดก']\n",
      "202 ['ลด', 'กาแฟ', 'โอเค']\n",
      "215 ['เลิก', 'กาแฟ', 'กาแฟ', 'คุณภาพ']\n",
      "229 ['เรื่อง', 'กาแฟ']\n",
      "247 ['กาแฟ', 'สด']\n",
      "261 ['กาแฟ', 'น้ำ', 'เปล่า']\n",
      "271 ['กาแฟ']\n",
      "273 ['กาแฟ', 'รสชาติ', 'เหมือน']\n",
      "278 ['จืด', 'อ่า', 'กาก', 'กาแฟ']\n",
      "285 ['กาแฟ', 'เรื่อง']\n",
      "288 ['กาแฟ', 'น้ำ', 'ล้าง']\n",
      "290 ['แพง', 'กาแฟ', 'จืด']\n",
      "295 ['กาแฟ', 'จืด']\n",
      "297 ['กาแฟ', 'แรง']\n",
      "326 ['กาแฟ', 'เหมือน', 'กิน', 'น้ำ', 'เปล่า', 'กลิ่น', 'กาแฟ']\n",
      "339 ['กาแฟ', 'กาก']\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['ร้าน', 'สวย', 'สวย', 'เงิน', 'พนง', 'เรียน', 'ชง', 'กาแฟ']\n",
      "ร้านสวยมาก สวยจนอยากบอกว่า เอาเงินแต่งร้าน ส่ง พนง ไปเรียนชงกาแฟก่อนเถอะค่ะ\n",
      "['ร้าน', 'สวย', 'สวย', 'เงิน', 'พนง', 'เรียน', 'ชง', 'กาแฟ']\n",
      "ร้านสวยมาก สวยจนอยากบอกว่า เอาเงินแต่งร้าน ส่ง พนง ไปเรียนชงกาแฟก่อนเถอะค่ะ\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(onehot_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(onehot_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
