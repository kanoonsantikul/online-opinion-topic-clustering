{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 269\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1313 words\n",
      "filter frequent words: 540 words\n",
      "filter letter words: 539 words\n",
      "filter stop words: 352 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "average_doc_size\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "\n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    cluster_num = 0\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            points.remove(seed)\n",
    "\n",
    "    while cluster_num != 0:\n",
    "        cluster = [numpy.array(bow_corpus.iloc[i]) for i, label in enumerate(labels) if label == cluster_num]\n",
    "        eps_temp = eps\n",
    "        \n",
    "        while True:\n",
    "            centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "            eps_temp -= delta_eps\n",
    "            \n",
    "            count = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                point = numpy.array(bow_corpus.iloc[i]).reshape(1, -1)\n",
    "                if label == 0 and cosine_similarity(centroid, point) >= eps_temp:\n",
    "                    cluster.append(point[0])\n",
    "                    labels[i] = cluster_num\n",
    "                    count += 1\n",
    "            if count == 0:\n",
    "                break\n",
    "        \n",
    "        cluster_num -= 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    clusters = []\n",
    "    cluster_num = 0\n",
    "    clusters.append([])\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            clusters.append([])\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "                clusters[cluster_num].append(numpy.array(bow_corpus.iloc[p]))\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            clusters[0].append((seed, numpy.array(bow_corpus.iloc[seed])))\n",
    "            points.remove(seed)\n",
    "\n",
    "    grow = numpy.zeros(cluster_num + 1)    \n",
    "    while sum(grow) != -cluster_num:\n",
    "        eps -= delta_eps\n",
    "        count = numpy.zeros(cluster_num + 1)\n",
    "        for point in clusters[0]:\n",
    "            if labels[point[0]] != 0:\n",
    "                continue\n",
    "\n",
    "            num = point[0]\n",
    "            p = point[1].reshape(1, -1)\n",
    "            sim = 0\n",
    "            for c, cluster in enumerate(clusters[1:]):\n",
    "                if grow[c] == -1:\n",
    "                    continue\n",
    "                centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "                if cosine_similarity(centroid, p) >= sim:\n",
    "                    sim = cosine_similarity(centroid, p)\n",
    "                if sim >= eps:\n",
    "                    labels[num] = c + 1\n",
    "\n",
    "            if labels[num] != 0:\n",
    "                count[labels[num]] += 1\n",
    "                clusters[labels[num]].append(point[1])\n",
    "\n",
    "        for i, num in enumerate(count):\n",
    "            if num == 0:\n",
    "                grow[i] = -1\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(bow_corpus, min_samples, eps):\n",
    "    predicted_labels = upgrade_sdc(bow_corpus, min_samples, eps)\n",
    "\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ทำ</th>\n",
       "      <th>พนักงาน</th>\n",
       "      <th>ข้าว</th>\n",
       "      <th>ดี</th>\n",
       "      <th>น้ำ</th>\n",
       "      <th>อี</th>\n",
       "      <th>เครื่อง</th>\n",
       "      <th>เลิก</th>\n",
       "      <th>กก</th>\n",
       "      <th>งง</th>\n",
       "      <th>...</th>\n",
       "      <th>พลังงาน</th>\n",
       "      <th>คอย</th>\n",
       "      <th>แอร์ไม่</th>\n",
       "      <th>สลิป</th>\n",
       "      <th>คู</th>\n",
       "      <th>ปอง</th>\n",
       "      <th>นวมินทร์</th>\n",
       "      <th>สิบ</th>\n",
       "      <th>ห้า</th>\n",
       "      <th>แจก</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ทำ   พนักงาน      ข้าว        ดี       น้ำ        อี   เครื่อง  \\\n",
       "0  0.333333  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.166667  0.166667  0.166667  0.166667  0.166667   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.111111  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.111111  0.111111  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       เลิก        กก        งง ...   พลังงาน  คอย  แอร์ไม่  สลิป   คู  ปอง  \\\n",
       "0  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "1  0.166667  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "2  0.000000  0.111111  0.111111 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "3  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "4  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "\n",
       "   นวมินทร์  สิบ  ห้า  แจก  \n",
       "0       0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow_corpus = get_bow(idx_corpus)\n",
    "bow_corpus = get_bow(filtered_corpus)\n",
    "result = predict_cluster(bow_corpus, 7, 0.2)\n",
    "bow_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([34, 42, 44, 15, 57, 11,  8, 12, 12, 12, 22]))\n",
      "\t\tpercent\n",
      "0  |\t34\t1.0\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_clusters = label_count[0][-1] + 1\n",
    "print(label_count)\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\t' + cluster, end='')\n",
    "print('\\tpercent')\n",
    "\n",
    "for label in range(len(clusters)):\n",
    "    print(str(label) + '  |', end='')\n",
    "    \n",
    "    num_max = 0\n",
    "    for cluster in clusters:\n",
    "        loc = result[(result['label'] == cluster) & (result['predicted_label'] == label)]\n",
    "        if len(loc) > num_max:\n",
    "            num_max = len(loc)\n",
    "        print('\\t' + str(len(loc)), end='')\n",
    "    \n",
    "    print('\\t' + str(num_max / label_count[1][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54eee27eef6949c58f9b4208643d9bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), value=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 อย่าบังคับน้องเข้าประชุมเชียร์ อย่าลงโทษโดยเหตุผลงี่เง่าๆ เกิดก่อนไม่กี่ปีเอง\n",
      "26 โลตลาด พนงปากตลาด นินทาลคเผาขน ตะโกนโหวกเหวกข้ามหัวลค วันหวยออกสนั่นเป็นพิเศษ เห็นพฤติกรรมแล้วแย่ ปรับปรุงภาพลักษณ์เถอะ\n",
      "55 ของมึงไม่ต้องถูกมากก็ได้ เน้นคุณภาพบ้าง ไส้กรอกเน่า แต่ยังไม่หมดอายุงี้อ่ะ สาขาฟอร์จูน ขนมปังขึ้นราแล้วยังวางขายอีก สาขาลาดพร้าว กีวี่หนอนเข้า กะหล่ำปลีเน่าใน ผักบุ้งเน่าแทรกในมัด สาขาอ่อนนุช\n",
      "57 ของป้ายเหลืองพวกผัก เน่าจนไม่รู้จะเน่ายังไง คือทิ้งๆไปก็ได้มั่ง\n",
      "84 ฝากถึง สาขาแพร่นะคะตอนเช้าๆเลิกมาทอดโดนัทด้านนอกซะทีเถอะค่ะ กลิ่นเหม็นน้ำมันทอดติดผมเผ้าเสื้อผ้า\n",
      "95 โสตัสเอ็กเพรส สาขาตลาดหนองจอก ระยอง ตู้แช่เย็นฝาตู้ปิดไม่สนิททุกตู้\n",
      "104 ของเซลคือเซลเเต่หมดอายุมาหลายเดือนเเล้วยังเซลได้ด้วยหรอของกินไงบางที\n",
      "112 วันใหนมาตั้งใจมาชื้อหมู หมูหมดเหลือไก่  วันใหนชื้อไก่ไก่หมดได้หมู  ตั้งใจอย่างได้อีกอย่าง\n",
      "122 วางของเกะกะมาก เป็นแทบทุกสาขาที่เคยเข้าเลย ไม่มีมาตรการจัดของกันเลยหรอ เอ็กซ์เพรส\n",
      "131 เอ็กเพรสนี้แลกของแล้วไม่เคยจะโทรบอกเลยหรืออะไรเลยต้องได้ทวง ชอบไม่ให้แสตมป์ เป็นไรมากไหมนี้\n",
      "163 ไปทีไรไม่เห็นมีดอกบัวขาย\n",
      "166 ล้อรถๆ จะหลุดไปไหนพังทุกคัน\n",
      "169 ใช้เครื่องรูดไม่เป็น ไม่รู้จัก\n",
      "171 คืออ่านจนเหนื่อยค่ะ\n",
      "178 ไม่จำเป็นไม่เข้าจริงๆ\n",
      "182 แมลงสาปให้มันน้อยๆหน่อย\n",
      "192 เส้นสีเขียวยังใช้อยู่ไหม\n",
      "202 พระราม 4 ที่จอดรถ   ดีมาก มีแต่ผู้ชายมาจอด\n",
      "205 กลิ่น\n",
      "212 ซื้อไฃ่เจอไข่เน่าเอาไข่ดีๆมาขายบางนะ\n",
      "215 ถามบัตรคลับการ์กูด้วย กูไม่อยากบอกเอง\n",
      "219 หนูวิ่งเป็นของสด\n",
      "220 ตัดบิล กะ ลืมยิงคลับการ์ดให้\n",
      "223 โปรโมชั่นกำกวมมากกกก\n",
      "224 มาตรฐานพลังงานต่ำมากถ้าเทียบกับเซเว่นและบิ๊กซี\n",
      "225 อยากให้ห้องนํ้ามีนํ้า\n",
      "227 ขอยามคอยตรวจที่จอดรถสาขาสระบุรีรถหายบ่อยมาก\n",
      "229 ที่จอดรถควรมีรับบัตรเข้าออก\n",
      "234 แอร์ไม่เย็น\n",
      "239 คูปองไม่ต้องส่งมาที่บ้าน อยากได้ในบิลเลย\n",
      "250 แล้วแต่ที่จริงๆ\n",
      "254 ที่จอดรถสาขานวมินทร์ หาที่จอดยากมากค่ะ\n",
      "255 แอร์ไม่เปิด\n",
      "262 แสตมป์คลับการ์ดควรมีแบบเดิมนะ\n"
     ]
    }
   ],
   "source": [
    "comment_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_comment_widget_click(change):\n",
    "    clear_output()\n",
    "    display(comment_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "comment_widget.observe(on_comment_widget_click, names='index')\n",
    "on_comment_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c112e483db477f992d0d8d081c1ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), value=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ['อย่า', 'บังคับ', 'น้อง', 'เชียร์', 'อย่า', 'งี่', 'เง่า', 'กี่', 'ปี']\n",
      "26 ['พนง', 'ตะโกน', 'ข้าม', 'หวย', 'พิเศษ', 'พฤติกรรม', 'แย่', 'ปรับปรุง']\n",
      "55 ['คุณภาพ', 'เน่า', 'อายุ', 'อ่ะ', 'สาขา', 'วาง', 'ขาย', 'สาขา', 'เน่า', 'เน่า']\n",
      "57 ['ป้ายเหลือง', 'ผัก', 'เน่า', 'รู้', 'เน่า', 'ทิ้ง']\n",
      "84 ['ฝาก', 'สาขา', 'ตอน', 'เช้า', 'เลิก', 'กลิ่น', 'เหม็น', 'ติด', 'ผม']\n",
      "95 ['สาขา', 'ตู้', 'เย็น', 'ตู้', 'ตู้']\n",
      "104 ['เเต่', 'อายุ', 'เดือน', 'เเล้ว', 'หรอ', 'กิน']\n",
      "112 ['ใหน', 'ตั้งใจ', 'ชื้อ', 'หมู', 'หมู', 'ไก่', 'ใหน', 'ชื้อ', 'ไก่', 'ไก่', 'หมู', 'ตั้งใจ']\n",
      "122 ['วาง', 'เกะกะ', 'แทบ', 'สาขา', 'หรอ', 'เอ็กซ์เพรส']\n",
      "131 ['เอ็กเพรส', 'แลก', 'โทร', 'ชอบ', 'แสตมป์', 'ไหม']\n",
      "163 ['ดอก', 'ขาย']\n",
      "166 ['ล้อ', 'รถ', 'หลุด', 'พัง', 'คัน']\n",
      "169 []\n",
      "171 ['เหนื่อย']\n",
      "178 []\n",
      "182 []\n",
      "192 ['สี', 'เขียว', 'ไหม']\n",
      "202 ['จอด', 'รถ', 'ดี', 'ชาย', 'จอด']\n",
      "205 ['กลิ่น']\n",
      "212 ['ซื้อ', 'เจอ', 'เน่า', 'ดี', 'ขาย']\n",
      "215 ['ถาม', 'บัตร', 'คลับ']\n",
      "219 ['หนู', 'วิ่ง', 'สด']\n",
      "220 ['กะ', 'ยิง', 'คลับ', 'การ์ด']\n",
      "223 ['โปรโมชั่น', 'กกก']\n",
      "224 ['พลังงาน', 'ต่ำ', 'เซเว่น']\n",
      "225 []\n",
      "227 ['คอย', 'ตรวจ', 'จอด', 'รถ', 'รถ', 'หาย']\n",
      "229 ['จอด', 'รถ', 'บัตร']\n",
      "234 ['แอร์ไม่', 'เย็น']\n",
      "239 ['คู', 'ปอง', 'บ้าน', 'บิล']\n",
      "250 []\n",
      "254 ['จอด', 'รถ', 'สาขา', 'นวมินทร์', 'หา', 'จอด']\n",
      "255 ['แอร์ไม่']\n",
      "262 ['แสตมป์', 'คลับ', 'การ์ด']\n"
     ]
    }
   ],
   "source": [
    "token_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_token_widget_click(change):\n",
    "    clear_output()\n",
    "    display(token_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['tokenized_comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "token_widget.observe(on_token_widget_click, names='index')\n",
    "on_token_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
