{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus\n",
    "\n",
    "from model.new_sdc import NewSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 350\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ - TrueCoffee'\n",
    "\n",
    "corpus, labels = load_corpus('../data/' + file_name + '.txt')\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/tokenized/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 948 words\n",
      "filter frequent words: 370 words\n",
      "filter letter words: 368 words\n",
      "filter stop words: 219 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.extend(['‡∏ô‡∏µ‡πâ'])\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(idx_corpus)]\n",
    "model = Doc2Vec(tagged_corpus, vector_size=average_doc_size, window=4, min_count=2, epochs=100)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "paragraph_vectors = [model.infer_vector(doc) for doc in idx_corpus]\n",
    "paragraph_vectors = pandas.DataFrame(paragraph_vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(corpus, weight):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if weight == 'normal':\n",
    "        weight_corpus = bow_corpus\n",
    "    elif weight == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        weight_corpus = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(weight_corpus):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if weight == 'normal' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(onehot_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(onehot_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(onehot_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.32\n",
    "expand_rate = 0.05\n",
    "\n",
    "onehot_corpus = get_onehot(idx_corpus, 'normal')\n",
    "# onehot_corpus = get_onehot(filtered_corpus, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.1837864203253\n",
      "[ 30  10  30   9  12  12  25  22  10  10  25 149   4   2]\n"
     ]
    }
   ],
   "source": [
    "max_compactness = 0\n",
    "epoch = 15\n",
    "for i in range(epoch):\n",
    "#     model = NewSDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps)\n",
    "    \n",
    "    marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "    model = KMeans(n_clusters=14).fit(onehot_corpus)\n",
    "    _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(onehot_corpus, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "num_cluster = len(label_count)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None \n",
    "prev_label_count = None\n",
    "while True:\n",
    "    model = NewSDC()\n",
    "    predicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(onehot_corpus, result)\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    if numpy.array_equal(label_count, prev_label_count):\n",
    "        break\n",
    "    prev_label_count = label_count\n",
    "    centroids = centroids[1:]\n",
    "    \n",
    "    print(compactness)\n",
    "    print(label_count)\n",
    "num_cluster = len(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "11 1 0.30535198048644013\n",
      "10 9 0.8783020838381587\n",
      "10 3 0.34298458536886484\n",
      "3 2 0.33847700234811184\n",
      "[0, 1, 2, 2, 4, 5, 6, 7, 8, 2, 2, 1, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "result.to_csv('../data/results/k-mean/' + file_name + '.csv')\n",
    "\n",
    "# result = pandas.read_csv('../data/results/k-mean/' + file_name + '.csv')\n",
    "\n",
    "count = 0\n",
    "for index, value in result.iterrows():\n",
    "    if value['marks'] == -1:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe75fef9c50422488221bdaa5117744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 4, 5, 6, 7, 8, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ‡πÑ‡∏°‡πà ‡∏ä‡∏≠‡∏ö ‡∏Å‡∏≤‡∏£ ‡πÅ‡∏ö‡πà‡∏á ‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô ‡πÉ‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏ö‡∏±‡∏ï‡∏£‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö ‡∏ô‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏ö‡∏±‡∏ï‡∏£‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πà‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏à‡πà‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏´‡∏°\n",
      "57 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏î‡∏Å‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πÑ‡∏£ ‡πÅ‡∏°‡πà‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏¥‡πâ‡∏ô‡πÑ‡∏´‡∏°‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏•‡∏ï‡∏•‡∏≠‡∏î ‡∏ô‡πâ‡∏≥‡∏£‡πâ‡∏≠‡∏ô‡πÄ‡∏≠‡πá‡∏á‡∏à‡∏∞‡∏£‡πâ‡∏≠‡∏ô‡πÑ‡∏õ‡πÑ‡∏´‡∏ô‡∏ß‡∏∞ ‡∏™‡∏á‡∏™‡∏±‡∏¢‡πÅ‡∏°‡πà‡∏á‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ö‡πà‡∏≠‡πÉ‡∏ô‡∏ô‡∏£‡∏Å‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏≠‡πá‡∏á‡∏£‡πâ‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ\n",
      "58 ‡∏ó‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∏‡∏°‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡∏Å‡∏≤‡πÅ‡∏ü‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏µ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ over ‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
      "90 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ô‡∏∞ ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏≠‡∏¢‡∏∏‡πà ‡∏•‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏°‡∏≤‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡πÇ‡∏ó‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡∏ö‡πâ‡∏≤‡∏á\n",
      "99 ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
      "101 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏°‡πà‡∏á‡πÅ‡∏£‡∏á‡∏™‡∏±‡∏™‡∏°‡∏∂‡∏á‡∏à‡∏∞‡∏î‡∏µ‡∏î‡πÑ‡∏õ‡πÑ‡∏´‡∏ô\n",
      "114 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏à‡∏∑‡∏î‡∏™‡∏±‡∏î‡πÜ ‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏ä‡πâ‡πÇ‡∏£‡∏ö‡∏±‡∏™‡∏ï‡πâ‡∏≤ ‡πÉ‡∏ä‡πà‡∏°‡∏±‡πâ‡∏¢‡∏°‡∏∂‡∏á\n",
      "128 ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏á‡∏°‡∏µ ‡∏î‡∏µ‡πÅ‡∏ó‡πá‡∏Å/‡πÄ‡∏≠‡πÑ‡∏≠‡πÄ‡∏≠‡∏™ ‡∏Å‡∏≤‡πÅ‡∏ü ‡∏Ñ‡∏≠‡∏¢‡∏î‡∏π\n",
      "131 ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÅ‡∏î‡∏Å\n",
      "136 ‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡πÇ‡∏ó...‡∏î‡πâ‡∏ß‡∏¢...‡∏´‡∏≠‡∏°‡∏Å‡∏•‡∏¥‡πà‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü..‡∏î‡πâ‡∏ß‡∏¢.. ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡∏á‡∏Ñ‡πå‡∏ô‡∏≤‡∏á‡∏´‡∏£‡πâ‡∏≠‡∏Å\n",
      "149 ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏î ‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏î‡πâ‡πÇ‡∏õ‡∏£‡∏ü‡∏£‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡πâ‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≠‡∏ô\n",
      "152 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏•‡πâ‡∏≤‡∏á..(‡∏°‡∏∑‡∏≠)\n",
      "154 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢‡πÄ‡∏•‡∏¢ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°\n",
      "159 ‡∏≠‡∏µ‡∏ô‡∏µ‡πà‡∏Å‡πá‡πÅ‡∏≠‡∏ö‡πÅ‡∏û‡∏á‡πÑ‡∏õ..‡πÅ‡∏î‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏ô‡∏ï‡πÑ‡∏°‡πà‡∏û‡∏≠ ‡∏¢‡∏±‡∏á‡∏°‡∏≤‡∏Å‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏≠‡∏µ‡∏Å üò°\n",
      "169 ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÅ‡∏î‡∏Å\n",
      "202 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏∞\n",
      "215 ‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö\n",
      "229 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡πÅ‡∏ü\n",
      "247 ‡∏Ç‡∏≠‡∏Å‡∏≤‡πÅ‡∏ü‡∏™‡∏î‡πÄ‡∏ñ‡∏≠‡∏∞‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á‡∏á\n",
      "261 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤?..ü§î\n",
      "271 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏Ç‡∏≤‡∏™‡∏¢‡∏≤‡∏°‡∏î‡∏¥‡∏™ ‡∏ú‡πà‡∏≤‡∏ô\n",
      "273 ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏Å‡∏≤‡πÅ‡∏ü ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠\n",
      "278 ‡∏à‡∏∑‡∏î‡∏≠‡πà‡∏≤ ‡∏ó‡∏£‡∏π‡πÄ‡∏ô‡∏ï‡∏Å‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Å‡∏≤‡πÅ‡∏ü ‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ã‡∏õ\n",
      "285 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏•‡∏¢\n",
      "288 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏•‡πâ‡∏≤‡∏á‡∏ï‡∏µ‡∏ô..\n",
      "290 ‡πÅ‡∏û‡∏á ‡∏Å‡∏≤‡πÅ‡∏ü‡∏à‡∏∑‡∏î\n",
      "295 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡∏à‡∏∑‡∏î\n",
      "297 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å\n",
      "326 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏¥‡∏ô‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏Å‡∏•‡∏¥‡πà‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü‡∏Ñ‡πà‡∏∞\n",
      "339 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏°‡∏∂‡∏á‡∏Å‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7fb981be9d4f9f83a7840f21b3d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ‡πÑ‡∏°‡πà ‡∏ä‡∏≠‡∏ö ‡∏Å‡∏≤‡∏£ ‡πÅ‡∏ö‡πà‡∏á ‡∏ä‡∏ô‡∏ä‡∏±‡πâ‡∏ô ‡πÉ‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏ö‡∏±‡∏ï‡∏£‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö ‡∏ô‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏ö‡∏±‡∏ï‡∏£‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡πà‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡∏à‡πà‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏´‡∏°\n",
      "57 ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏î‡∏Å‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡πâ‡∏≠‡∏ô‡∏ó‡∏µ‡πÑ‡∏£ ‡πÅ‡∏°‡πà‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏¥‡πâ‡∏ô‡πÑ‡∏´‡∏°‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏•‡∏ï‡∏•‡∏≠‡∏î ‡∏ô‡πâ‡∏≥‡∏£‡πâ‡∏≠‡∏ô‡πÄ‡∏≠‡πá‡∏á‡∏à‡∏∞‡∏£‡πâ‡∏≠‡∏ô‡πÑ‡∏õ‡πÑ‡∏´‡∏ô‡∏ß‡∏∞ ‡∏™‡∏á‡∏™‡∏±‡∏¢‡πÅ‡∏°‡πà‡∏á‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ö‡πà‡∏≠‡πÉ‡∏ô‡∏ô‡∏£‡∏Å‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏≠‡πá‡∏á‡∏£‡πâ‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ\n",
      "58 ‡∏ó‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∏‡∏°‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡∏Å‡∏≤‡πÅ‡∏ü‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏µ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤ over ‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
      "90 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏û‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ô‡∏∞ ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏≠‡∏¢‡∏∏‡πà ‡∏•‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏°‡∏≤‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡πÇ‡∏ó‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡∏ö‡πâ‡∏≤‡∏á\n",
      "99 ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n",
      "101 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏°‡πà‡∏á‡πÅ‡∏£‡∏á‡∏™‡∏±‡∏™‡∏°‡∏∂‡∏á‡∏à‡∏∞‡∏î‡∏µ‡∏î‡πÑ‡∏õ‡πÑ‡∏´‡∏ô\n",
      "114 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏à‡∏∑‡∏î‡∏™‡∏±‡∏î‡πÜ ‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏ä‡πâ‡πÇ‡∏£‡∏ö‡∏±‡∏™‡∏ï‡πâ‡∏≤ ‡πÉ‡∏ä‡πà‡∏°‡∏±‡πâ‡∏¢‡∏°‡∏∂‡∏á\n",
      "128 ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏á‡∏°‡∏µ ‡∏î‡∏µ‡πÅ‡∏ó‡πá‡∏Å/‡πÄ‡∏≠‡πÑ‡∏≠‡πÄ‡∏≠‡∏™ ‡∏Å‡∏≤‡πÅ‡∏ü ‡∏Ñ‡∏≠‡∏¢‡∏î‡∏π\n",
      "131 ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÅ‡∏î‡∏Å\n",
      "136 ‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡πÇ‡∏ó...‡∏î‡πâ‡∏ß‡∏¢...‡∏´‡∏≠‡∏°‡∏Å‡∏•‡∏¥‡πà‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü..‡∏î‡πâ‡∏ß‡∏¢.. ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡∏á‡∏Ñ‡πå‡∏ô‡∏≤‡∏á‡∏´‡∏£‡πâ‡∏≠‡∏Å\n",
      "149 ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏î ‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏î‡πâ‡πÇ‡∏õ‡∏£‡∏ü‡∏£‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡πâ‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏£‡πâ‡∏≠‡∏ô\n",
      "152 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏•‡πâ‡∏≤‡∏á..(‡∏°‡∏∑‡∏≠)\n",
      "154 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢‡πÄ‡∏•‡∏¢ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°\n",
      "159 ‡∏≠‡∏µ‡∏ô‡∏µ‡πà‡∏Å‡πá‡πÅ‡∏≠‡∏ö‡πÅ‡∏û‡∏á‡πÑ‡∏õ..‡πÅ‡∏î‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏ô‡∏ï‡πÑ‡∏°‡πà‡∏û‡∏≠ ‡∏¢‡∏±‡∏á‡∏°‡∏≤‡∏Å‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡∏≤‡πÅ‡∏ü‡∏≠‡∏µ‡∏Å üò°\n",
      "169 ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÅ‡∏î‡∏Å\n",
      "202 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏∞\n",
      "215 ‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö\n",
      "229 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡πÅ‡∏ü\n",
      "247 ‡∏Ç‡∏≠‡∏Å‡∏≤‡πÅ‡∏ü‡∏™‡∏î‡πÄ‡∏ñ‡∏≠‡∏∞‡∏Ç‡∏≠‡∏£‡πâ‡∏≠‡∏á‡∏á\n",
      "261 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤?..ü§î\n",
      "271 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏Ç‡∏≤‡∏™‡∏¢‡∏≤‡∏°‡∏î‡∏¥‡∏™ ‡∏ú‡πà‡∏≤‡∏ô\n",
      "273 ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏Å‡∏≤‡πÅ‡∏ü ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠\n",
      "278 ‡∏à‡∏∑‡∏î‡∏≠‡πà‡∏≤ ‡∏ó‡∏£‡∏π‡πÄ‡∏ô‡∏ï‡∏Å‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Å‡∏≤‡πÅ‡∏ü ‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ã‡∏õ\n",
      "285 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏•‡∏¢\n",
      "288 ‡∏ô‡∏µ‡πà‡∏Å‡∏≤‡πÅ‡∏ü‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡πâ‡∏≥‡∏•‡πâ‡∏≤‡∏á‡∏ï‡∏µ‡∏ô..\n",
      "290 ‡πÅ‡∏û‡∏á ‡∏Å‡∏≤‡πÅ‡∏ü‡∏à‡∏∑‡∏î\n",
      "295 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πá‡∏à‡∏∑‡∏î\n",
      "297 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å\n",
      "326 ‡∏Å‡∏≤‡πÅ‡∏ü‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏¥‡∏ô‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏Å‡∏•‡∏¥‡πà‡∏ô‡∏Å‡∏≤‡πÅ‡∏ü‡∏Ñ‡πà‡∏∞\n",
      "339 ‡∏Å‡∏≤‡πÅ‡∏ü‡∏°‡∏∂‡∏á‡∏Å‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å‡∏Å\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ea77617795440595ab24342c415756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['‡∏ä‡∏≠‡∏ö', '‡∏£‡πâ‡∏≤‡∏ô', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏ö‡∏±‡∏ï‡∏£', '‡∏ô‡∏±‡πà‡∏á', '‡∏ö‡∏±‡∏ï‡∏£', '‡∏ô‡∏±‡πà‡∏á', '‡∏à‡πà‡∏≤‡∏¢', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÑ‡∏´‡∏°']\n",
      "57 ['‡∏£‡πâ‡∏≤‡∏ô', '‡πÅ‡∏î‡∏Å', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏£‡πâ‡∏≠‡∏ô', '‡πÅ‡∏°‡πà‡∏á', '‡∏ô‡πâ‡∏≥', '‡∏£‡πâ‡∏≠‡∏ô', '‡∏£‡πâ‡∏≠‡∏ô', '‡πÅ‡∏°‡πà‡∏á', '‡∏£‡πâ‡∏≠‡∏ô']\n",
      "58 ['‡∏ó‡∏≤‡∏ô', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥', '‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏î‡∏µ', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏î‡∏µ', '‡∏£‡πâ‡∏≤‡∏ô', '‡∏´‡πâ‡∏≤‡∏á', '‡∏£‡∏≤‡∏Ñ‡∏≤']\n",
      "90 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏£‡πâ‡∏≤‡∏ô']\n",
      "99 ['‡∏•‡∏≠‡∏á', '‡∏Ç‡∏≤‡∏¢', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏î‡∏µ']\n",
      "101 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÅ‡∏°‡πà‡∏á', '‡πÅ‡∏£‡∏á']\n",
      "114 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏à‡∏∑‡∏î', '‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß']\n",
      "128 ['‡∏î‡∏µ', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏î‡∏π']\n",
      "131 ['‡πÅ‡∏î‡∏Å']\n",
      "136 ['‡∏à‡πà‡∏≤‡∏¢', '‡∏Ñ‡πà‡∏≤', '‡∏´‡∏≠‡∏°', '‡∏Å‡∏•‡∏¥‡πà‡∏ô', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "149 ['‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏£‡πâ‡∏≠‡∏ô', '‡∏ï‡∏≠‡∏ô', '‡∏´‡∏ô‡πâ‡∏≤', '‡∏£‡πâ‡∏≠‡∏ô']\n",
      "152 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏ô‡πâ‡∏≥', '‡∏•‡πâ‡∏≤‡∏á', '‡∏°‡∏∑‡∏≠']\n",
      "154 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏≠‡∏£‡πà‡∏≠‡∏¢', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Ñ‡∏∏‡πâ‡∏°']\n",
      "159 ['‡πÅ‡∏î‡∏Å', '‡∏Ñ‡πà‡∏≤', '‡πÄ‡∏ô‡∏ï', '‡∏Å‡∏î', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "169 ['‡πÅ‡∏î‡∏Å']\n",
      "202 ['‡∏•‡∏î', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÇ‡∏≠‡πÄ‡∏Ñ']\n",
      "215 ['‡πÄ‡∏•‡∏¥‡∏Å', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û']\n",
      "229 ['‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "247 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏™‡∏î']\n",
      "261 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏ô‡πâ‡∏≥', '‡πÄ‡∏õ‡∏•‡πà‡∏≤']\n",
      "271 ['‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "273 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥', '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô']\n",
      "278 ['‡∏à‡∏∑‡∏î', '‡∏≠‡πà‡∏≤', '‡∏Å‡∏≤‡∏Å', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "285 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á']\n",
      "288 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏ô‡πâ‡∏≥', '‡∏•‡πâ‡∏≤‡∏á']\n",
      "290 ['‡πÅ‡∏û‡∏á', '‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏à‡∏∑‡∏î']\n",
      "295 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏à‡∏∑‡∏î']\n",
      "297 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÅ‡∏£‡∏á']\n",
      "326 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô', '‡∏Å‡∏¥‡∏ô', '‡∏ô‡πâ‡∏≥', '‡πÄ‡∏õ‡∏•‡πà‡∏≤', '‡∏Å‡∏•‡∏¥‡πà‡∏ô', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "339 ['‡∏Å‡∏≤‡πÅ‡∏ü', '‡∏Å‡∏≤‡∏Å']\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['‡∏£‡πâ‡∏≤‡∏ô', '‡∏™‡∏ß‡∏¢', '‡∏™‡∏ß‡∏¢', '‡πÄ‡∏á‡∏¥‡∏ô', '‡∏û‡∏ô‡∏á', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏ä‡∏á', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "‡∏£‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏¢‡∏°‡∏≤‡∏Å ‡∏™‡∏ß‡∏¢‡∏à‡∏ô‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏≠‡∏≤‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡∏á‡∏£‡πâ‡∏≤‡∏ô ‡∏™‡πà‡∏á ‡∏û‡∏ô‡∏á ‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ä‡∏á‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞‡∏Ñ‡πà‡∏∞\n",
      "['‡∏£‡πâ‡∏≤‡∏ô', '‡∏™‡∏ß‡∏¢', '‡∏™‡∏ß‡∏¢', '‡πÄ‡∏á‡∏¥‡∏ô', '‡∏û‡∏ô‡∏á', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏ä‡∏á', '‡∏Å‡∏≤‡πÅ‡∏ü']\n",
      "‡∏£‡πâ‡∏≤‡∏ô‡∏™‡∏ß‡∏¢‡∏°‡∏≤‡∏Å ‡∏™‡∏ß‡∏¢‡∏à‡∏ô‡∏≠‡∏¢‡∏≤‡∏Å‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏≠‡∏≤‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡∏á‡∏£‡πâ‡∏≤‡∏ô ‡∏™‡πà‡∏á ‡∏û‡∏ô‡∏á ‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ä‡∏á‡∏Å‡∏≤‡πÅ‡∏ü‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏ñ‡∏≠‡∏∞‡∏Ñ‡πà‡∏∞\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(onehot_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(onehot_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
