{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 269\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1313 words\n",
      "filter frequent words: 540 words\n",
      "filter letter words: 539 words\n",
      "filter stop words: 352 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "average_doc_size\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "\n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    cluster_num = 0\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            points.remove(seed)\n",
    "\n",
    "    while cluster_num != 0:\n",
    "        cluster = [numpy.array(bow_corpus.iloc[i]) for i, label in enumerate(labels) if label == cluster_num]\n",
    "        eps_temp = eps\n",
    "        \n",
    "        while True:\n",
    "            centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "            eps_temp -= delta_eps\n",
    "            \n",
    "            count = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                point = numpy.array(bow_corpus.iloc[i]).reshape(1, -1)\n",
    "                if label == 0 and cosine_similarity(centroid, point) >= eps_temp:\n",
    "                    cluster.append(point[0])\n",
    "                    labels[i] = cluster_num\n",
    "                    count += 1\n",
    "            if count == 0:\n",
    "                break\n",
    "        \n",
    "        cluster_num -= 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_sdc(bow_corpus, min_samples, eps):\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    initials = []\n",
    "    delta_eps = eps / 10\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    clusters = []\n",
    "    cluster_num = 0\n",
    "    clusters.append([])\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            print(seed)\n",
    "            cluster_num += 1\n",
    "            clusters.append([])\n",
    "            for p in eps_neighbors:\n",
    "                initials.append(p)\n",
    "                labels[p] = cluster_num\n",
    "                clusters[cluster_num].append(numpy.array(bow_corpus.iloc[p]))\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            clusters[0].append((seed, numpy.array(bow_corpus.iloc[seed])))\n",
    "            points.remove(seed)\n",
    "            \n",
    "    expandable = numpy.zeros(cluster_num + 1)    \n",
    "    while sum(expandable) != -cluster_num - 1:\n",
    "        eps -= delta_eps\n",
    "        count = numpy.zeros(cluster_num + 1)\n",
    "        for point in clusters[0]:\n",
    "            if labels[point[0]] != 0:\n",
    "                continue\n",
    "\n",
    "            num = point[0]\n",
    "            p = point[1].reshape(1, -1)\n",
    "            sim = 0\n",
    "            for c, cluster in enumerate(clusters[1:]):\n",
    "                if expandable[c + 1] == -1:\n",
    "                    continue\n",
    "                centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "                if cosine_similarity(centroid, p) >= sim:\n",
    "                    sim = cosine_similarity(centroid, p)\n",
    "                    if sim >= eps:\n",
    "                        labels[num] = c + 1\n",
    "\n",
    "            if labels[num] != 0:\n",
    "                count[labels[num]] += 1\n",
    "                clusters[labels[num]].append(point[1])\n",
    "\n",
    "        for i, num in enumerate(count):\n",
    "            if num == 0:\n",
    "                expandable[i] = -1\n",
    "        \n",
    "    return labels, initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(bow_corpus, min_samples, eps):\n",
    "    predicted_labels, initials = upgrade_sdc(bow_corpus, min_samples, eps)\n",
    "\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = filtered_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    \n",
    "    return result, initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "265\n",
      "153\n",
      "200\n",
      "134\n",
      "198\n",
      "85\n",
      "77\n",
      "154\n",
      "82\n",
      "41\n",
      "184\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ทำ</th>\n",
       "      <th>พนักงาน</th>\n",
       "      <th>ข้าว</th>\n",
       "      <th>ดี</th>\n",
       "      <th>น้ำ</th>\n",
       "      <th>อี</th>\n",
       "      <th>เครื่อง</th>\n",
       "      <th>เลิก</th>\n",
       "      <th>กก</th>\n",
       "      <th>งง</th>\n",
       "      <th>...</th>\n",
       "      <th>พลังงาน</th>\n",
       "      <th>คอย</th>\n",
       "      <th>แอร์ไม่</th>\n",
       "      <th>สลิป</th>\n",
       "      <th>คู</th>\n",
       "      <th>ปอง</th>\n",
       "      <th>นวมินทร์</th>\n",
       "      <th>สิบ</th>\n",
       "      <th>ห้า</th>\n",
       "      <th>แจก</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ทำ   พนักงาน      ข้าว        ดี       น้ำ        อี   เครื่อง  \\\n",
       "0  0.333333  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.166667  0.166667  0.166667  0.166667  0.166667   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.111111  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.111111  0.111111  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       เลิก        กก        งง ...   พลังงาน  คอย  แอร์ไม่  สลิป   คู  ปอง  \\\n",
       "0  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "1  0.166667  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "2  0.000000  0.111111  0.111111 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "3  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "4  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "\n",
       "   นวมินทร์  สิบ  ห้า  แจก  \n",
       "0       0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow_corpus = get_bow(idx_corpus)\n",
    "bow_corpus = get_bow(filtered_corpus)\n",
    "result, initials = predict_cluster(bow_corpus, 7, 0.3)\n",
    "bow_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]), array([59, 52, 31, 21, 13, 17, 15, 14, 12,  9,  8,  7, 11]))\n",
      "\t\tpercent\n",
      "0  |\t59\t1.0\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_clusters = label_count[0][-1] + 1\n",
    "print(label_count)\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\t' + cluster, end='')\n",
    "print('\\tpercent')\n",
    "\n",
    "for label in range(len(clusters)):\n",
    "    print(str(label) + '  |', end='')\n",
    "    \n",
    "    num_max = 0\n",
    "    for cluster in clusters:\n",
    "        loc = result[(result['label'] == cluster) & (result['predicted_label'] == label)]\n",
    "        if len(loc) > num_max:\n",
    "            num_max = len(loc)\n",
    "        print('\\t' + str(len(loc)), end='')\n",
    "    \n",
    "    print('\\t' + str(num_max / label_count[1][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014c52def3f04504a0eeea0248049452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพียว ได้แล้ว\n",
      "5 อย่าบังคับน้องเข้าประชุมเชียร์ อย่าลงโทษโดยเหตุผลงี่เง่าๆ เกิดก่อนไม่กี่ปีเอง\n",
      "20 เช็คเอาท์  หน้าจะงอไปไหนสวัสดีค่ะมีอะไรให้ช่วยมั้ยคะ\n",
      "26 โลตลาด พนงปากตลาด นินทาลคเผาขน ตะโกนโหวกเหวกข้ามหัวลค วันหวยออกสนั่นเป็นพิเศษ เห็นพฤติกรรมแล้วแย่ ปรับปรุงภาพลักษณ์เถอะ\n",
      "29 ไม่ต้องตัดแต้มเป็นคูปองเงินได้ไหมอ่ะ ไม่เคยได้ใช้เพราะ คูปองมันหายก่อน เอาแบบ พอไปซื้อแล้วแต้มครบค่อยถามว่าตัดแต้มไหมคะ อะไรแบบนี้จะดีกว่า  แล้วยังมีโบชัวมาบอกอีกนะ แต่ม เท่านี้แล้วเป็นตั๋วหนังได้ แล้วเล่นตัดไปแล้วมันเหลือแต้ม150จะทำไรได้\n",
      "31 ตะคิดเป็น 25 สต 50สต 75สต ทำไม ในเมื่อลวท้ายด้วยราคาพวกนี้ ตอนคิดเงินปัดเศษขึ้นไปเต็มบาทเลยพอเราเอาเหรียญสตางค์ที่เป็นเศษให้ ชักสีหน้า ไม่อยากรับ\n",
      "32 สาขายโสธรเปิดใหม่ พนงแอบโกง   ลูกค้ากุเอง จ่ายหลายใบแล้วทำเนียนว่ามีใบนึงชำรุดใช้ไม่ได้แล้วพอขอคืนก็สลับใบที่รูดใช้แล้วมาแทน แจ้งผู้จัดการขอดูกล้องวงจรปิดไม่ยอมให้ดูเลยต้องขู่ว่าจะแจ้งความถึงยอมคืนให้ แต่ไม่มีคำขอโทษใดๆ จาก พนงติด\n",
      "43 มารยาท คำพูดคำจา ไม่ต้องเพราะหรอกคะแค่ไม่ตะคอก ไม่ชักสีหน้าใสเวลาถามสินค้าอยู่ตรงไหนเวลาไม่เจอ ตอนนี้ก็เลือกที่จะไม่ไปใช้บริการแล้ว มี มาเปิดใกล้บ้าน สบายใจ\n",
      "57 ของป้ายเหลืองพวกผัก เน่าจนไม่รู้จะเน่ายังไง คือทิ้งๆไปก็ได้มั่ง\n",
      "59 กลิ่นของห้างค่ะ ที่สาสขาบางซื่อ คือ มันทั้งอาหาร ความชื้น ฝุ่น  แอร์สกปรกๆๆรวมกัน\n",
      "69 ราคาที่ชั้นวางสินค้า กับราคาตอนคิดเงินที่แคชเชียร์ไม่ตรงกันเจอบ่อยม๊ากกกกกกก\n",
      "71 เวลาผมซื้อเนื้อสัตว์ช่วยเอาที่คีบหรือช้อนหรืออะไรก็ได้มาวางให้ทีอย่าให้กุต้องใช้มือเปล่าๆจับอีกอย่างจะเก็บช้อนกะที่คีบเร็วไปไหนเพิ่งจะ6โมงเย็นเห็นเอาไปล้างแล้ว\n",
      "84 ฝากถึง สาขาแพร่นะคะตอนเช้าๆเลิกมาทอดโดนัทด้านนอกซะทีเถอะค่ะ กลิ่นเหม็นน้ำมันทอดติดผมเผ้าเสื้อผ้า\n",
      "91 โลตัส รัตนาธิเบศร์ แอร์เหม็นมาก เหม็นสาบหนูสุดๆ ตรงโซนอาหารแห้ง บะหมี่กึ่งสำเร็จรูป เดินผ่านก็รู้เลยมีหนู ตรงโซนข้าวสารอีก\n",
      "95 โสตัสเอ็กเพรส สาขาตลาดหนองจอก ระยอง ตู้แช่เย็นฝาตู้ปิดไม่สนิททุกตู้\n",
      "98 ไปใช้บริการหลายที่  ร้านสกปรก สินค้ามีฝุ่นเกาะ พื้นเหนียว บางทีตู้เย็นของหมด และสกปรก พนง คิดเงินช้าาาามากกกกกก ค่ะ\n",
      "101 คุยกันเสียงดังขณะคิดเงินพูดจาหยาบคายเหี้ยสัตว์ต่อหน้าลค คิดว่าเท่มั้ง\n",
      "103 พนงเคาร์เตอร์แสตนบายค่ะคิดเงินคร้าเอ๊กเพลสแถวบ้านช่วงเช้า\n",
      "104 ของเซลคือเซลเเต่หมดอายุมาหลายเดือนเเล้วยังเซลได้ด้วยหรอของกินไงบางที\n",
      "109 แมลงสาบอยู่ตามเครื่องคิดเงิน ตรงลู่วางของเลยค่ะ เจอทุกรอบ สาขาบางกะปิ\n",
      "112 วันใหนมาตั้งใจมาชื้อหมู หมูหมดเหลือไก่  วันใหนชื้อไก่ไก่หมดได้หมู  ตั้งใจอย่างได้อีกอย่าง\n",
      "117 อาหารสด ผัก หมูไก่ต่างๆ พนงด้วย สาขาสนามบินน้ำมาเก็ตพนงไม่ยิ้ม พูดจาแย่ ไม่มีสวัสดีขอบคุณเลย ผชฝากด้วยนะ\n",
      "123 ถ้ามึงไม่อยากไหว้ ไม่ต้องยกมือไหว้ก้อได้ กูรู้มึงโดนบังคับ ให้ไหว้คนที่ไม่ใช่พ่อใช่แม่ เยสเข้\n",
      "131 เอ็กเพรสนี้แลกของแล้วไม่เคยจะโทรบอกเลยหรืออะไรเลยต้องได้ทวง ชอบไม่ให้แสตมป์ เป็นไรมากไหมนี้\n",
      "133 พื้นที่โซนอาหารสดถูบ่อยๆ เถอะค่ะ พื้นเปียกมันทำให้ลื่นนะคะ แล้วมันมีรอยเท้ารอยโคลนลากไปลากมา ดูสกปรกแท้\n",
      "136 เคืองสะสมสแตมป์แลกร่มผ่านมา3ปีจนสาขาย่อยปิดไปแล้วข้ายังไม่ได้ร่มเลยหลอกลวงถามทีไรกำลังผลิตๆที่ดาวดวงใหนรึ\n",
      "140 เครื่องที่ให้แสกนของเองน่ะ อยากให้ปรับตำแหน่งให้มันแสกนจากมือถือได้ด้วยค่ะ บัตรหายไปแล้วแสกนจากมือถือไม่ได้มันติด\n",
      "147 สาขาแคราย ที่ศูนย์อาหารที่ดูดกลิ่นอาหารเสียหรอทำอาหารกันทีจามกันทั้งศูนย์อาหาร\n",
      "156 เทสโก้ โลตัส สุขุมวิท50 เมื่อไม่กี่วันก่อน หน้าเชสเตอร์ชั้น1 น้ำรั่วลงมาจากช่องแอร์ เหม็นมากแอร์ก็ไม่เย็น\n",
      "163 ไปทีไรไม่เห็นมีดอกบัวขาย\n",
      "164 อาหารฟู้ดคอร์ทรสชาติแย่มาก แถมแพงด้วย\n",
      "169 ใช้เครื่องรูดไม่เป็น ไม่รู้จัก\n",
      "171 คืออ่านจนเหนื่อยค่ะ\n",
      "177 อาหารไม่ร่อย\n",
      "178 ไม่จำเป็นไม่เข้าจริงๆ\n",
      "181 เลิกเก็บกระเป๋าตูสักที5555555\n",
      "182 แมลงสาปให้มันน้อยๆหน่อย\n",
      "192 เส้นสีเขียวยังใช้อยู่ไหม\n",
      "194 เคยใช้บริการที่โลตัสอรัญประเทศ โดยรวมแล้วดีครับ\n",
      "201 แผนกอาหารสด กลิ่นแรงมากๆ แทบไม่อยากเดินผ่านเลย\n",
      "212 ซื้อไฃ่เจอไข่เน่าเอาไข่ดีๆมาขายบางนะ\n",
      "215 ถามบัตรคลับการ์กูด้วย กูไม่อยากบอกเอง\n",
      "218 ราคาพิเศษเท่ากับราคาปกติ\n",
      "219 หนูวิ่งเป็นของสด\n",
      "220 ตัดบิล กะ ลืมยิงคลับการ์ดให้\n",
      "223 โปรโมชั่นกำกวมมากกกก\n",
      "224 มาตรฐานพลังงานต่ำมากถ้าเทียบกับเซเว่นและบิ๊กซี\n",
      "225 อยากให้ห้องนํ้ามีนํ้า\n",
      "234 แอร์ไม่เย็น\n",
      "239 คูปองไม่ต้องส่งมาที่บ้าน อยากได้ในบิลเลย\n",
      "243 ป้ายราคา ป้ายโปรโมชั่น   คือใช้ศัพท์แบบงง มากกกก\n",
      "249 ยกเลิกราคา 2550 สตางค์\n",
      "250 แล้วแต่ที่จริงๆ\n",
      "251 ป้ายราคาไม่ตรงกับสินค้า\n",
      "252 เอกเพรส ซทานสัมฤทธิ์ หน้าหงิกเป็นเล็บขบ\n",
      "255 แอร์ไม่เปิด\n",
      "256 ซื้อของห้าสิบ สลิปยาวเป็นศอก\n",
      "262 แสตมป์คลับการ์ดควรมีแบบเดิมนะ\n",
      "268 ตอนรับลูกค้า\n"
     ]
    }
   ],
   "source": [
    "comment_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_comment_widget_click(change):\n",
    "    clear_output()\n",
    "    display(comment_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['comment'].iteritems():\n",
    "        if index in initials:\n",
    "            print(\"*\", end=\"\")\n",
    "        print(index, value)\n",
    "\n",
    "comment_widget.observe(on_comment_widget_click, names='index')\n",
    "on_comment_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e199e0e74e46899924fd85c73f38a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['ดี', 'เลิก', 'น้ำ', 'เครื่อง', 'ข้าว', 'อี']\n",
      "5 ['อย่า', 'อย่า', 'กี่', 'ปี', 'บังคับ', 'น้อง', 'เชียร์', 'งี่', 'เง่า']\n",
      "20 ['หน้า', 'เช็ค', 'งอ', 'สวัสดี']\n",
      "26 ['ปรับปรุง', 'พนง', 'แย่', 'ตะโกน', 'ข้าม', 'หวย', 'พิเศษ', 'พฤติกรรม']\n",
      "29 ['เงิน', 'ซื้อ', 'ดี', 'ถาม', 'อ่ะ', 'หาย', 'ไหม', 'ไหม', 'เล่น']\n",
      "31 ['เงิน', 'ราคา', 'ตอน', 'สีหน้า', 'เต็ม', 'บาท', 'เศษ', 'สตางค์', 'เศษ']\n",
      "32 ['ลูกค้า', 'จ่าย', 'พนง', 'พนง', 'ใบ', 'ใบ', 'ใบ', 'ติด', 'นึง']\n",
      "43 ['เวลา', 'เวลา', 'บริการ', 'สินค้า', 'ถาม', 'มารยาท', 'เจอ', 'ตอน', 'บ้าน']\n",
      "57 ['รู้', 'ผัก', 'เน่า', 'เน่า', 'ทิ้ง', 'ป้ายเหลือง']\n",
      "59 ['ห้าง', 'อาหาร', 'กลิ่น', 'ฝุ่น']\n",
      "69 ['เงิน', 'สินค้า', 'ราคา', 'ราคา', 'ตอน', 'แคชเชียร์', 'วาง', 'ชั้น']\n",
      "71 ['ซื้อ', 'เวลา', 'ผม', 'กะ', 'อย่า', 'วาง', 'มือ', 'เย็น', 'ล้าง']\n",
      "84 ['สาขา', 'ตอน', 'เลิก', 'ติด', 'ผม', 'กลิ่น', 'เหม็น', 'ฝาก', 'เช้า']\n",
      "91 ['เดิน', 'รู้', 'อาหาร', 'เหม็น', 'ข้าว', 'หนู', 'หนู', 'โซน', 'แห้ง']\n",
      "95 ['สาขา', 'เย็น', 'ตู้', 'ตู้', 'ตู้']\n",
      "98 ['เงิน', 'บริการ', 'สินค้า', 'พนง', 'ร้าน', 'เย็น', 'สกปรก', 'สกปรก', 'พื้น']\n",
      "101 ['หน้า', 'เงิน', 'คุย', 'พูดจา', 'เสียง', 'เหี้ย', 'สัตว์', 'ลค']\n",
      "103 ['เงิน', 'บ้าน', 'แถว', 'เช้า']\n",
      "104 ['กิน', 'เดือน', 'หรอ', 'เเต่', 'อายุ', 'เเล้ว']\n",
      "109 ['เงิน', 'เจอ', 'เครื่อง', 'วาง', 'รอบ']\n",
      "112 ['หมู', 'หมู', 'หมู', 'ไก่', 'ไก่', 'ไก่', 'ใหน', 'ตั้งใจ', 'ชื้อ']\n",
      "117 ['สาขา', 'พนง', 'พนง', 'แย่', 'อาหาร', 'พูดจา', 'น้ำ', 'ผัก', 'สด']\n",
      "123 ['คน', 'รู้', 'โดน', 'มือ', 'ก้อ', 'บังคับ', 'ไหว้', 'ไหว้', 'ไหว้']\n",
      "131 ['ชอบ', 'เอ็กเพรส', 'แลก', 'ไหม', 'แสตมป์', 'โทร']\n",
      "133 ['ทำ', 'อาหาร', 'ดู', 'สด', 'สกปรก', 'พื้น', 'พื้นที่', 'โซน', 'ลาก']\n",
      "136 ['สาขา', 'ถาม', 'แลก', 'ปี', 'สะสม', 'ร่ม', 'ร่ม', 'ใหน']\n",
      "140 ['ติด', 'บัตร', 'เครื่อง', 'หาย']\n",
      "147 ['สาขา', 'ทำ', 'อาหาร', 'อาหาร', 'อาหาร', 'กลิ่น', 'ศูนย์อาหาร']\n",
      "156 ['หน้า', 'น้ำ', 'เย็น', 'กี่', 'เหม็น', 'ชั้น', 'แอร์']\n",
      "163 ['ขาย', 'ดอก']\n",
      "164 ['แย่', 'อาหาร', 'แถม']\n",
      "169 []\n",
      "171 ['เหนื่อย']\n",
      "177 ['อาหาร']\n",
      "178 []\n",
      "181 ['เลิก', 'สัก', 'ตู']\n",
      "182 []\n",
      "192 ['ไหม', 'สี', 'เขียว']\n",
      "194 ['ดี', 'บริการ']\n",
      "201 ['เดิน', 'อาหาร', 'แทบ', 'สด', 'กลิ่น', 'แรง', 'แผนก']\n",
      "212 ['ซื้อ', 'ดี', 'เจอ', 'ขาย', 'เน่า']\n",
      "215 ['ถาม', 'บัตร', 'คลับ']\n",
      "218 ['ราคา', 'ราคา', 'ปกติ', 'พิเศษ']\n",
      "219 ['สด', 'วิ่ง', 'หนู']\n",
      "220 ['กะ', 'การ์ด', 'ยิง', 'คลับ']\n",
      "223 ['โปรโมชั่น', 'กกก']\n",
      "224 ['เซเว่น', 'พลังงาน', 'ต่ำ']\n",
      "225 []\n",
      "234 ['เย็น', 'แอร์ไม่']\n",
      "239 ['บ้าน', 'บิล', 'คู', 'ปอง']\n",
      "243 ['ราคา', 'ป้าย', 'ป้าย', 'โปรโมชั่น', 'งง', 'กกก']\n",
      "249 ['ราคา', 'สตางค์']\n",
      "250 []\n",
      "251 ['สินค้า', 'ราคา', 'ป้าย']\n",
      "252 ['หน้า', 'หงิก', 'เอกเพรส', 'เล็บ']\n",
      "255 ['แอร์ไม่']\n",
      "256 ['ซื้อ', 'สิบ', 'ห้า', 'สลิป']\n",
      "262 ['การ์ด', 'แสตมป์', 'คลับ']\n",
      "268 ['ลูกค้า', 'ตอน']\n"
     ]
    }
   ],
   "source": [
    "token_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_token_widget_click(change):\n",
    "    clear_output()\n",
    "    display(token_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['tokenized_comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "token_widget.observe(on_token_widget_click, names='index')\n",
    "on_token_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17407766]]\n",
      "['แย่', 'อาหาร', 'แถม']\n",
      "['เดิน', 'รู้', 'อาหาร', 'เหม็น', 'ข้าว', 'หนู', 'หนู', 'โซน', 'แห้ง']\n"
     ]
    }
   ],
   "source": [
    "seed = 164\n",
    "compare = 91\n",
    "\n",
    "a = numpy.array(bow_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(bow_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "print(filtered_corpus[seed])\n",
    "print(filtered_corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
