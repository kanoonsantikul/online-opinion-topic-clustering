{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 353\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TrueCoffee.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 953 words\n",
      "filter frequent words: 374 words\n",
      "filter letter words: 372 words\n",
      "filter stop words: 221 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "average_doc_size\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "\n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    cluster_num = 0\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            points.remove(seed)\n",
    "    \n",
    "    while cluster_num != 0:\n",
    "        cluster = [numpy.array(bow_corpus.iloc[i]) for i, label in enumerate(labels) if label == cluster_num]\n",
    "        eps_temp = eps\n",
    "        \n",
    "        while True:\n",
    "            centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "            eps_temp -= delta_eps\n",
    "            \n",
    "            count = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                point = numpy.array(bow_corpus.iloc[i]).reshape(1, -1)\n",
    "                if label == 0 and cosine_similarity(centroid, point) >= eps_temp:\n",
    "                    cluster.append(point[0])\n",
    "                    labels[i] = cluster_num\n",
    "                    count += 1\n",
    "            if count == 0:\n",
    "                break\n",
    "        \n",
    "        cluster_num -= 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(bow_corpus, min_samples, eps):\n",
    "    predicted_labels = sdc(bow_corpus, min_samples, eps)\n",
    "\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>กาแฟ</th>\n",
       "      <th>ชง</th>\n",
       "      <th>ร้าน</th>\n",
       "      <th>สวย</th>\n",
       "      <th>ทรูมูฟ</th>\n",
       "      <th>ลูกค้า</th>\n",
       "      <th>สิทธิ์</th>\n",
       "      <th>เต็ม</th>\n",
       "      <th>โปร</th>\n",
       "      <th>รสชาติ</th>\n",
       "      <th>...</th>\n",
       "      <th>แถว</th>\n",
       "      <th>ชัด</th>\n",
       "      <th>บริการ</th>\n",
       "      <th>โปรโมชั่น</th>\n",
       "      <th>ชม</th>\n",
       "      <th>เวลา</th>\n",
       "      <th>ชาไทย</th>\n",
       "      <th>หวัง</th>\n",
       "      <th>เข้มข้น</th>\n",
       "      <th>ดืม</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   กาแฟ   ชง  ร้าน  สวย  ทรูมูฟ  ลูกค้า  สิทธิ์  เต็ม  โปร    รสชาติ ...   \\\n",
       "0   0.2  0.2   0.2  0.4     0.0     0.0     0.0   0.0  0.0  0.000000 ...    \n",
       "1   0.0  0.0   0.0  0.0     0.2     0.2     0.2   0.2  0.2  0.000000 ...    \n",
       "2   0.0  0.0   0.0  0.0     0.0     0.0     0.0   0.0  0.0  0.333333 ...    \n",
       "3   0.0  0.0   0.0  0.0     0.0     0.0     0.0   0.0  0.0  0.000000 ...    \n",
       "4   0.4  0.0   0.2  0.0     0.0     0.0     0.0   0.0  0.0  0.000000 ...    \n",
       "\n",
       "   แถว  ชัด  บริการ  โปรโมชั่น   ชม  เวลา  ชาไทย  หวัง  เข้มข้น  ดืม  \n",
       "0  0.0  0.0     0.0        0.0  0.0   0.0    0.0   0.0      0.0  0.0  \n",
       "1  0.0  0.0     0.0        0.0  0.0   0.0    0.0   0.0      0.0  0.0  \n",
       "2  0.0  0.0     0.0        0.0  0.0   0.0    0.0   0.0      0.0  0.0  \n",
       "3  0.0  0.0     0.0        0.0  0.0   0.0    0.0   0.0      0.0  0.0  \n",
       "4  0.0  0.0     0.0        0.0  0.0   0.0    0.0   0.0      0.0  0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow_corpus = get_bow(idx_corpus)\n",
    "bow_corpus = get_bow(filtered_corpus)\n",
    "result = predict_cluster(bow_corpus, 5, 0.2)\n",
    "bow_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]), array([22, 11,  5,  1,  3, 22, 23,  4, 16,  5, 14, 24,  4,  6,  2,  6,  4,\n",
      "        4,  6,  7,  6,  8, 14, 14,  7,  5,  7,  9,  5, 30,  5, 26, 21,  7]))\n",
      "\t\tpercent\n",
      "0  |\t22\t1.0\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_clusters = len(label_count[0])\n",
    "print(label_count)\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\t' + cluster, end='')\n",
    "print('\\tpercent')\n",
    "\n",
    "for label in range(len(clusters)):\n",
    "    print(str(label) + '  |', end='')\n",
    "    \n",
    "    num_max = 0\n",
    "    for cluster in clusters:\n",
    "        loc = result[(result['label'] == cluster) & (result['predicted_label'] == label)]\n",
    "        if len(loc) > num_max:\n",
    "            num_max = len(loc)\n",
    "        print('\\t' + str(len(loc)), end='')\n",
    "    \n",
    "    print('\\t' + str(num_max / label_count[1][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df4e3de45c433c9816d6d0ae99cf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 ทุกสูตรที่มี\n",
      "111 เนทกาก ฝนตกปรอยๆทีวีบ้านกุดูไม่ได้ละ  นะเห้ย\n",
      "124 แพงเกอน แต่รสชาติเอิ่มมมม\n",
      "125 คุณเมิงจะครอบคลุมทุกอย่างเลยแม่นบ่ 555\n",
      "128 ความเป็นออริจินอล\n",
      "133 พนงหน้าตูด สาขาเซ้นทรัลพระราม2\n",
      "139 เครือข่าย ละกัน 555\n",
      "151 จางมากกกกก ไม่โอเครเลย\n",
      "155 ไม่เคยทาน\n",
      "164 อันนี้ก็ไม่ต่างจากสตาร์บั๊ค\n",
      "181 ไม่ค่อยมีคลื่น\n",
      "208 ลาเต้เย็นควรใส่นมเยอะๆนะค่ะ นี่ขมอย่ากะอเมริกาโน่\n",
      "228 แพงเกิ้นนน\n",
      "242 เมื่อไหร่จะมี\n",
      "259 รสชาดห่วยสุดในสามโลก\n",
      "265 ครั้งเดียวก็เกินจะพอ\n",
      "266 อันนี้มาแนวเดียวกะแมคคาเฟ่\n",
      "293 โลโก้\n",
      "306 กระจอกมาก\n",
      "319 สาขาน้อย\n",
      "331 มาอ่านคอมเม้น 5555555555555555\n",
      "349 หมาไม่ดืมว่ะ\n"
     ]
    }
   ],
   "source": [
    "comment_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_comment_widget_click(change):\n",
    "    clear_output()\n",
    "    display(comment_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "comment_widget.observe(on_comment_widget_click, names='index')\n",
    "on_comment_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46ac2904da64ae2a12468557e1fe0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 []\n",
      "111 []\n",
      "124 ['รส', 'ชาติ']\n",
      "125 []\n",
      "128 []\n",
      "133 ['พนง']\n",
      "139 []\n",
      "151 ['จาง']\n",
      "155 ['ทาน']\n",
      "164 []\n",
      "181 []\n",
      "208 ['เย็น', 'ใส่', 'นม', 'ขม']\n",
      "228 []\n",
      "242 ['ไหร่']\n",
      "259 ['รส']\n",
      "265 []\n",
      "266 []\n",
      "293 []\n",
      "306 []\n",
      "319 []\n",
      "331 []\n",
      "349 ['ดืม']\n"
     ]
    }
   ],
   "source": [
    "token_widget = widgets.ToggleButtons(\n",
    "    options=[num + 1 for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_token_widget_click(change):\n",
    "    clear_output()\n",
    "    display(token_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['tokenized_comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "token_widget.observe(on_token_widget_click, names='index')\n",
    "on_token_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
