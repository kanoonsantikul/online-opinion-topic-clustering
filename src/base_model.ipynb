{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus\n",
    "\n",
    "from model.new_sdc import NewSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 268\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus'\n",
    "\n",
    "corpus, labels = load_corpus('../data/' + file_name + '.txt')\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/tokenized/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1449 words\n",
      "filter frequent words: 605 words\n",
      "filter letter words: 604 words\n",
      "filter stop words: 403 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.extend(['นี้'])\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(idx_corpus)]\n",
    "model = Doc2Vec(tagged_corpus, vector_size=average_doc_size, window=4, min_count=2, epochs=100)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "paragraph_vectors = [model.infer_vector(doc) for doc in idx_corpus]\n",
    "paragraph_vectors = pandas.DataFrame(paragraph_vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(corpus, weight):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if weight == 'normal':\n",
    "        weight_corpus = bow_corpus\n",
    "    elif weight == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        weight_corpus = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(weight_corpus):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if weight == 'normal' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(onehot_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(onehot_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(onehot_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.32\n",
    "expand_rate = 0.05\n",
    "epoch = 15\n",
    "\n",
    "onehot_corpus = get_onehot(idx_corpus, 'normal')\n",
    "# onehot_corpus = get_onehot(filtered_corpus, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196.09866662549476\n",
      "[51 13 47 19  7 15 12 10 19 24 26  8 10 14 30 10 11  7 10  7]\n"
     ]
    }
   ],
   "source": [
    "max_compactness = 0\n",
    "for i in range(epoch):\n",
    "    model = NewSDC()\n",
    "    _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate)\n",
    "    \n",
    "#     marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "#     model = KMeans(n_clusters=14).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(onehot_corpus, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "num_cluster = len(label_count)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = None \n",
    "prev_label_count = None\n",
    "while True:\n",
    "    model = NewSDC()\n",
    "    predicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, expand_rate, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(onehot_corpus, result)\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    if numpy.array_equal(label_count, prev_label_count):\n",
    "        break\n",
    "    prev_label_count = label_count\n",
    "    centroids = centroids[1:]\n",
    "    \n",
    "    print(compactness)\n",
    "    print(label_count)\n",
    "num_cluster = len(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "17 14 0.3469191623179261\n",
      "14 13 0.35747313686377363\n",
      "13 11 0.7625572195447531\n",
      "10 4 0.34222424468593116\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 11, 12, 11, 11, 15, 16, 11, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.90348922297676\n",
      "[72 30 75 20 16 16 11  8 12  8]\n"
     ]
    }
   ],
   "source": [
    "# result.to_csv('../data/results/new_sdc/' + file_name + '.csv')\n",
    "\n",
    "result = pandas.read_csv('../data/results/new_sdc_em/' + file_name + '.csv')\n",
    "\n",
    "print(eval_cluster(onehot_corpus, result)[0])\n",
    "print(numpy.unique(result['predicted_label'], return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fcb69fad904ca1ad22d8ea51aa9232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 18, 19), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ซื้อลิขสิทธิ์ UCL มาด้วยนะ จ่ายรายเดือนไป แต่ไม่มีให้ดู\n",
      "17 Wi-Fi ทรูที่บ้าน ใช้ได้แต่กลางคืน กลางวันใช้ไม่ได้ แต่จ่ายพันกว่าบาททุกเดือน ควรปรับปรุง ฝากทรูคอฟฟี่ไปบอกด้วย\n",
      "21 ทำไมเอาเมนูทวิซที่ปั่นใส่ขนมช็อกโกแลตทวิซคาลาเมลออก 💥🔥🔥🔥โกรธหนักค่ะ เพราะชอบมากกกกกกกก\n",
      "28 ขอทำขนมไปส่งได้ไหมคะ นักเรียนบ้านยากจนจะสอนนักเรียนแต่ไม่มีที่ให้โอกาสนักเรียนคะ แต่รับรองขนมอร่อยคะ\n",
      "42 ตัวอักษรใหญ่ๆ มองเมนูไม่ชัด ไม่กล้าสั่ง กลัวสั่งผิดละเขินน 555+\n",
      "45 บอกพนักงงานชงกาแฟทำหน้าตาดีๆ เลิกกระแทกแก้วสักที เวลาส่งคืนลูกค้าเยอะๆอ่ะ ละก็ช่วยทำคอมเม้นลูกค้าให้มันเป็นตามที่ขอด้วยอิเวน\n",
      "52 ตอนปรับโปรใหม่เดือนเเรกเนตอย่างเเรง ผ่านไปสักพัก ช้าโคตร ส่วนกาเเฟงั้นๆ ราคาเเพงไป\n",
      "54 พนง.น่ารักๆ เยอะนะ แต่เพราะชื่อ ทรู นี่แหละตรูเลยไม่เข้า 5555\n",
      "57 เป็นร้านที่แดกกาแฟร้อนทีไร แม่งต้องล้วกลิ้นไหม้เป็นแผลตลอด น้ำร้อนเอ็งจะร้อนไปไหนวะ สงสัยแม่งเอามาจากบ่อในนรกชัวร์ๆ ถ้าเอ็งร้อนแบบนี้\n",
      "70 Iced Black Tea Latte หาชงอร่อยยากมาก อยากกินทีต้องลุ้นทีว่ามือชงจะทำได้แค่ไหน เข้าร้านทรูเพื่อการนี้โดยเฉพาะ เมื่อไม่เจอมือเซียนชง ก็ไม่เข้าไปเหยียบร้านค่ะ\n",
      "83 ใช้ค่ายโทรศัพ์ ใช้ wifi โปรแบบแพงๆ มีสิทแบคการ์ดถือไว้หลายปีสิทธ์เต็มตลอดทั้งดูหนังทั้งเครื่องดื่ม สุดท้ายย้ายทิ้งหมดเพราะเรื่องสิทธ์ดูหนังลดราคาเครื่องดื่มนี่แหละ\n",
      "91 ช่วยแจ้งโปรที่ใช้ได้จริง พอไป อะไรก็เต็ม “หวานน้อย “ ของแต่ละสาขาความหวานไม่เท่ากัน หวานน้อยคือ หวาน และ ว๊านหวาน , บอกไม่หวานคือน้ำล้างแก้ว แม้แต่กลิ่นกาแฟยังจาง...\n",
      "93 เนตช้ามากเลยค่ะเดี๋ยวนี้ ขนาดรอบบิลเพิ่งตัด เนต20GB หมุนแล้วหมุนอีก\n",
      "98 คำว่าหวานน้อยไม่มีอยู่จริง\n",
      "104 อยากให้ทำโปรเยอะกว่านี้ 🙏\n",
      "105 ทุกสูตรที่มี!!\n",
      "107 เนทกาก ฝนตกปรอยๆทีวีบ้านกุดูไม่ได้ละ Platinum นะเห้ย\n",
      "112 สัญญาณไม่ลื่นเลย เล่น ROV กระตุก\n",
      "117 แพงเกอน แต่รสชาติเอิ่มมมม\n",
      "118 คุณเมิงจะครอบคลุมทุกอย่างเลยแม่นบ่? 555 😂\n",
      "121 ความเป็นออริจินอล\n",
      "126 พนง.หน้าตูด (สาขาเซ้นทรัลพระราม2)\n",
      "134 เครือข่าย ละกัน 555\n",
      "146 americano จางมากกกกก ไม่โอเครเลย\n",
      "151 ไม่เคยทาน\n",
      "160 ไปเรียนมาใหม่\n",
      "161 อันนี้ก็ไม่ต่างจากสตาร์บั๊ค\n",
      "165 เราลูกค้าทรู แต่เราไม่เคยได้ใช้สิทธิทรู ในการซื้อทรูคอฟฟี่ เลยยยยยย\n",
      "179 ไม่ค่อยมีคลื่น\n",
      "191 สำหรับผม โฮจิชา ขอให้มีขายตลอดปีพอ ปลื้มมาก\n",
      "206 ลาเต้เย็นควรใส่นมเยอะๆนะค่ะ นี่ขมอย่ากะอเมริกาโน่\n",
      "216 ปรับปรุงเรื่องราคา กับ ฝีมือการชงของ พนง. ค่ะ 😂\n",
      "226 แพงเกิ้นนน\n",
      "230 ไล่ลูกค้านั่งติวนาน(สาด) ที่พาราก้อน\n",
      "231 ไม่กล้าเข้า เราใช้ AIS. 😂\n",
      "235 เหลือแก้วละ50-60จะเป็นช้อยแรกเลยตัด starbuck ทิ้งไป\n",
      "240 เมื่อไหร่จะมี​  dtaccoffee  aiscoffee\n",
      "255 wifi บ้าน กากมาก หมุนติ้วๆๆ\n",
      "257 รสชาดห่วยสุดในสามโลก\n",
      "263 ครั้งเดียวก็เกินจะพอ\n",
      "264 อันนี้มาแนวเดียวกะแมคคาเฟ่\n",
      "277 ชามะนาว ชาเฝื่อนมากเลยงะ\n",
      "287 ทำไมรู้สึกว่าชาเย็นที่นี่จะออกเค็มๆหน่อย\n",
      "291 โลโก้\n",
      "303 กระจอกมาก\n",
      "316 สาขาน้อย\n",
      "318 เปลี่ยนชื่อเมนู จากชานมเย็น เป็น ชาไทย\n",
      "331 +1\n",
      "337 ชอบ\n",
      "345 หมาไม่ดืมว่ะ !\n",
      "347 สัญญาณอินเตอร์เน็ตครับ\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8eaa2a70824e70b6345ba90a46b1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=3, options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), value=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*3 คิว เปิดช่องเยอะๆหน่อยค่ะ คนไหนใช้ช่องทางด่วน ผิดกติกา จะยอมช่วยเห็นใจคนทำถูกกติกาด้วยจ้า\n",
      "*10 ทุกคนล้วนแล้ว แต่มีหน้าที่ของเขา คนเราควรให้โอกาสซึ่งกันและกันนะครับให้คิดว่าคนดีก็มี คนไม่ดีก็มีนะครับ ผมไม่ได้โลกสวยนะครับ แต่ผมก็ทำงานในด้านบริการเหมือนกัน ถ้าไม่มายืนตรงจุดที่เขายืนคุณก็จะไม่รู้ คิดบวกนะครับ พนักงานเหมือนกันนะครับ ปรับปรุงตรงไหนแล้วดี ก็ควรที่ต้องปรับปรุงแก้ไขส่วนที่ผิดพลาดของตัวเรา ดีแล้วยังดีได้อีกนะครับ Mc.Tom\n",
      "*17 เทสโก้โลตัสสาขาบางกะปิทีวีตัวนี้อยู่บริเวณหน้าร้าน Treephaphan Spa ชั้นลอย(ชั้นเดียวกับไปรษณีย์)ใกล้ประตูออกลานจอดรถวันนั้นผมเดินชนเต็มๆ และผมเชื่อว่าต้องมีลูกค้าท่านอื่นเดินชนทีวีตัวนี้มาแล้วหลายครั้ง มันไม่ควรอยู่ตรงนี้นะครับเพราะนี่คือทางเดิน และมันก็ไม่ได้ชิดเข้ามุมกำแพงเลย เดินเลี้ยวขวาจากทีวีตัวนี้ไปก็ลงไปชั้นG ซึ่งมันเป็นทางหลักที่คนเข็นรถลงจากซุปเปอร์ฯใช้ประจำนะครับ ไม่ใช่ทางลับที่ไม่มีคนเดินผ่าน เพดานชั้นนี้ก็ค่อนข้างต่ำอยู่แล้วแถมเอาทีวีตัวนี้มาติด ต้องคนที่มีความสูง150ลงไปเท่านั้นแหล่ะครับที่จะเดินลอดผ่านได้ (ผมสูง175)ผมเชื่อว่าทางตัวเทสโก้โลตัสยึดมั่นในการบริการที่ดีและอำนวยความสะดวกต่อลูกค้าที่มาใช้บริการ แต่ทีวีตัวนี้ ***ไม่อำนวยความสะดวก*** แถมยังทำผมเจ็บตัวอีก ผมเข้าใจว่าผมเองในฐานะลูกค้าก็ควรระมัดระวังตัวเองให้มากกว่านี้อยู่แล้ว แต่สถานที่ก็ควรมีการจัดการที่ดี สะดวก ปลอดภัยร่วมด้วยนะครับหวังว่าจะได้รับการแก้ไขนะครับ เพื่อที่จะได้ไม่มีใครเจ็บตัวอีก ขอบคุณ!\n",
      "*20 พนักงานเอ็กเพรส บางสาขาหน้าบูดมาก  และทำงานช้ามากกกกกกถามอะไร ไม่ค่อยรู้เรื่อง แก้ไขสถานการณ์เฉพาะหน้าได้ไม่ดี ฟีลแบบ ฉันยืนรอจ่ายตังค์ มีคนยืนเคาเตอร์ 3 คน เปิดแค่เลนเดียว สองคนยืนคุยกัน รุ่นน้องทำงานคนเดียวอะไรเงี๊ย ลูกค้าจะรอ 4-5 คนก็รอไปเถอะ น่ารำคาญมากบางทีไอ้เราก็รอได้นะ แต่นึกโมโหแทนเด็กใหม่อะ โอ๊ยหนู... โดนเอาเปรียบแบบน้ำท่วมปาก พูดอะไรก็ไม่ได้ นี่ลูกค้าแบกของหนักๆรองี้ ไม่มีท่าทีว่าจะรีบกับเรา\n",
      "*27 โลตัสเอ็กซ์​เพลสสาขายางตลาด.. จ.กาฬสินธุ์​ค่ะแย่มากวันนั้นไปซื้อแพมเพิสให้ลูกยืนรอคิดเงินมันไม่คิดให้ทั้งที่ไม่มีลุกค้ามันยืนคุยกันสองคนหยอกล้อไปมามองหน้าเราเฉยเลยไม่พุดอะไรสักคำยืนรอ3นาทีได้ค่ะตอนนั้นโมโหมากว่าจะด่าเหมือนกันเลยไม่ทำ​  เดินมาอีกช่องนึงเขาก็คิดเงินให้ประเด็น​คือ1มันไม่ใส่ใจลุกค้าถึงจะซื้อชิ้นเดียวก็เถอะ2ถ้าเครื่องไม่พร้อมใช้งานมันน่าจะบอกนี่มันยืนยุ2คน2เคาเตอร์​คิดออกมั้ยเวลายืนคิดเงินเคาเตอร์​มันหันหลังชนกันนี่มันเล่นหันหน้ามาคุยกันหยอกล้อกันไม่คิดเงินไม่สนใจเรายืนรอตั้งนานสุดท้ายก็เลยเดินไปช่องอื่นเสียความรุ้สึกมาก\n",
      "*33 ช่องจ่ายเงินรื้ออกบ้างก็ได้ถ้าไม่มีคนทำ วันเสาร์อาทิตย์รอนานโคตร\n",
      "*38 สาขาใหญ่ๆ คนเยอะต่อแถวจ่ายเงินช้า ช่องเยอะแต่ไม่เปิดทำมาเพื่อ???\n",
      "*39 สาขามหาชัย(คลองครุ) พนง.แคชเชียร์ชอบคุยกันข้ามหัวลูกค้า แถมคิดเงินผิดโดยการยิงสินค้าซ้ำกัน 2 ครั้ง ดีนะที่เช็คบิลล์ใบเสร็จก่อนเดินออกจากห้าง นี้เลยรีบไปที่จุดบริการเพื่อทำการรีฟันเงินคืน และต่อว่าพนง.แคชเชียร์หน่อยนึง (ปล. คิดเงินเกินกับสาขานี้มา 3 ครั้งแล้ว ) 👎👎👎\n",
      "*52 โลตัส Express แทบทุกสาขา.....แคชเชียร์ไม่ค่อยมีคนยืนที่เคาเตอร์ มัวแต่ไปจัดของ  เวลาซื้อของต้องรอ พนง.จัดสต๊อกให้เสร็จก่อนอ่ะค่ะทำไมผู้จัดการไม่แบ่งงานกันดีๆคะคือรอนาน แถวยาวว ไม่เหมือนเซเว่น มี พนง ยืนเคาเตอร์ตลอด\n",
      "*54 ใช่งานเยอะมากกะนึง2คน งานเป็น100อย่าง พอลูกค้าเยอะ งานอื่นก็ไม่ได้ทำ กะที่ต่อกะก็หาว่าไม่ทำอะไรอีก เหนื่อยใจกับการบริหารงาน โลตัสเอ็กเพรส ค่าจ้างวันละ300 เเต่ทำงานยังกะค่าจ้างวัน1000เกลียดสุดกะไอตอนที่ มีช่างมาซ่อมอะไรนิดอะไรหน่อย ต้องเเจ้งต้องรายงาน เห้อ ปล่อยๆบ้าก็ได้ เอกสารก็มีกำกับกล้องก็มี\n",
      "*86 เปลี่ยนคนดูแลพื้นที่ค่ะมาใหม้ย้าอำนาจไม่ฟังคนอิ่นเอาแต่ความคิดของตัวเองเปนใหญ่ถูกผิดกุไม่สนขอแค่กุถูกคนเดวพอ\n",
      "*92 นี่เคยโดนตอนเด็กประมาณ ป.3-ป.4 พนง.โลตัสสาขาซ.12พัฒนานิคม เป็นผญ.แกล้งแบบแรงมากๆใช้คำพูดขมขู่ตลอดแบบ เห้ยมาทำไม ใครให้มา ชอบหัวเระเยาะแล้วก็แกล้งหลายๆอย่าง ตอนนั้นจะหยิบใบรายงานพฤติกรรม พนง.คนนั้นบอกจะรายงานใคร! จะทำไรถ้ามึงเขียนเดี๋ยวเจอกู แล้วคือพนง.คนอื่นไม่ช่วยแถมหัวเราะเยาะ ไม่รู้ตอนนี้ออกยังนะ เห็นล่าสุด2-3เดือนที่แล้ว อยากให้อบรมนิสัยสักนิดก่อนรับทำงานค่ะ\n",
      "*111 เอ็กเพรส  เวลาจัดโปรสินค้าราคาถูก แต่พอจ่ายเงินจริงๆระบบคิดเงินราคาปกติ...ต้องมาดูบิลเอาเอง..ตอนทำเงินคืนลูกค้าต้องมารอเสียเวลาอีก\n",
      "*130 แคชเชียร์ค่ะ ช่วงเวลา21.00-22.00 ช่วงเวลาที่ห้างใกล้ปิด แล้วคนจะจ่ายของเยอะมาก ต่อแถวยาวววววว เพราะแคชเชียร์เปิดประมาณ3-4แคชเชียร์ ทั้งที่มีเป็น10 ทำให้จ่ายเงินได้ล่าช้าามากกกกค่ะอยากให้เปิดเยอะกว่านี้หน่อยต่อแถวนานเกิ๊น\n",
      "*133 โลตัสใหญ่สระบุรีคนซื้อเยอะมากเครื่องpostเยอะนะแต่เปิดไม่กี่ช่อง เวลาไปซื้อแถวยาวมาก\n",
      "*147 เวลามีคนมาตรวจห้าง อะไรซักอย่างนี่แหละ พนง.วิ่งกันโกลาหล โดยเฉพาะ หน.พนง  หญ.\n",
      "*164 90%หน้าหงิกทุกคน\n",
      "*169 แต่ละคนที่มาเม้นนี้ เคยทำโลตัสทั้งน้านนน😁😂😂\n",
      "*195 เคาว์เตอร์คิดเงินเยอะแต่ไม่มีคนคิดเงิน\n",
      "*211 โทรกลับด้วยเวลาคนรับสมัครงานอ่า#โลตัสสาขานวนคร\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c97b617a2904a84bd215229d345ca0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=17, options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18), value=17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*20 ['เหมือน', 'สาขา', 'รสชาติ', 'เหมือน', 'คน', 'ทำ', 'กาแฟ', 'รสชาติ', 'ลูกค้า', 'ทรู', 'สิทธิ์', 'ลด', 'พิเศษ']\n",
      "*71 ['ปรับปรุง', 'เรื่อง', 'สัญญาน']\n",
      "*100 ['ปรับปรุง', 'สัญญาน', 'มือถือ']\n",
      "*164 ['รสชาติ', 'งั้น']\n",
      "*166 ['สิทธิ์', 'รสชาติ', 'งั้น']\n",
      "*180 ['รู้', 'ปรับปรุง', 'แดก']\n",
      "*198 ['ปรับปรุง']\n",
      "*200 ['ปรับปรุง', 'เรื่อง', 'อินเตอร์เน็ต']\n",
      "@234 ['ปรับปรุง', 'รสชาติ']\n",
      "*273 ['กาแฟ', 'รสชาติ', 'เหมือน']\n",
      "*313 ['ปรับปรุง', 'เวลา', 'นั่ง', 'ร้าน']\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['โลตัส', 'สาขา', 'จ้าง', 'พนักงาน', 'พนักงาน', 'แผนก', 'คน', 'หน้าที่', 'ตัว', 'รับผิดชอบ', 'แคชเชียร์', 'งาน', 'แผนก', 'ตัว', 'เละ', 'โดน', 'ด่า', 'โดน', 'ด่า', 'เครียด', 'หน้า', 'ใส่', 'ลูกค้า', 'ทำ', 'งาน', 'บริการ', 'ลูกค้า', 'ดี', 'ลูกค้า', 'รัก', 'แนะนำ', 'ใจ', 'เด็ก', 'โลตัส', 'เก่า']\n",
      "โลตัสไฮเปอร์มาเก็ต​ ทุกสาขา​ ควรจ้างพนักงานเยอะกว่านี้​ ไม่ควรเรียกพนักงานแผนกอื่นไปลงเครื่องแคชเชียร์​บ่อยจนเกินไป​ ทุกคนต้องมีหน้าที่ที่ตัวเองต้องรับผิดชอบ​ พอช่วยแคชเชียร์เสร็จ​ งานแผนกตัวเองเละก็โดนด่า​ พอโดนด่าก็เกิดความเครียด​ หน้าบึ้งตึงใส่ลูกค้า​ ทำงานไม่มีความสุข​ บริการลูกค้าไม่ดี​ แล้วลูกค้าก็ร้องเรียน​ เพราะรักจึงอยากแนะนำ​ จากใจเด็กโลตัสเก่า​\n",
      "['โลตัส', 'สาขา', 'จ้าง', 'พนักงาน', 'พนักงาน', 'แผนก', 'คน', 'หน้าที่', 'ตัว', 'รับผิดชอบ', 'แคชเชียร์', 'งาน', 'แผนก', 'ตัว', 'เละ', 'โดน', 'ด่า', 'โดน', 'ด่า', 'เครียด', 'หน้า', 'ใส่', 'ลูกค้า', 'ทำ', 'งาน', 'บริการ', 'ลูกค้า', 'ดี', 'ลูกค้า', 'รัก', 'แนะนำ', 'ใจ', 'เด็ก', 'โลตัส', 'เก่า']\n",
      "โลตัสไฮเปอร์มาเก็ต​ ทุกสาขา​ ควรจ้างพนักงานเยอะกว่านี้​ ไม่ควรเรียกพนักงานแผนกอื่นไปลงเครื่องแคชเชียร์​บ่อยจนเกินไป​ ทุกคนต้องมีหน้าที่ที่ตัวเองต้องรับผิดชอบ​ พอช่วยแคชเชียร์เสร็จ​ งานแผนกตัวเองเละก็โดนด่า​ พอโดนด่าก็เกิดความเครียด​ หน้าบึ้งตึงใส่ลูกค้า​ ทำงานไม่มีความสุข​ บริการลูกค้าไม่ดี​ แล้วลูกค้าก็ร้องเรียน​ เพราะรักจึงอยากแนะนำ​ จากใจเด็กโลตัสเก่า​\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(onehot_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(onehot_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
