{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 153\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'จบข่าว - ใบขับขี่.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 616 words\n",
      "filter frequent words: 263 words\n",
      "filter letter words: 260 words\n",
      "filter stop words: 129 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "#             array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    cluster_num = 0\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            points.remove(seed)\n",
    "\n",
    "    while cluster_num != 0:\n",
    "        cluster = [numpy.array(bow_corpus.iloc[i]) for i, label in enumerate(labels) if label == cluster_num]\n",
    "        eps_temp = eps\n",
    "        \n",
    "        while True:\n",
    "            centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "            eps_temp -= delta_eps\n",
    "            \n",
    "            count = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                point = numpy.array(bow_corpus.iloc[i]).reshape(1, -1)\n",
    "                if label == 0 and cosine_similarity(centroid, point) >= eps_temp:\n",
    "                    cluster.append(point[0])\n",
    "                    labels[i] = cluster_num\n",
    "                    count += 1\n",
    "            if count == 0:\n",
    "                break\n",
    "        \n",
    "        cluster_num -= 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 20\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    initials = [[],[]]\n",
    "    \n",
    "    clusters = []\n",
    "    clusters.append([])\n",
    "    cluster_num = 0\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            clusters.append([])\n",
    "            for p in eps_neighbors:\n",
    "                clusters[cluster_num].append(numpy.array(bow_corpus.iloc[p]))\n",
    "                labels[p] = cluster_num\n",
    "                \n",
    "                initials[0].append(p)\n",
    "                if p == seed:\n",
    "                    initials[1].append(p)\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            clusters[0].append((seed, numpy.array(bow_corpus.iloc[seed])))\n",
    "            points.remove(seed)\n",
    "    \n",
    "    expandable = numpy.zeros(cluster_num + 1)    \n",
    "    while numpy.sum(expandable) != -cluster_num - 1:\n",
    "        eps -= delta_eps\n",
    "        count = numpy.zeros(cluster_num + 1)\n",
    "        for point in clusters[0]:\n",
    "            if labels[point[0]] != 0:\n",
    "                continue\n",
    "\n",
    "            num = point[0]\n",
    "            p = point[1].reshape(1, -1)\n",
    "            max_sim = 0\n",
    "            for c, cluster in enumerate(clusters[1:]):\n",
    "                if expandable[c + 1] == -1:\n",
    "                    continue\n",
    "                centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "                if cosine_similarity(centroid, p) >= max_sim:\n",
    "                    max_sim = cosine_similarity(centroid, p)\n",
    "                    if max_sim >= eps:\n",
    "                        labels[num] = c + 1\n",
    "\n",
    "            if labels[num] != 0:\n",
    "                count[labels[num]] += 1\n",
    "                clusters[labels[num]].append(point[1])\n",
    "\n",
    "        for i, num in enumerate(count):\n",
    "            if num == 0:\n",
    "                expandable[i] = -1\n",
    "        \n",
    "    return labels, initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = filtered_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(bow_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(bow_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(bow_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.50790617783765\n",
      "(array([0, 1, 2, 3, 4, 5, 6]), array([47, 19, 16, 19, 23, 18, 11])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_samples = 7\n",
    "eps = 0.3\n",
    "\n",
    "max_compactness = 0\n",
    "for i in range(10):\n",
    "    # _tbow_corpus = get_bow(idx_corpus)\n",
    "    _tbow_corpus = get_bow(filtered_corpus)\n",
    "    _tpredicted_labels, _tinitials = upgrade_sdc(_tbow_corpus, min_samples, eps)\n",
    "    _tresult = generate_result(_tpredicted_labels)\n",
    "    \n",
    "    compactness, _tcentroids = eval_cluster(_tbow_corpus, _tresult)\n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        bow_corpus = _tbow_corpus\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        initials = _tinitials\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_cluster = label_count[0][-1] + 1\n",
    "print(label_count, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "3 2 0.34397956112645556\n",
      "[0, 1, 2, 2, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            new_labels = [new_labels[j] if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, initials, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        self.initials = initials\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new][self.column_name].iteritems():\n",
    "            if index in self.initials[0]:\n",
    "                if index in self.initials[1]:\n",
    "                    print(\"@\", end=\"\")\n",
    "                else:\n",
    "                    print(\"*\", end=\"\")\n",
    "            print(index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3be86c28584f2aa914c2e3f08f57f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=5, options=(0, 1, 2, 4, 5, 6), value=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*31 ใช้บัตรเดียวกับบัตรประชาชนจบ ใช้   กล้องมือถือถ่าย\n",
      "*34 ใช้บัตรเดียวกับบัตรประชาชนจบ ใช้   กล้องมือถือถ่าย\n",
      "*53 กฏหมายเขายึดบัตรได้หรอ แค่สามารถดูแล้วคืนว่ามีใบอนุญาติ รึป่าว\n",
      "*57 ยึดไบขับขี่ไม่ได้ตอรองเล่นแง่ไม่ได้ยึดโทรสัพไม่ได้เดี๋ยวโดนลักทรัพย์\n",
      "*59 คือมันก็แค่แก้แอปให้ตรยึดผ่านแอป ใครโดนยึดก็ขึ้นหน้าจอแค่นั่นเอง ยากอะไร\n",
      "*88 ยึดไม่ได้ก็จ่ายน้อยหน่อย\n",
      "*104 ถ้ายึดโทรศัพท์เดี๋ยวตรโดนจับเอง ถถถ\n",
      "*116 ไม่มีไรให้ยึด 555\n",
      "*128 ยึดมือถือเลย ไม่ชอบรึ อิอิ\n",
      "*133 ดิจิตัลมันยึดไม่ได้มั้ง\n",
      "@139 ยึดบัตรไม่ได้ ยึดมือถือนะนักเรียน\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, initials, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2935e4603795441fb623a04c49500bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=4, options=(0, 1, 2, 3, 4, 5, 6), value=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*6 ตำรวจไม่รู้จะยึดใบขับขี่ยังไง ตอนเขียนใบสั่งครับ ปัญหาระดับชาติเลยสินะ\n",
      "*12 ไม่ได้ยึดใบขับขี่ ก็ไม่มีคนไปจ่ายค่าปรับ\n",
      "*18 มันไม่ได้เกี่ยวกับพกไม่พก มันเกี่ยวกับว่าทำใบขับขี่รึยัง ถ้าใบขับขี่นอนอยู่บ้าน มันคือทำแล้วป่าว่ะ\n",
      "*20 ควยไรครับคุณตำรวจ ขนส่งเป็นผู้ออกใบขับขี่ ถ้าขนส่งบอกใช้ได้คือต้องใช้ได้ครับ คุณมีหน้าที่ปฏิบัติตามครับ ไอ้ซัซ\n",
      "*21 ใครโดนใบสั่ง โดนยึดใบขับขี่ เพราะ ใช้ มาต่อทะเบียน ทำใบขับขี่ใหม่ได้เลยยยยยยย\n",
      "*23 ถ้าเอาตามที่เค้าว่ากฎหมายยังไม่พร้อมก็ตามนั้นก็ดีแล้วพกใบขับขี่ใบเดียวไม่ตายห่าหรอก\n",
      "*32 ไม่ใบขับขี่ ไม่เป็นไร ไม่มีใบขับขี่  ก็ไม่เป็นไร ถ้าไม้ให้ค่าลิโพ สั้ก 45 ร้อยมึงเจอยึด ตร ท ไม่ต้องกล่าวก็รู้ใว้\n",
      "33 ทะเลาะกันแหละดีละ ทีนี้ขนส่งก็บอกคันไหนค้างใบสั่งก็ยังต่อภาษีได้   ต่างคนต่างหาเงินเนอะ\n",
      "*38 ขนส่งแก้เผ็ดด้วยการ อนุญาตให้ประชาชนทำใบขับขี่ได้ครั้งละไม่เกิน100ใบ ใบละ10บาท ถ้าได้แบบนี้สนุกเลย เอาใบขับขี่ให้ไปกองเต็ม โรงพักให้ปลวกกินไปเลย\n",
      "@40 กูว่าแล้ว ถ้ายังคงเป็นโรโบคอปยุคปัจจุบันยังไงใบขับขี่ดิจิตอลไม่มีทางเกิดแน่นอน ต่อให้มีกฎหมายออกมามันก็ไม่รับลูกหรอกเชื่อดิ เสียเวลา ถถถถถถถถ\n",
      "*44 ผมว่านะมีไม่มีมันก็ขับตามสันดานเหมือนๆกันไอ้แบบขับรถถูกกฎอยู่ดีๆเรียกขอดูใบขับขี่ มันไม่ควรจะเกิดด้วยซ้ำ\n",
      "*45 จะแดกกันยังจะขัดขากันอีก ไทยแท้ๆ ในเยอรมัน ใบขับขี่มันมีลืมกันได้ ตำรวจเขาจะเรียกดูบัตรประชาชน เช็คข้อมูลเอาจากหน่วยข้อมูลทันที รู้ว่าใครมีใบขับขี่อะไรยังไงแบบไหน ไม่ได้ปรับหาแดกกะหลั่วๆแบบนี้  ถามพ่องบ้างนะ เห็นเอาลูกเอาเมียไปอยู่เยอรมันอ่ะ ว่าประเทศเค้าบริหารกันยังไง\n",
      "*47 ใบขับขี่นะมะช่ายบิทคอยน์ มา  แล้วเพ่เค้าจะยึดอารายยย อะไรนะ กม เค้าไม่ได้ให้ยึดใบขับขี่เหรอ แป่ววว\n",
      "*50 ตำหนวดกลัวรายได้จากการยึดใบขับขี่จะลดลงก็มีแต่ไทยนี่ละมั๊งที่ตำหนวดมีสิทธิ์ยึดใบขับขี่น่ะต้องไปถกสะสางกันตั้งแต่ ตำหนวดมีอำนาจยึดใบขับขี่ประชาชนหรือไม่ แล้วถ้ายึดไปถือว่าทำเกินกว่าหน้าที่หรือไม่เบื่อตำหนวด\n",
      "56 ใบสั่งจากกล้องที่ส่งจดหมายมาก็ใช้แทนใบสั่งเดิมไม่ได้ใช่ไหมครับ\n",
      "*69 ใบขับขี่ ออกโดยกรมการขนส่ง และกรมการขนส่งประกาศแล้วว่าใช้ได้ แล้วตำรวจมีสิทธิไรไม่เอาด้วยวะ\n",
      "70 สงสัยอยากงัดกับขนส่งที่ไม่ยอม อายัดทะเบียนพวกไม่จ่ายค่าปรับใบสั่งจากกล้อง\n",
      "*71 \"ยึดใบขับขี่ยังดีกว่ายึดโทรศัพท์มือถือ\"ตำรวยไทยไม่ได้กล่าว\n",
      "*80 ก็ใบขับขี่หนิ ใบขับขี่ดิจิตอล 555\n",
      "*91 จับแน่นอน 555\n",
      "118 แต่ใบสั่ง ออนไลน์ได้นะครับ\n",
      "*140 คือยึดใบขับขี่ไม่ได้ช่ะ  เลยงอแงเนี่ย\n",
      "*152 เรื่องของตำรวจสิ ในเมื่อใบขับขี่เป็นอำนาจหน้าที่ของขนส่ง ฟ้องเม่ง\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, initials, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2151356d7c4b06af726230e702a762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=4, options=(0, 1, 2, 3, 4, 5, 6), value=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*6 ['ตำรวจ', 'ใบ', 'ใบ', 'ยึด', 'ขับขี่']\n",
      "*12 ['ใบ', 'ยึด', 'ขับขี่', 'คน', 'จ่าย']\n",
      "*18 ['ใบ', 'ใบ', 'ทำ', 'ขับขี่', 'ขับขี่']\n",
      "*20 ['ตำรวจ', 'ใบ', 'ขับขี่', 'ขนส่ง', 'ขนส่ง']\n",
      "*21 ['ใบ', 'ใบ', 'ใบ', 'ยึด', 'ขับขี่']\n",
      "*23 ['ใบ', 'ใบ', 'ขับขี่', 'กฎหมาย', 'ดี']\n",
      "*32 ['ใบ', 'ใบ', 'ยึด', 'ขับขี่', 'ขับขี่']\n",
      "33 ['ใบ', 'ขนส่ง', 'คน', 'ดี', 'สั่ง']\n",
      "*38 ['ใบ', 'ใบ', 'ใบ', 'ใบ', 'ทำ']\n",
      "@40 ['ใบ', 'ขับขี่', 'กฎหมาย', 'แน่นอน', 'เวลา']\n",
      "*44 ['ใบ', 'ขับขี่', 'ดี', 'ผม', 'เหมือน']\n",
      "*45 ['ตำรวจ', 'ใบ', 'ใบ', 'ขับขี่', 'ขับขี่']\n",
      "*47 ['ใบ', 'ใบ', 'ยึด', 'ยึด', 'ขับขี่']\n",
      "*50 ['ใบ', 'ใบ', 'ใบ', 'ยึด', 'ยึด']\n",
      "56 ['ใบ', 'ใบ', 'สั่ง', 'สั่ง', 'ไหม']\n",
      "*69 ['ตำรวจ', 'ใบ', 'ขับขี่', 'วะ', 'ประกาศ']\n",
      "70 ['ใบ', 'ขนส่ง', 'จ่าย', 'สั่ง', 'ค่า']\n",
      "*71 ['ใบ', 'ยึด', 'ยึด', 'ขับขี่', 'ดี']\n",
      "*80 ['ใบ', 'ใบ', 'ขับขี่', 'ขับขี่', 'ดิจิตอล']\n",
      "*91 ['แน่นอน']\n",
      "118 ['ใบ', 'สั่ง', 'ออนไลน์']\n",
      "*140 ['ใบ', 'ยึด', 'ขับขี่']\n",
      "*152 ['ตำรวจ', 'ใบ', 'ขับขี่', 'ขนส่ง', 'เรื่อง']\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, initials, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['บัตร', 'ข่าว', 'จบ', 'ประกัน', 'เลิก']\n",
      "['บัตร', 'ข่าว', 'จบ', 'ประกัน', 'เลิก']\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(bow_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(bow_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "print(filtered_corpus[seed])\n",
    "print(filtered_corpus[compare])\n",
    "\n",
    "# print(sims[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
