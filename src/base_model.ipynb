{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "import pythainlp.word_vector\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus, clean\n",
    "\n",
    "from model.new_sdc import NewSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 229\n"
     ]
    }
   ],
   "source": [
    "file_name = 'แก้ปัญหาฝุ่น'\n",
    "\n",
    "corpus = load_corpus('../data/' + file_name + '.txt')\n",
    "print('Total documents', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('../data/tokenized/newmm/tokenized_' + file_name + '.txt')\n",
    "f = open('../data/tokenized/deepcut/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1401 words\n",
      "filter frequent words: 658 words\n",
      "filter letter words: 654 words\n",
      "filter stop words: 428 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.common.thai_stopwords()\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanoonsantikul/Documents/senior-project/venv/lib/python3.6/site-packages/pythainlp/word_vector/__init__.py:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "  vec /= len(words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan\n"
     ]
    }
   ],
   "source": [
    "def token_to_string(tokenized_list):\n",
    "    string = str(tokenized_list)\n",
    "    string.replace(',', ' ')\n",
    "    return clean(string)\n",
    "\n",
    "doc2vec_corpus = []\n",
    "for doc in tokenized_corpus:\n",
    "    array = pythainlp.word_vector.sentence_vectorizer(token_to_string(doc))[0]\n",
    "    if any(numpy.isnan(array)):\n",
    "        print('Nan')\n",
    "        array = numpy.zeros(300)\n",
    "    doc2vec_corpus.append(array)\n",
    "doc2vec_corpus = pandas.DataFrame(doc2vec_corpus, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vectors(corpus, value):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if value == 'bow':\n",
    "        vectors = bow_corpus\n",
    "    elif value == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        vectors = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(vectors):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if value == 'bow' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(document_vectors, result):\n",
    "    num_cluster = numpy.unique(result['predicted_label'])[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(document_vectors.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(document_vectors.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.75\n",
    "expand_rate = 0.05\n",
    "epoch = 15\n",
    "\n",
    "document_vectors = get_document_vectors(idx_corpus, 'bow')\n",
    "# document_vectors = doc2vec_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_compactness = 0\n",
    "for i in range(epoch):\n",
    "    model = NewSDC()\n",
    "    _tpredicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate)\n",
    "    \n",
    "#     marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(document_vectors)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "#     model = KMeans(n_clusters=14).fit(document_vectors)\n",
    "#     _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(document_vectors, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "num_cluster = len(label_count)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative New SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.99646535408963\n",
      "[229]\n"
     ]
    }
   ],
   "source": [
    "centroids = None \n",
    "prev_label_count = None\n",
    "model = NewSDC()\n",
    "while True:\n",
    "    predicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(document_vectors, result)\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    if numpy.array_equal(label_count, prev_label_count):\n",
    "        break\n",
    "    prev_label_count = label_count\n",
    "    centroids = centroids[1:]\n",
    "    \n",
    "    print(compactness)\n",
    "    print(label_count)\n",
    "num_cluster = len(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 [229]\n",
      "0.02 [229]\n",
      "0.03 [229]\n",
      "0.04 [229]\n",
      "0.05 [229]\n",
      "0.06 [229]\n",
      "0.07 [229]\n",
      "0.08 [229]\n",
      "0.09 [229]\n",
      "0.10 [229]\n",
      "0.11 [229]\n",
      "0.12 [229]\n",
      "0.13 [229]\n",
      "0.14 [229]\n",
      "0.15 [229]\n",
      "0.16 [229]\n",
      "0.17 [229]\n",
      "0.18 [229]\n",
      "0.19 [229]\n",
      "0.20 [229]\n",
      "0.21 [229]\n",
      "0.22 [229]\n",
      "0.23 [229]\n",
      "0.24 [229]\n",
      "0.25 [229]\n",
      "0.26 [229]\n",
      "0.27 [229]\n",
      "0.28 [229]\n",
      "0.29 [229]\n",
      "0.30 [213   9   7]\n",
      "0.31 [213   9   7]\n",
      "0.32 [213   9   7]\n",
      "0.33 [213   9   7]\n",
      "0.34 [213   9   7]\n",
      "0.35 [213   9   7]\n",
      "0.36 [213   9   7]\n",
      "0.37 [212  10   7]\n",
      "0.38 [212  10   7]\n",
      "0.39 [212  10   7]\n",
      "0.40 [212  10   7]\n",
      "0.41 [205   7  10   7]\n",
      "0.42 [204   7  11   7]\n",
      "0.43 [199  10  20]\n",
      "0.44 [198  11  20]\n",
      "0.45 [189   8  11  21]\n",
      "0.46 [187   8  12  22]\n",
      "0.47 [186   9  12  22]\n",
      "0.48 [183   9  14  23]\n",
      "0.49 [181   9  16  23]\n",
      "0.50 [165  10  28  20   6]\n",
      "0.51 [152  61   9   7]\n",
      "0.52 [147  65   9   8]\n",
      "0.53 [141  71   9   8]\n",
      "0.54 [139  73   9   8]\n",
      "0.55 [137  75   9   8]\n",
      "0.56 [125  88   9   7]\n",
      "0.57 [124 105]\n",
      "0.58 [113 116]\n",
      "0.59 [109 120]\n",
      "0.60 [ 91 132   6]\n",
      "0.61 [ 88 135   6]\n",
      "0.62 [ 84 145]\n",
      "0.63 [ 81 148]\n",
      "0.64 [ 75 154]\n",
      "0.65 [ 63 166]\n",
      "0.66 [ 58 171]\n",
      "0.67 [ 48 181]\n",
      "0.68 [ 47 182]\n",
      "0.69 [ 40 189]\n",
      "0.70 [ 37 192]\n",
      "0.71 [ 34 195]\n",
      "0.72 [ 30 199]\n",
      "0.73 [ 27 202]\n",
      "0.74 [ 22 207]\n",
      "0.75 [ 17 212]\n",
      "0.76 [ 16 213]\n",
      "0.77 [ 14 215]\n",
      "0.78 [ 11 218]\n",
      "0.79 [ 11 218]\n",
      "0.80 [ 10 219]\n",
      "0.81 [ 10 219]\n",
      "0.82 [ 10 219]\n",
      "0.83 [ 10 219]\n",
      "0.84 [  9 220]\n",
      "0.85 [  9 220]\n",
      "0.86 [  8 221]\n",
      "0.87 [  8 221]\n",
      "0.88 [  8 221]\n",
      "0.89 [  8 221]\n",
      "0.90 [  8 221]\n",
      "0.91 [  8 221]\n",
      "0.92 [  8 221]\n",
      "0.93 [  8 221]\n",
      "0.94 [  8 221]\n",
      "0.95 [  8 221]\n",
      "0.96 [  8 221]\n",
      "0.97 [  8 221]\n",
      "0.98 [  8 221]\n",
      "0.99 [  8 221]\n"
     ]
    }
   ],
   "source": [
    "eps = 0.01\n",
    "marks = None\n",
    "\n",
    "while eps <= 1:\n",
    "    max_compactness = 0\n",
    "    for i in range(epoch):\n",
    "        model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(document_vectors)\n",
    "        _tpredicted_labels = model.labels_ + 1\n",
    "        \n",
    "        _tresult = generate_result(_tpredicted_labels, marks)\n",
    "        compactness, _tcentroids = eval_cluster(document_vectors, _tresult)\n",
    "\n",
    "        if compactness > max_compactness:\n",
    "            max_compactness = compactness\n",
    "            predicted_labels = _tpredicted_labels\n",
    "            result = _tresult\n",
    "            centroids = _tcentroids\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    print('%.2f' % eps, label_count)\n",
    "    eps += 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "8 7 0.8755721909893287\n",
      "8 6 0.8586066011239457\n",
      "8 5 0.8507223950545542\n",
      "8 4 0.875182053956368\n",
      "8 3 0.8589657405412545\n",
      "8 2 0.8675732251896662\n",
      "8 1 0.8875363722048335\n",
      "7 6 0.9384452834460689\n",
      "7 5 0.9141454743659437\n",
      "7 4 0.9521101960285854\n",
      "7 3 0.8930926351335264\n",
      "7 2 0.9101058935577682\n",
      "7 1 0.9658037508375265\n",
      "6 5 0.9025615013232784\n",
      "6 4 0.9354670147584043\n",
      "6 3 0.9285334363748009\n",
      "6 2 0.9410823368053696\n",
      "6 1 0.9634998744557152\n",
      "5 4 0.9131194410648852\n",
      "5 3 0.8498011498062978\n",
      "5 2 0.9004323324387875\n",
      "5 1 0.9329955899312412\n",
      "4 3 0.9034467677178339\n",
      "4 2 0.9273065499085396\n",
      "4 1 0.9723202135430872\n",
      "3 2 0.9125217520384445\n",
      "3 1 0.930462408162529\n",
      "2 1 0.9438673547511558\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len(corpus))\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.78241587508978\n",
      "[ 5 74 24 15 11 39 16 13 32]\n"
     ]
    }
   ],
   "source": [
    "# result.to_csv('../data/results/iterative_new_sdc/' + file_name + '_2.csv', index=False)\n",
    "\n",
    "# result = pandas.read_csv('../data/results/iterative_new_sdc/' + file_name + '.csv')\n",
    "\n",
    "print(eval_cluster(document_vectors, result)[0])\n",
    "print(numpy.unique(result['predicted_label'], return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260a2a55b3dd42b9bc6cf5aca9088c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 This shows everything about Thailand\n",
      "183 รัฐบาลโจร\n",
      "187 รถล้านคัน 555    กูจำได้\n",
      "220 +1\n",
      "227 https//www.posttoday.com/social/general/577217\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59a93a28c9f48828ec120796f701c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=4, options=(0, 1, 2, 3, 4, 5, 6, 7, 8), value=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*7 การตัดแต่งต้นไม้ของ กทม. สักแต่ว่าตัดตัดจนมันเหี้ยนกุด ไม่เหลือสถาพของต้นไม้ประเทศไทยมีรุกขกรณ์ ที่มีความรู้ด้านการตัดแต่งต้นไม้ให้ได้ดีและสวยงามแค่ปรับปรุง ปรับเปรี่ยนการตัดแต่งต้นไม้ เราก็จะได้ประสิทธิภาพการกรองฝุ่นและมลพิษ กลับมามากมายแล้ว\n",
      "24 คือแบบ ไม่มีการตายให้เห็นปัจจุบันทันด่วน ก็เลยไม่วิตกว่าเรื่องนี้ มันเป็นวิกฤติของประเทศ...มังคะ..\n",
      "*43 ทุกวันนี้ลูกก็ยังได้เข้าแถวหน้าเสาธง ห้องเรียนยังเปิดประตูหน้าต่าง ไปค่ายลูกเสือทำกิจกรรมกลางแจ้งเฉย ชีวิตคนไทยมันก็เป็นแบบนี้แหละ ต้องดูแลตัวเองจริงๆ ถ้าหวังพึ่งพารัฐก็เครียดไปเปล่าๆ แต่ก็ขอบคุณหมอนะที่เขียนบทความแสดงออกถึงความห่วงใย\n",
      "*55 ขอบคุณ​คุณหมอค่ะ​ รัฐบาลไม่ประกาศ​สิ่งที่เป็นรูปธรรม​อะไรสักอย่าง​ มีแต่เราที่ต้องป้องกันตัว​เอง​ เฉยแบบนี้​ ก็ไม่มีการตื่นตัว​ว่ามันอันตรายแล้ว​ รู้ทั้ง​รู้​แต่กลับ​เฉย\n",
      "78 คุณหมอคะ นอกจากรัฐควรเร่งปลูกต้นไม้. รัฐบาลควรออกกฏหมายรักษาต้นไม้ที่มีอยู่อย่างเคร่งครัดด้วยค่ะ. เห็นตัดกันเป็นว่าเล่น ต้นใหญ่ๆทั้งนั้น 😪\n",
      "*82 มันคือฝุ่นที่มองไม่เห็น และมีผลระยะยาว เลยทำให้คนไม่ตื่นกลัว ส่วนรัฐก็บอกไม่มีอะไร สงสารเด็กที่ต้องไปรร.มากสุดเลยค่ะ\n",
      "*84 ขนาดผักที่กินอยู่ก็ปนเปื้อนสารพิษรัฐบาลยังนิ่งเฉยไม่ยกเลิกกฎหมายนำเข้าสารเคมีผู้บริโภคก็ต้องป้องกันตัวเอง​ตอนนี้อากาศเป็นพิษรัฐบาลก็เฉยๆปล่อยให้ประชาชนเผชิญวิกฤติ​ไปมันนากลัวตรงที่ว่าถ้ามันเกิดขึ้นทุกปีล่ะ\n",
      "*91 ถ. ศาลาธรรมสพน์ ตอนไปอยู่ใหม่เมื่อ15ปีที่แล้ว  มีต้นไม่สองข้างทางบางช่วงเป็นอุโมงค์ต้นไม้เลย  แต่ตอนนี้ขยายถนน  ต้นไม้โดนขุดหมดต่อไปไม่มีอีกแล้ว  แถมช่วงนี้แถวนั้นมีเศษขึ้เถ้าที่เผาพืชเต็มไปหมดลอยมาจากไหนไม่รู้  แค่นี้ยังจัดการไม่ได้เลยแล้วจะจัดการภาพรวมได้ยังไง\n",
      "*102 ทุกวันนี้ต้นไม้ยังโดนหน่วยงานรัฐตัดจนเหี้ยนอยู่เลย บางที่ไม่มีสายไฟก็ตัด  แบบนี่แหละอากาศมันถึงแย่ไง แต่รัฐบาลไม่ยักกะออกมารณรงค์ให้คนปลูกต้นไม้นะ คงกลัวคนด่ามากกว่านี้มั้ง\n",
      "*166 ยากมาก ในฐานะซิงเกิ้ลมัมต่อให้ลูกปิดเทอมเราก็ยังต้องทำงาน หน้ากากก็หาซื้อยากมากๆ\n",
      "@167 ต้นไม้ในกรุงเทพพอมีใบหน่อยก็ตัดซะเหี้ยน ลำบากนักก็อย่าปลูกเลย ไม่ต้องเสียค่าดูแลรักษาด้วย\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28493f7625f4ccd91c3e430f3c3b161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 []\n",
      "183 ['รัฐบาล']\n",
      "187 ['รถ', 'ล้าน', 'คัน']\n",
      "220 []\n",
      "227 []\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['ประกาศ', 'หยุด', 'เรียน', 'ระดับ', 'ชั้น', 'บริหาร', 'โรง', 'เรียน', 'ประกาศ', 'ชม']\n",
      "รร.รุ่งอรุณประกาศหยุดเรียนทุกระดับชั้น ชื่นชมผู้บริหารโรงเรียนในการตัดสิ้นใจค่ะ ประกาศเมื่อราว 1 ชม.ที่ผ่านมาค่ะ\n",
      "['ประกาศ', 'หยุด', 'เรียน', 'ระดับ', 'ชั้น', 'บริหาร', 'โรง', 'เรียน', 'ประกาศ', 'ชม']\n",
      "รร.รุ่งอรุณประกาศหยุดเรียนทุกระดับชั้น ชื่นชมผู้บริหารโรงเรียนในการตัดสิ้นใจค่ะ ประกาศเมื่อราว 1 ชม.ที่ผ่านมาค่ะ\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(document_vectors.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(document_vectors.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seniorproject",
   "language": "python",
   "name": "seniorproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
