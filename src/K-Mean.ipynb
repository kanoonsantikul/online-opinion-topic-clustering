{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 269\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1313 words\n",
      "filter frequent words: 540 words\n",
      "filter letter words: 539 words\n",
      "filter stop words: 352 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "appear_rate = 1\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = []\n",
    "for word in dictionary.keys():\n",
    "    if len(dictionary[word]) <= 1:\n",
    "        letter_words.append(word)\n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "average_doc_size\n",
    "\n",
    "word_freq = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, word_freq[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "\n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(bow_corpus, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters).fit(bow_corpus)\n",
    "    predicted_labels = kmeans.labels_\n",
    "\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>บางครั้งพนักงานโลตัสเอ็กเพลส กับพนักงานโลตัสให...</td>\n",
       "      <td>[พนักงาน, พนักงาน, โลตัส, โลตัส, ทำ, เอ็กเพลส,...</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพีย...</td>\n",
       "      <td>[เลิก, ข้าว, ดี, อี, เครื่อง, น้ำ]</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถุงงง  ถุงมึงบอบบางมากกก ยิ่งเจอน้ำยาซักผ้า น้...</td>\n",
       "      <td>[ถุง, งง, ถุง, กก, เจอ, น้ำยา, ซัก, ผ้า, น้ำ, ...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>เราไม่เคยรับถุงเลย แถมได้แต้มกรีนพ้อยไว้ใช้เป็...</td>\n",
       "      <td>[ถุง, แถม, ลด, ลอง, ดู, ถุง, หนา]</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>เคยไปทำ เอ้กซ์เพรส เดือนเดียว พอเลย  พนักงานต่...</td>\n",
       "      <td>[ทำ, เดือน, พนักงาน, คน, คน, คน, นึง, พัน, เช้...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  บางครั้งพนักงานโลตัสเอ็กเพลส กับพนักงานโลตัสให...   \n",
       "1  เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพีย...   \n",
       "2  ถุงงง  ถุงมึงบอบบางมากกก ยิ่งเจอน้ำยาซักผ้า น้...   \n",
       "3  เราไม่เคยรับถุงเลย แถมได้แต้มกรีนพ้อยไว้ใช้เป็...   \n",
       "4  เคยไปทำ เอ้กซ์เพรส เดือนเดียว พอเลย  พนักงานต่...   \n",
       "\n",
       "                                   tokenized_comment label  predicted_label  \n",
       "0  [พนักงาน, พนักงาน, โลตัส, โลตัส, ทำ, เอ็กเพลส,...                      5  \n",
       "1                 [เลิก, ข้าว, ดี, อี, เครื่อง, น้ำ]                      1  \n",
       "2  [ถุง, งง, ถุง, กก, เจอ, น้ำยา, ซัก, ผ้า, น้ำ, ...                      2  \n",
       "3                  [ถุง, แถม, ลด, ลอง, ดู, ถุง, หนา]                      2  \n",
       "4  [ทำ, เดือน, พนักงาน, คน, คน, คน, นึง, พัน, เช้...                      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "# bow_corpus = get_bow(idx_corpus)\n",
    "bow_corpus = get_bow(filtered_corpus)\n",
    "result = predict_cluster(bow_corpus, num_clusters)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5], dtype=int32), array([ 16, 201,  24,   4,  10,  14]))\n",
      "\t\tpercent\n",
      "0  |\t16\t1.0\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "print(label_count)\n",
    "\n",
    "for cluster in clusters:\n",
    "    print('\\t' + cluster, end='')\n",
    "print('\\tpercent')\n",
    "\n",
    "for label in range(len(clusters)):\n",
    "    print(str(label) + '  |', end='')\n",
    "    \n",
    "    num_max = 0\n",
    "    for cluster in clusters:\n",
    "        loc = result[(result['label'] == cluster) & (result['predicted_label'] == label)]\n",
    "        if len(loc) > num_max:\n",
    "            num_max = len(loc)\n",
    "        print('\\t' + str(len(loc)), end='')\n",
    "    \n",
    "    print('\\t' + str(num_max / label_count[1][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4481fa83b914837beec84a658f58ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=5, options=(0, 1, 2, 3, 4, 5), value=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 บางครั้งพนักงานโลตัสเอ็กเพลส กับพนักงานโลตัสใหญ่ ก็ต่างกันครับ โลตัสใหญ่ เขาทำเป็นเเผนกๆไปครับ เเต่โลตัสเอ็กเพลสนั้น พนักงานเเต่ละคน ต้องทำทุกอย่างครับ ไม่ใช่เเค่ยืนขายของ อยากให้ทุกคนเข้าใจด้วยครับ บางคนเวลากินข้าวเเทบไม่มี ผมเชื่อว่าทุกคนก็เป็นพนักงานเงินเดือนเหมือนกัน ควรเข้าใจกันบ้างครับ พนักงานคนอื่นเป็นเเบบไหนผมไม่รู้ เเต่มันก็ยังมีพนักงานที่ตั้งใจทำงานครับ ขอให้ทุกคนเข้าใจครับ\n",
      "39 90 เปอร์เซ็นต์ของคอมเม้นต์นี่ เกี่ยวกับพนักงานล้วนๆ 555รวมทั้งของนี่ด้วย\n",
      "45 พนักงานชอบเหวี่ยง กระชากเงินจากมือ ยื่นเงินทอนให้แบบไม่เต็มใจ โดยเฉพาะพนักงานพาร์ทไทม์\n",
      "116 พนักงานขายที่นี่มีการคัดกรองมั้ยคะคุณสมบัติพนักงานที่นี่\n",
      "184 พนักงานตามเอ็กเพลสนี่ต้องฝึกมารยาทนะ เห็นอยู่กันตามยถากรรม\n",
      "185 มารยาทพนักงาน\n",
      "188 ปรับปรุงพนักงานค่ะ\n",
      "221 พนักงาน\n",
      "228 มารยาทของพนักงาน\n",
      "233 เอ๊กเพรส ต้องปรับปรุงพนักงาน\n",
      "236 พนักงาน\n",
      "238 พนักงาน\n",
      "241 มารยาทพนักงานควรปรับปรุงมากที่สุด\n",
      "253 มารยาทของพนักงานของพนักงานค่ะ\n"
     ]
    }
   ],
   "source": [
    "comment_widget = widgets.ToggleButtons(\n",
    "    options=[num for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_comment_widget_click(change):\n",
    "    clear_output()\n",
    "    display(comment_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "comment_widget.observe(on_comment_widget_click, names='index')\n",
    "on_comment_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e396dcbd62034c198cdf69f268f1dd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['สาขา', 'กะปิ', 'ตัว', 'หน้า', 'ร้าน', 'ชั้น', 'ชั้น', 'ไปรษณีย์', 'ประตู', 'จอด', 'รถ']\n",
      "44 ['ช่อง', 'จ่าย', 'เงิน', 'ด่วน', 'สินค้า', 'ชิ้น', 'อย่า', 'เข็น', 'จ่าย', 'เต็ม', 'รถ', 'เข็น', 'พนักงาน', 'สัก', 'ก้อ', 'ห่วย', 'รอ', 'คิว', 'ชาติ', 'ช่อง', 'ช่อง', 'กุ', 'บ้า']\n",
      "60 ['รถ', 'เข็น', 'เติม', 'สินค้า', 'ลูกค้า', 'ขนาด', 'กกก', 'กลัว', 'ซื้อ']\n",
      "76 ['ดี', 'คิว', 'ซื้อ', 'พนักงาน', 'ลูกค้า', 'ซื้อ', 'รถ', 'เข็น']\n",
      "90 ['รถ', 'เข็น', 'หา', 'พัง', 'พนักงาน']\n",
      "100 ['พนง', 'หน้า', 'แย่', 'ปล่อย', 'เรื่อง', 'จอด', 'รถ', 'ลูกค้า', 'ปกติ', 'จอด', 'คน', 'หน้าตา', 'พนง', 'บัตร', 'จอด', 'รถ', 'คุย', 'เจอ', 'ด่า', 'สาย', 'หยาบ', 'นึก', 'ด่า']\n",
      "125 ['รถ', 'เข็น', 'เหมือน', 'กิน', 'รถ', 'ปกติ', 'เหมือน', 'หวย']\n",
      "134 ['สาขา', 'ติด', 'รถ', 'เข็น', 'พื้น', 'หน้า', 'ซื้อ', 'โดน']\n",
      "151 ['รถ', 'เข็น', 'รถ', 'เด็ก', 'นั่ง', 'สาขา', 'สกปรก']\n",
      "166 ['ล้อ', 'รถ', 'หลุด', 'พัง', 'คัน']\n",
      "174 ['รถ', 'เข็น', 'คัน', 'ล้อ', 'ล้อ', 'ซ่อม', 'เข็น', 'ลำบาก', 'งาน']\n",
      "202 ['จอด', 'รถ', 'ดี', 'ชาย', 'จอด']\n",
      "208 ['รถ', 'เข็น', 'ล้อ', 'พัง', 'คัน']\n",
      "227 ['คอย', 'ตรวจ', 'จอด', 'รถ', 'รถ', 'หาย']\n",
      "229 ['จอด', 'รถ', 'บัตร']\n",
      "254 ['จอด', 'รถ', 'สาขา', 'นวมินทร์', 'หา', 'จอด']\n"
     ]
    }
   ],
   "source": [
    "token_widget = widgets.ToggleButtons(\n",
    "    options=[num for num in range(num_clusters)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_token_widget_click(change):\n",
    "    clear_output()\n",
    "    display(token_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['tokenized_comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "token_widget.observe(on_token_widget_click, names='index')\n",
    "on_token_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
