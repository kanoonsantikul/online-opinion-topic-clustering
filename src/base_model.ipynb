{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus\n",
    "\n",
    "from model.upgrade_sdc import UpgradeSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 268\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1449 words\n",
      "filter frequent words: 605 words\n",
      "filter letter words: 604 words\n",
      "filter stop words: 403 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "#     new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = filtered_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(onehot_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(onehot_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(onehot_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.18919302853803\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([44, 16, 11, 30, 16, 18, 18,  9, 13,  8, 17, 15, 11, 15, 15, 12])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_samples = 7\n",
    "eps = 0.32\n",
    "epoch = 10\n",
    "\n",
    "# onehot_corpus = get_onehot(idx_corpus)\n",
    "onehot_corpus = get_onehot(filtered_corpus)\n",
    "\n",
    "max_compactness = 0\n",
    "for i in range(epoch):\n",
    "    model = UpgradeSDC()\n",
    "    _tpredicted_labels, _tmarks = model.predict(onehot_corpus, min_samples, eps)\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels)\n",
    "    compactness, _tcentroids = eval_cluster(onehot_corpus, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        marks = _tmarks\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_cluster = label_count[0][-1] + 1\n",
    "print(label_count, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "15 13 0.6096108378847768\n",
      "15 11 0.43226598500062746\n",
      "15 10 0.36761594848667045\n",
      "15 8 0.48450067049442397\n",
      "15 2 0.4514777087111918\n",
      "15 1 0.6589595473778568\n",
      "13 11 0.4616059184780303\n",
      "13 8 0.40684081181430076\n",
      "13 2 0.6879395650378226\n",
      "13 1 0.5670507786353982\n",
      "11 8 0.32352399582517527\n",
      "11 5 0.3393126405305244\n",
      "11 2 0.3260578179335435\n",
      "11 1 0.586389800892499\n",
      "10 1 0.38660770429536456\n",
      "8 5 0.5677602361929177\n",
      "8 2 0.3383174849913636\n",
      "8 1 0.5499459045573912\n",
      "6 5 0.313122169377218\n",
      "6 1 0.3104758423106557\n",
      "5 1 0.4307602225523216\n",
      "4 1 0.3269203694086348\n",
      "2 1 0.4673068712094792\n",
      "[0, 5, 5, 3, 5, 5, 5, 7, 5, 9, 5, 5, 12, 5, 14, 5]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            new_labels = [new_labels[j] if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, marks, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        self.marks = marks\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new][self.column_name].iteritems():\n",
    "            if self.marks:\n",
    "                if index in self.marks[0]:\n",
    "                    print(\"@\", end=\"\")\n",
    "                elif index in self.marks[1]:\n",
    "                    print(\"*\", end=\"\")\n",
    "            print(index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459772a3bc7641729b6f0dbb7b39a167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 3, 5, 7, 9, 12, 14), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพียว ได้แล้ว!!!!\n",
      "8 อย่าบังคับน้องเข้าประชุมเชียร์ อย่าลงโทษโดยเหตุผลงี่เง่าๆ เกิดก่อนไม่กี่ปีเอง\n",
      "14 เลือกผัก เลือกแล้ว เลือกอีก พอมาคิดเงินพนักงานยัดผักใส่ถุงจนผักหัก โอ้ยใจ ใจสลาย\n",
      "29 โลตลาด พนง.ปากตลาด นินทาลค.เผาขน ตะโกนโหวกเหวกข้ามหัวลค. วันหวยออกสนั่นเป็นพิเศษ เห็นพฤติกรรมแล้วแย่ ปรับปรุงภาพลักษณ์เถอะ\n",
      "35 Lotus สาขายโสธร-เปิดใหม่ พนง.แอบโกง Give Card ลูกค้า(กุเอง) จ่ายหลายใบแล้วทำเนียนว่ามีใบนึงชำรุดใช้ไม่ได้แล้วพอขอคืนก็สลับใบที่รูดใช้แล้วมาแทน แจ้งผู้จัดการขอดูกล้องวงจรปิดไม่ยอมให้ดูเลยต้องขู่ว่าจะแจ้งความถึงยอมคืนให้ แต่ไม่มีคำขอโทษใดๆ จาก พนง.-ติด…See more\n",
      "43 สาขาพัทลุง เหม็นคาวมากกกกกกกกก แถมแผนกของกินของสดแม่งไปไหนหมด มีแต่เสื้อผ้าละ\n",
      "56 ของมึงไม่ต้องถูกมากก็ได้ เน้นคุณภาพบ้าง ไส้กรอกเน่า แต่ยังไม่หมดอายุงี้อ่ะ (สาขาฟอร์จูน) ขนมปังขึ้นราแล้วยังวางขายอีก (สาขาลาดพร้าว) กีวี่หนอนเข้า กะหล่ำปลีเน่าใน ผักบุ้งเน่าแทรกในมัด (สาขาอ่อนนุช)\n",
      "60 ของป้ายเหลืองพวกผัก เน่าจนไม่รู้จะเน่ายังไง คือทิ้งๆไปก็ได้มั่ง\n",
      "68 ช่องจ่ายเยอะ เยอะเกินพนง.ใช่ไหม ไม่ระบุสาขานะ เกือบทุกที่ที่ไปมาเลยค่ะ\n",
      "75 เวลาผมซื้อเนื้อสัตว์ช่วยเอาที่คีบหรือช้อนหรืออะไรก็ได้มาวางให้ทีอย่าให้กุต้องใช้มือเปล่าๆจับอีกอย่างจะเก็บช้อนกะที่คีบเร็วไปไหนเพิ่งจะ6โมงเย็นเห็นเอาไปล้างแล้ว\n",
      "83 เลิกก๊อปปี้สินค้าแบรนด์ แล้วทำคุณภาพห่วยๆ แม่งเล่นเลย์ซะเหมือนเลย สาดดด\n",
      "88 ฝากถึง สาขาแพร่นะคะตอนเช้าๆเลิกมาทอดโดนัทด้านนอกซะทีเถอะค่ะ กลิ่นเหม็นน้ำมันทอดติดผมเผ้าเสื้อผ้า\n",
      "95 โลตัส รัตนาธิเบศร์ แอร์เหม็นมาก เหม็นสาบหนูสุดๆ ตรงโซนอาหารแห้ง บะหมี่กึ่งสำเร็จรูป เดินผ่านก็รู้เลยมีหนู ตรงโซนข้าวสารอีก\n",
      "98 เวลาจัดโปรแลกของโลตัส เอ็กเพรส ถ้าของหมดให้จอง ให้เขียนชื่อใส่เบอร์โทร แต่เวลาของมาไม่เคยโทรมาบอกเลย ให้ใส่ทำมะเขือไรอะคะ ไม่เข้าใจอะ แล้วเคยโทรไปคอลเซนเตอร์ มีระบบให้ฝากเบอร์ไว้ ก็โทรกลับมาจริงๆนะ ผ่านไปเดือนกว่าถึงโทรกลับมา ไม่รอครบปีก่อนเลยล่ะ แหม่\n",
      "99 โสตัสเอ็กเพรส สาขาตลาดหนองจอก ระยอง ตู้แช่เย็นฝาตู้ปิดไม่สนิททุกตู้\n",
      "108 ของเซลคือเซลเเต่หมดอายุมาหลายเดือนเเล้วยังเซลได้ด้วยหรอ???(ของกินไงบางที)\n",
      "116 วันใหนมาตั้งใจมาชื้อหมู หมูหมดเหลือไก่  วันใหนชื้อไก่ไก่หมดได้หมู  ตั้งใจอย่างได้อีกอย่าง\n",
      "126 วางของเกะกะมาก เป็นแทบทุกสาขาที่เคยเข้าเลย ไม่มีมาตรการจัดของกันเลยหรอ #เอ็กซ์เพรส\n",
      "127 ถ้ามึงไม่อยากไหว้ ไม่ต้องยกมือไหว้ก้อได้ กูรู้มึงโดนบังคับ ให้ไหว้คนที่ไม่ใช่พ่อใช่แม่ เยสเข้\n",
      "135 เอ็กเพรสนี้แลกของแล้วไม่เคยจะโทรบอกเลยหรืออะไรเลยต้องได้ทวง. ชอบไม่ให้แสตมป์. เป็นไรมากไหมนี้\n",
      "140 เคือง...สะสมสแตมป์แลกร่มผ่านมา3ปีจนสาขาย่อยปิดไปแล้วข้ายังไม่ได้ร่มเลย/หลอกลวงถามทีไรกำลังผลิตๆที่ดาวดวงใหนรึ?\n",
      "144 เครื่องที่ให้แสกนของเองน่ะ อยากให้ปรับตำแหน่งให้มันแสกนจากมือถือได้ด้วยค่ะ บัตรหายไปแล้วแสกนจากมือถือไม่ได้มันติด\n",
      "165 ไปทีไรไม่เห็นมีดอกบัวขาย\n",
      "167 ตอนเลือกซื้อผัก เลือกเเล้ว เลือกอีก พอจ่ายตังค์ใส่ถุง ผักหักสะงั้น\n",
      "171 cashier ใช้เครื่องรูดไม่เป็น ไม่รู้จัก paywave\n",
      "173 คืออ่านจนเหนื่อยค่ะ\n",
      "179 ไม่จำเป็นไม่เข้าจริงๆ\n",
      "181 แมลงสาปให้มันน้อยๆหน่อย\n",
      "191 ห้องน้ำควรมีสายฉีดตูดนะครับ  ทั้งห้างสีเขียวและห้างสีเหลือง\n",
      "203 กลิ่น\n",
      "205 อยากให้คิดตัวเร็วๆหน่อยต่อคิวรอจ่ายตังกันยาวชิยหาย\n",
      "210 ซื้อไฃ่เจอไข่เน่าเอาไข่ดีๆมาขายบางนะ\n",
      "213 ถามบัตรคลับการ์กูด้วย กูไม่อยากบอกเอง\n",
      "217 หนูวิ่งเป็นของสด\n",
      "218 ตัดบิล กะ ลืมยิงคลับการ์ดให้\n",
      "221 โปรโมชั่นกำกวมมากกกก\n",
      "222 มาตรฐานพลังงานต่ำมากถ้าเทียบกับเซเว่นและบิ๊กซี\n",
      "223 อยากให้ห้องนํ้ามีนํ้า!!\n",
      "232 แอร์ไม่เย็น\n",
      "237 คูปองไม่ต้องส่งมาที่บ้าน อยากได้ในบิลเลย!!!\n",
      "248 แล้วแต่ที่จริงๆ\n",
      "254 แอร์ไม่เปิด\n",
      "260 แสตมป์คลับการ์ดควรมีแบบเดิมนะ\n",
      "264 .\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, marks, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e71afb6711946ed9f9a774901c34eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพียว ได้แล้ว!!!!\n",
      "8 อย่าบังคับน้องเข้าประชุมเชียร์ อย่าลงโทษโดยเหตุผลงี่เง่าๆ เกิดก่อนไม่กี่ปีเอง\n",
      "14 เลือกผัก เลือกแล้ว เลือกอีก พอมาคิดเงินพนักงานยัดผักใส่ถุงจนผักหัก โอ้ยใจ ใจสลาย\n",
      "29 โลตลาด พนง.ปากตลาด นินทาลค.เผาขน ตะโกนโหวกเหวกข้ามหัวลค. วันหวยออกสนั่นเป็นพิเศษ เห็นพฤติกรรมแล้วแย่ ปรับปรุงภาพลักษณ์เถอะ\n",
      "35 Lotus สาขายโสธร-เปิดใหม่ พนง.แอบโกง Give Card ลูกค้า(กุเอง) จ่ายหลายใบแล้วทำเนียนว่ามีใบนึงชำรุดใช้ไม่ได้แล้วพอขอคืนก็สลับใบที่รูดใช้แล้วมาแทน แจ้งผู้จัดการขอดูกล้องวงจรปิดไม่ยอมให้ดูเลยต้องขู่ว่าจะแจ้งความถึงยอมคืนให้ แต่ไม่มีคำขอโทษใดๆ จาก พนง.-ติด…See more\n",
      "43 สาขาพัทลุง เหม็นคาวมากกกกกกกกก แถมแผนกของกินของสดแม่งไปไหนหมด มีแต่เสื้อผ้าละ\n",
      "56 ของมึงไม่ต้องถูกมากก็ได้ เน้นคุณภาพบ้าง ไส้กรอกเน่า แต่ยังไม่หมดอายุงี้อ่ะ (สาขาฟอร์จูน) ขนมปังขึ้นราแล้วยังวางขายอีก (สาขาลาดพร้าว) กีวี่หนอนเข้า กะหล่ำปลีเน่าใน ผักบุ้งเน่าแทรกในมัด (สาขาอ่อนนุช)\n",
      "60 ของป้ายเหลืองพวกผัก เน่าจนไม่รู้จะเน่ายังไง คือทิ้งๆไปก็ได้มั่ง\n",
      "68 ช่องจ่ายเยอะ เยอะเกินพนง.ใช่ไหม ไม่ระบุสาขานะ เกือบทุกที่ที่ไปมาเลยค่ะ\n",
      "75 เวลาผมซื้อเนื้อสัตว์ช่วยเอาที่คีบหรือช้อนหรืออะไรก็ได้มาวางให้ทีอย่าให้กุต้องใช้มือเปล่าๆจับอีกอย่างจะเก็บช้อนกะที่คีบเร็วไปไหนเพิ่งจะ6โมงเย็นเห็นเอาไปล้างแล้ว\n",
      "83 เลิกก๊อปปี้สินค้าแบรนด์ แล้วทำคุณภาพห่วยๆ แม่งเล่นเลย์ซะเหมือนเลย สาดดด\n",
      "88 ฝากถึง สาขาแพร่นะคะตอนเช้าๆเลิกมาทอดโดนัทด้านนอกซะทีเถอะค่ะ กลิ่นเหม็นน้ำมันทอดติดผมเผ้าเสื้อผ้า\n",
      "95 โลตัส รัตนาธิเบศร์ แอร์เหม็นมาก เหม็นสาบหนูสุดๆ ตรงโซนอาหารแห้ง บะหมี่กึ่งสำเร็จรูป เดินผ่านก็รู้เลยมีหนู ตรงโซนข้าวสารอีก\n",
      "98 เวลาจัดโปรแลกของโลตัส เอ็กเพรส ถ้าของหมดให้จอง ให้เขียนชื่อใส่เบอร์โทร แต่เวลาของมาไม่เคยโทรมาบอกเลย ให้ใส่ทำมะเขือไรอะคะ ไม่เข้าใจอะ แล้วเคยโทรไปคอลเซนเตอร์ มีระบบให้ฝากเบอร์ไว้ ก็โทรกลับมาจริงๆนะ ผ่านไปเดือนกว่าถึงโทรกลับมา ไม่รอครบปีก่อนเลยล่ะ แหม่\n",
      "99 โสตัสเอ็กเพรส สาขาตลาดหนองจอก ระยอง ตู้แช่เย็นฝาตู้ปิดไม่สนิททุกตู้\n",
      "108 ของเซลคือเซลเเต่หมดอายุมาหลายเดือนเเล้วยังเซลได้ด้วยหรอ???(ของกินไงบางที)\n",
      "116 วันใหนมาตั้งใจมาชื้อหมู หมูหมดเหลือไก่  วันใหนชื้อไก่ไก่หมดได้หมู  ตั้งใจอย่างได้อีกอย่าง\n",
      "126 วางของเกะกะมาก เป็นแทบทุกสาขาที่เคยเข้าเลย ไม่มีมาตรการจัดของกันเลยหรอ #เอ็กซ์เพรส\n",
      "127 ถ้ามึงไม่อยากไหว้ ไม่ต้องยกมือไหว้ก้อได้ กูรู้มึงโดนบังคับ ให้ไหว้คนที่ไม่ใช่พ่อใช่แม่ เยสเข้\n",
      "135 เอ็กเพรสนี้แลกของแล้วไม่เคยจะโทรบอกเลยหรืออะไรเลยต้องได้ทวง. ชอบไม่ให้แสตมป์. เป็นไรมากไหมนี้\n",
      "140 เคือง...สะสมสแตมป์แลกร่มผ่านมา3ปีจนสาขาย่อยปิดไปแล้วข้ายังไม่ได้ร่มเลย/หลอกลวงถามทีไรกำลังผลิตๆที่ดาวดวงใหนรึ?\n",
      "144 เครื่องที่ให้แสกนของเองน่ะ อยากให้ปรับตำแหน่งให้มันแสกนจากมือถือได้ด้วยค่ะ บัตรหายไปแล้วแสกนจากมือถือไม่ได้มันติด\n",
      "165 ไปทีไรไม่เห็นมีดอกบัวขาย\n",
      "167 ตอนเลือกซื้อผัก เลือกเเล้ว เลือกอีก พอจ่ายตังค์ใส่ถุง ผักหักสะงั้น\n",
      "171 cashier ใช้เครื่องรูดไม่เป็น ไม่รู้จัก paywave\n",
      "173 คืออ่านจนเหนื่อยค่ะ\n",
      "179 ไม่จำเป็นไม่เข้าจริงๆ\n",
      "181 แมลงสาปให้มันน้อยๆหน่อย\n",
      "191 ห้องน้ำควรมีสายฉีดตูดนะครับ  ทั้งห้างสีเขียวและห้างสีเหลือง\n",
      "203 กลิ่น\n",
      "205 อยากให้คิดตัวเร็วๆหน่อยต่อคิวรอจ่ายตังกันยาวชิยหาย\n",
      "210 ซื้อไฃ่เจอไข่เน่าเอาไข่ดีๆมาขายบางนะ\n",
      "213 ถามบัตรคลับการ์กูด้วย กูไม่อยากบอกเอง\n",
      "217 หนูวิ่งเป็นของสด\n",
      "218 ตัดบิล กะ ลืมยิงคลับการ์ดให้\n",
      "221 โปรโมชั่นกำกวมมากกกก\n",
      "222 มาตรฐานพลังงานต่ำมากถ้าเทียบกับเซเว่นและบิ๊กซี\n",
      "223 อยากให้ห้องนํ้ามีนํ้า!!\n",
      "232 แอร์ไม่เย็น\n",
      "237 คูปองไม่ต้องส่งมาที่บ้าน อยากได้ในบิลเลย!!!\n",
      "248 แล้วแต่ที่จริงๆ\n",
      "254 แอร์ไม่เปิด\n",
      "260 แสตมป์คลับการ์ดควรมีแบบเดิมนะ\n",
      "264 .\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, marks, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w3 = Widget(result, marks, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 164\n",
    "compare = 145\n",
    "\n",
    "a = numpy.array(onehot_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(onehot_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "print(filtered_corpus[seed])\n",
    "print(filtered_corpus[compare])\n",
    "\n",
    "# print(sims[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
