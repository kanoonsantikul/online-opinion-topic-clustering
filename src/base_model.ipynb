{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "import pythainlp.word_vector\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus, clean\n",
    "\n",
    "from model.new_sdc import NewSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 229\n"
     ]
    }
   ],
   "source": [
    "file_name = '‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ù‡∏∏‡πà‡∏ô'\n",
    "\n",
    "corpus = load_corpus('../data/' + file_name + '.txt')\n",
    "print('Total documents', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('../data/tokenized/newmm/tokenized_' + file_name + '.txt')\n",
    "f = open('../data/tokenized/deepcut/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1401 words\n",
      "filter frequent words: 658 words\n",
      "filter letter words: 654 words\n",
      "filter stop words: 428 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.common.thai_stopwords()\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanoonsantikul/Documents/senior-project/venv/lib/python3.6/site-packages/pythainlp/word_vector/__init__.py:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "  vec /= len(words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan\n"
     ]
    }
   ],
   "source": [
    "def token_to_string(tokenized_list):\n",
    "    string = str(tokenized_list)\n",
    "    string.replace(',', ' ')\n",
    "    return clean(string)\n",
    "\n",
    "doc2vec_corpus = []\n",
    "for doc in tokenized_corpus:\n",
    "    array = pythainlp.word_vector.sentence_vectorizer(token_to_string(doc))[0]\n",
    "    if any(numpy.isnan(array)):\n",
    "        print('Nan')\n",
    "        array = numpy.zeros(300)\n",
    "    doc2vec_corpus.append(array)\n",
    "doc2vec_corpus = pandas.DataFrame(doc2vec_corpus, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vectors(corpus, value):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if value == 'bow':\n",
    "        vectors = bow_corpus\n",
    "    elif value == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        vectors = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(vectors):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if value == 'bow' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(document_vectors, result):\n",
    "    num_cluster = numpy.unique(result['predicted_label'])[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(document_vectors.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(document_vectors.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.75\n",
    "expand_rate = 0.05\n",
    "epoch = 15\n",
    "\n",
    "document_vectors = get_document_vectors(idx_corpus, 'bow')\n",
    "# document_vectors = doc2vec_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_compactness = 0\n",
    "for i in range(epoch):\n",
    "    model = NewSDC()\n",
    "    _tpredicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate)\n",
    "    \n",
    "#     marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(document_vectors)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "#     model = KMeans(n_clusters=14).fit(document_vectors)\n",
    "#     _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(document_vectors, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)\n",
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "num_cluster = len(label_count)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative New SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.99646535408963\n",
      "[229]\n"
     ]
    }
   ],
   "source": [
    "centroids = None \n",
    "prev_label_count = None\n",
    "model = NewSDC()\n",
    "while True:\n",
    "    predicted_labels, marks = model.predict(document_vectors, min_samples, eps, expand_rate, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(document_vectors, result)\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    if numpy.array_equal(label_count, prev_label_count):\n",
    "        break\n",
    "    prev_label_count = label_count\n",
    "    centroids = centroids[1:]\n",
    "    \n",
    "    print(compactness)\n",
    "    print(label_count)\n",
    "num_cluster = len(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 [229]\n",
      "0.02 [229]\n",
      "0.03 [229]\n",
      "0.04 [229]\n",
      "0.05 [229]\n",
      "0.06 [229]\n",
      "0.07 [229]\n",
      "0.08 [229]\n",
      "0.09 [229]\n",
      "0.10 [229]\n",
      "0.11 [229]\n",
      "0.12 [229]\n",
      "0.13 [229]\n",
      "0.14 [229]\n",
      "0.15 [229]\n",
      "0.16 [229]\n",
      "0.17 [229]\n",
      "0.18 [229]\n",
      "0.19 [229]\n",
      "0.20 [229]\n",
      "0.21 [229]\n",
      "0.22 [229]\n",
      "0.23 [229]\n",
      "0.24 [229]\n",
      "0.25 [229]\n",
      "0.26 [229]\n",
      "0.27 [229]\n",
      "0.28 [229]\n",
      "0.29 [229]\n",
      "0.30 [213   9   7]\n",
      "0.31 [213   9   7]\n",
      "0.32 [213   9   7]\n",
      "0.33 [213   9   7]\n",
      "0.34 [213   9   7]\n",
      "0.35 [213   9   7]\n",
      "0.36 [213   9   7]\n",
      "0.37 [212  10   7]\n",
      "0.38 [212  10   7]\n",
      "0.39 [212  10   7]\n",
      "0.40 [212  10   7]\n",
      "0.41 [205   7  10   7]\n",
      "0.42 [204   7  11   7]\n",
      "0.43 [199  10  20]\n",
      "0.44 [198  11  20]\n",
      "0.45 [189   8  11  21]\n",
      "0.46 [187   8  12  22]\n",
      "0.47 [186   9  12  22]\n",
      "0.48 [183   9  14  23]\n",
      "0.49 [181   9  16  23]\n",
      "0.50 [165  10  28  20   6]\n",
      "0.51 [152  61   9   7]\n",
      "0.52 [147  65   9   8]\n",
      "0.53 [141  71   9   8]\n",
      "0.54 [139  73   9   8]\n",
      "0.55 [137  75   9   8]\n",
      "0.56 [125  88   9   7]\n",
      "0.57 [124 105]\n",
      "0.58 [113 116]\n",
      "0.59 [109 120]\n",
      "0.60 [ 91 132   6]\n",
      "0.61 [ 88 135   6]\n",
      "0.62 [ 84 145]\n",
      "0.63 [ 81 148]\n",
      "0.64 [ 75 154]\n",
      "0.65 [ 63 166]\n",
      "0.66 [ 58 171]\n",
      "0.67 [ 48 181]\n",
      "0.68 [ 47 182]\n",
      "0.69 [ 40 189]\n",
      "0.70 [ 37 192]\n",
      "0.71 [ 34 195]\n",
      "0.72 [ 30 199]\n",
      "0.73 [ 27 202]\n",
      "0.74 [ 22 207]\n",
      "0.75 [ 17 212]\n",
      "0.76 [ 16 213]\n",
      "0.77 [ 14 215]\n",
      "0.78 [ 11 218]\n",
      "0.79 [ 11 218]\n",
      "0.80 [ 10 219]\n",
      "0.81 [ 10 219]\n",
      "0.82 [ 10 219]\n",
      "0.83 [ 10 219]\n",
      "0.84 [  9 220]\n",
      "0.85 [  9 220]\n",
      "0.86 [  8 221]\n",
      "0.87 [  8 221]\n",
      "0.88 [  8 221]\n",
      "0.89 [  8 221]\n",
      "0.90 [  8 221]\n",
      "0.91 [  8 221]\n",
      "0.92 [  8 221]\n",
      "0.93 [  8 221]\n",
      "0.94 [  8 221]\n",
      "0.95 [  8 221]\n",
      "0.96 [  8 221]\n",
      "0.97 [  8 221]\n",
      "0.98 [  8 221]\n",
      "0.99 [  8 221]\n"
     ]
    }
   ],
   "source": [
    "eps = 0.01\n",
    "marks = None\n",
    "\n",
    "while eps <= 1:\n",
    "    max_compactness = 0\n",
    "    for i in range(epoch):\n",
    "        model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(document_vectors)\n",
    "        _tpredicted_labels = model.labels_ + 1\n",
    "        \n",
    "        _tresult = generate_result(_tpredicted_labels, marks)\n",
    "        compactness, _tcentroids = eval_cluster(document_vectors, _tresult)\n",
    "\n",
    "        if compactness > max_compactness:\n",
    "            max_compactness = compactness\n",
    "            predicted_labels = _tpredicted_labels\n",
    "            result = _tresult\n",
    "            centroids = _tcentroids\n",
    "    \n",
    "    label_count = numpy.unique(result['predicted_label'], return_counts=True)[1]\n",
    "    print('%.2f' % eps, label_count)\n",
    "    eps += 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "8 7 0.8755721909893287\n",
      "8 6 0.8586066011239457\n",
      "8 5 0.8507223950545542\n",
      "8 4 0.875182053956368\n",
      "8 3 0.8589657405412545\n",
      "8 2 0.8675732251896662\n",
      "8 1 0.8875363722048335\n",
      "7 6 0.9384452834460689\n",
      "7 5 0.9141454743659437\n",
      "7 4 0.9521101960285854\n",
      "7 3 0.8930926351335264\n",
      "7 2 0.9101058935577682\n",
      "7 1 0.9658037508375265\n",
      "6 5 0.9025615013232784\n",
      "6 4 0.9354670147584043\n",
      "6 3 0.9285334363748009\n",
      "6 2 0.9410823368053696\n",
      "6 1 0.9634998744557152\n",
      "5 4 0.9131194410648852\n",
      "5 3 0.8498011498062978\n",
      "5 2 0.9004323324387875\n",
      "5 1 0.9329955899312412\n",
      "4 3 0.9034467677178339\n",
      "4 2 0.9273065499085396\n",
      "4 1 0.9723202135430872\n",
      "3 2 0.9125217520384445\n",
      "3 1 0.930462408162529\n",
      "2 1 0.9438673547511558\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len(corpus))\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.78241587508978\n",
      "[ 5 74 24 15 11 39 16 13 32]\n"
     ]
    }
   ],
   "source": [
    "# result.to_csv('../data/results/iterative_new_sdc/' + file_name + '_2.csv', index=False)\n",
    "\n",
    "# result = pandas.read_csv('../data/results/iterative_new_sdc/' + file_name + '.csv')\n",
    "\n",
    "print(eval_cluster(document_vectors, result)[0])\n",
    "print(numpy.unique(result['predicted_label'], return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260a2a55b3dd42b9bc6cf5aca9088c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 This shows everything about Thailand\n",
      "183 ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÇ‡∏à‡∏£\n",
      "187 ‡∏£‡∏ñ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏±‡∏ô 555    ‡∏Å‡∏π‡∏à‡∏≥‡πÑ‡∏î‡πâ\n",
      "220 +1\n",
      "227 https//www.posttoday.com/social/general/577217\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59a93a28c9f48828ec120796f701c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=4, options=(0, 1, 2, 3, 4, 5, 6, 7, 8), value=4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*7 ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ï‡πà‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏Ç‡∏≠‡∏á ‡∏Å‡∏ó‡∏°. ‡∏™‡∏±‡∏Å‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏î‡∏à‡∏ô‡∏°‡∏±‡∏ô‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏ô‡∏Å‡∏∏‡∏î ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏ñ‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏£‡∏∏‡∏Å‡∏Ç‡∏Å‡∏£‡∏ì‡πå ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ï‡πà‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏Ñ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡πÅ‡∏ï‡πà‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏ù‡∏∏‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏•‡∏û‡∏¥‡∏© ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß\n",
      "24 ‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏ö‡∏ö ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏±‡∏ô‡∏î‡πà‡∏ß‡∏ô ‡∏Å‡πá‡πÄ‡∏•‡∏¢‡πÑ‡∏°‡πà‡∏ß‡∏¥‡∏ï‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®...‡∏°‡∏±‡∏á‡∏Ñ‡∏∞..\n",
      "*43 ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏•‡∏π‡∏Å‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏ñ‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏™‡∏≤‡∏ò‡∏á ‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÑ‡∏õ‡∏Ñ‡πà‡∏≤‡∏¢‡∏•‡∏π‡∏Å‡πÄ‡∏™‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏â‡∏¢ ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢‡∏°‡∏±‡∏ô‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÅ‡∏´‡∏•‡∏∞ ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ñ‡πâ‡∏≤‡∏´‡∏ß‡∏±‡∏á‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤‡∏£‡∏±‡∏ê‡∏Å‡πá‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡πÑ‡∏õ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ ‡πÅ‡∏ï‡πà‡∏Å‡πá‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏´‡∏°‡∏≠‡∏ô‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≠‡∏Å‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡πà‡∏ß‡∏á‡πÉ‡∏¢\n",
      "*55 ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‚Äã‡∏Ñ‡∏∏‡∏ì‡∏´‡∏°‡∏≠‡∏Ñ‡πà‡∏∞‚Äã ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‚Äã‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°‚Äã‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏±‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‚Äã ‡∏°‡∏µ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏±‡∏ß‚Äã‡πÄ‡∏≠‡∏á‚Äã ‡πÄ‡∏â‡∏¢‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‚Äã ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏∑‡πà‡∏ô‡∏ï‡∏±‡∏ß‚Äã‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß‚Äã ‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‚Äã‡∏£‡∏π‡πâ‚Äã‡πÅ‡∏ï‡πà‡∏Å‡∏•‡∏±‡∏ö‚Äã‡πÄ‡∏â‡∏¢\n",
      "78 ‡∏Ñ‡∏∏‡∏ì‡∏´‡∏°‡∏≠‡∏Ñ‡∏∞ ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏±‡∏ê‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ. ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏Ñ‡∏ß‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏è‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏∞. ‡πÄ‡∏´‡πá‡∏ô‡∏ï‡∏±‡∏î‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πà‡∏ô ‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÜ‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏±‡πâ‡∏ô üò™\n",
      "*82 ‡∏°‡∏±‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ù‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ú‡∏•‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß ‡πÄ‡∏•‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡πÑ‡∏°‡πà‡∏ï‡∏∑‡πà‡∏ô‡∏Å‡∏•‡∏±‡∏ß ‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏±‡∏ê‡∏Å‡πá‡∏ö‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£ ‡∏™‡∏á‡∏™‡∏≤‡∏£‡πÄ‡∏î‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏£‡∏£.‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞\n",
      "*84 ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ú‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏¥‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πá‡∏õ‡∏ô‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô‡∏™‡∏≤‡∏£‡∏û‡∏¥‡∏©‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏¢‡∏±‡∏á‡∏ô‡∏¥‡πà‡∏á‡πÄ‡∏â‡∏¢‡πÑ‡∏°‡πà‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏≤‡∏£‡πÄ‡∏Ñ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‚Äã‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏Å‡πá‡πÄ‡∏â‡∏¢‡πÜ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÄ‡∏ú‡∏ä‡∏¥‡∏ç‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏¥‚Äã‡πÑ‡∏õ‡∏°‡∏±‡∏ô‡∏ô‡∏≤‡∏Å‡∏•‡∏±‡∏ß‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ‡∏•‡πà‡∏∞\n",
      "*91 ‡∏ñ. ‡∏®‡∏≤‡∏•‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏™‡∏û‡∏ô‡πå ‡∏ï‡∏≠‡∏ô‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠15‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß  ‡∏°‡∏µ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∏‡πÇ‡∏°‡∏á‡∏Ñ‡πå‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÄ‡∏•‡∏¢  ‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ñ‡∏ô‡∏ô  ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÇ‡∏î‡∏ô‡∏Ç‡∏∏‡∏î‡∏´‡∏°‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß  ‡πÅ‡∏ñ‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏ñ‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡πÄ‡∏®‡∏©‡∏Ç‡∏∂‡πâ‡πÄ‡∏ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ú‡∏≤‡∏û‡∏∑‡∏ä‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏õ‡∏´‡∏°‡∏î‡∏•‡∏≠‡∏¢‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ  ‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏á\n",
      "*102 ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏¢‡∏±‡∏á‡πÇ‡∏î‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê‡∏ï‡∏±‡∏î‡∏à‡∏ô‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏•‡∏¢ ‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏≤‡∏¢‡πÑ‡∏ü‡∏Å‡πá‡∏ï‡∏±‡∏î  ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πà‡πÅ‡∏´‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏°‡∏±‡∏ô‡∏ñ‡∏∂‡∏á‡πÅ‡∏¢‡πà‡πÑ‡∏á ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏°‡πà‡∏¢‡∏±‡∏Å‡∏Å‡∏∞‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏£‡∏ì‡∏£‡∏á‡∏Ñ‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡∏õ‡∏•‡∏π‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ô‡∏∞ ‡∏Ñ‡∏á‡∏Å‡∏•‡∏±‡∏ß‡∏Ñ‡∏ô‡∏î‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏±‡πâ‡∏á\n",
      "*166 ‡∏¢‡∏≤‡∏Å‡∏°‡∏≤‡∏Å ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏ã‡∏¥‡∏á‡πÄ‡∏Å‡∏¥‡πâ‡∏•‡∏°‡∏±‡∏°‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏Å‡πá‡∏´‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÜ\n",
      "@167 ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏û‡∏≠‡∏°‡∏µ‡πÉ‡∏ö‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Å‡πá‡∏ï‡∏±‡∏î‡∏ã‡∏∞‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏ô ‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏Å‡πá‡∏≠‡∏¢‡πà‡∏≤‡∏õ‡∏•‡∏π‡∏Å‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡πà‡∏≤‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28493f7625f4ccd91c3e430f3c3b161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 []\n",
      "183 ['‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•']\n",
      "187 ['‡∏£‡∏ñ', '‡∏•‡πâ‡∏≤‡∏ô', '‡∏Ñ‡∏±‡∏ô']\n",
      "220 []\n",
      "227 []\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "['‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏´‡∏¢‡∏∏‡∏î', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡∏ä‡∏±‡πâ‡∏ô', '‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡πÇ‡∏£‡∏á', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏ä‡∏°']\n",
      "‡∏£‡∏£.‡∏£‡∏∏‡πà‡∏á‡∏≠‡∏£‡∏∏‡∏ì‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô ‡∏ä‡∏∑‡πà‡∏ô‡∏ä‡∏°‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏à‡∏Ñ‡πà‡∏∞ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏ß 1 ‡∏ä‡∏°.‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏Ñ‡πà‡∏∞\n",
      "['‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏´‡∏¢‡∏∏‡∏î', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡∏ä‡∏±‡πâ‡∏ô', '‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡πÇ‡∏£‡∏á', '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏ä‡∏°']\n",
      "‡∏£‡∏£.‡∏£‡∏∏‡πà‡∏á‡∏≠‡∏£‡∏∏‡∏ì‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô ‡∏ä‡∏∑‡πà‡∏ô‡∏ä‡∏°‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏à‡∏Ñ‡πà‡∏∞ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏ß 1 ‡∏ä‡∏°.‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏Ñ‡πà‡∏∞\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(document_vectors.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(document_vectors.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seniorproject",
   "language": "python",
   "name": "seniorproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
