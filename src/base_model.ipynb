{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_tokenizer import load_corpus\n",
    "\n",
    "from model.upgrade_sdc import UpgradeSDC\n",
    "from model.sdc import SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 350\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TrueCoffee'\n",
    "\n",
    "corpus, labels = load_corpus('../data/' + file_name + '.txt')\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/tokenized/tokenized_' + file_name + '.txt')\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 948 words\n",
      "filter frequent words: 370 words\n",
      "filter letter words: 368 words\n",
      "filter stop words: 219 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.extend(['นี้'])\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(idx_corpus)]\n",
    "model = Doc2Vec(tagged_corpus, vector_size=average_doc_size, window=4, min_count=2, epochs=100)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "paragraph_vectors = [model.infer_vector(doc) for doc in idx_corpus]\n",
    "paragraph_vectors = pandas.DataFrame(paragraph_vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(corpus, weight):\n",
    "    dictionary = Dictionary(corpus)\n",
    "#     dictionary.filter_extremes(no_below=2, no_above=1, keep_n=len(dictionary))\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "    if weight == 'normal':\n",
    "        weight_corpus = bow_corpus\n",
    "    elif weight == 'tfidf':\n",
    "        tfidf = TfidfModel(bow_corpus, smartirs='ltc')\n",
    "        weight_corpus = [tfidf[doc] for doc in bow_corpus]\n",
    "\n",
    "    unique_words = [dictionary[id] for id in range(len(dictionary))]\n",
    "    array = numpy.zeros((len(corpus), len(unique_words)), dtype=float)\n",
    "    for i, doc in enumerate(weight_corpus):\n",
    "        for id, score in doc:\n",
    "            array[i, id] = score\n",
    "\n",
    "        if weight == 'normal' and len(doc) != 0:\n",
    "#             array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "            array[i] = numpy.divide(array[i], len(doc))\n",
    "    \n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(predicted_labels, marks):\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = idx_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    if marks:\n",
    "        result['marks'] = marks\n",
    "    else:\n",
    "        result['marks'] = -1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cluster(onehot_corpus, result):\n",
    "    label_count = numpy.unique(result['predicted_label'])\n",
    "    num_cluster = label_count[-1] + 1\n",
    "\n",
    "    clusters = [[] for i in range(num_cluster)]\n",
    "    corpus_centroid = []\n",
    "    for i, label in result['predicted_label'].iteritems():\n",
    "        clusters[label].append(numpy.array(onehot_corpus.iloc[i]))\n",
    "        corpus_centroid.append(numpy.array(onehot_corpus.iloc[i]))\n",
    "    corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "#     print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "    compactness = 0\n",
    "    centroids = []\n",
    "    for i in range(num_cluster):\n",
    "        size = len(clusters[i])\n",
    "        if size != 0:\n",
    "            centroid = numpy.mean(clusters[i], axis=0)\n",
    "            centroids.append(centroid)\n",
    "            centroid = centroid.reshape(1, -1)\n",
    "            similarities = cosine_similarity(centroid, clusters[i])\n",
    "            compactness += numpy.sum(similarities)\n",
    "\n",
    "#             intra = numpy.sum(similarities) / size\n",
    "#             inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "#             print(i, end='\\t')\n",
    "#             print(intra, end='\\t')\n",
    "#             print(inter, end='\\t')\n",
    "#             print(intra / inter)\n",
    "    return compactness, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 7\n",
    "eps = 0.32\n",
    "\n",
    "onehot_corpus = get_onehot(idx_corpus, 'normal')\n",
    "# onehot_corpus = get_onehot(filtered_corpus, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.566296070018\n"
     ]
    }
   ],
   "source": [
    "max_compactness = 0\n",
    "epoch = 15\n",
    "for i in range(epoch):\n",
    "    model = UpgradeSDC()\n",
    "    _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps)\n",
    "\n",
    "#     model = SDC()\n",
    "#     _tpredicted_labels, marks = model.predict(onehot_corpus, min_samples, eps)\n",
    "    \n",
    "#     marks = None\n",
    "    \n",
    "#     model = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_ + 1\n",
    "\n",
    "#     model = KMeans(n_clusters=7).fit(onehot_corpus)\n",
    "#     _tpredicted_labels = model.labels_\n",
    "    \n",
    "    _tresult = generate_result(_tpredicted_labels, marks)\n",
    "    compactness, _tcentroids = eval_cluster(onehot_corpus, _tresult)\n",
    "    \n",
    "    if compactness > max_compactness:\n",
    "        max_compactness = compactness\n",
    "        predicted_labels = _tpredicted_labels\n",
    "        result = _tresult\n",
    "        centroids = _tcentroids\n",
    "        \n",
    "print(max_compactness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.43197277018987 73\n"
     ]
    }
   ],
   "source": [
    "centroids = None \n",
    "for i in range(1):\n",
    "    model = UpgradeSDC()\n",
    "    if centroids:\n",
    "        centroids = centroids[1:]\n",
    "    predicted_labels, marks = model.predict(onehot_corpus, min_samples, eps, seeds=centroids)\n",
    "    \n",
    "    result = generate_result(predicted_labels, marks)\n",
    "    compactness, centroids = eval_cluster(onehot_corpus, result)\n",
    "            \n",
    "    print(compactness, numpy.unique(result['predicted_label'], return_counts=True)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), array([82, 46, 37, 44, 18, 21,  8, 15, 12, 21, 11, 14, 10, 11])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True)\n",
    "num_cluster = label_count[0][-1] + 1\n",
    "print(label_count, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(centroids)\n",
    "new_labels = [i for i in range(num_cluster)]\n",
    "print(new_labels)\n",
    "for i, row in reversed(list(enumerate(sims))):\n",
    "    for j, value in reversed(list(enumerate(row[:i + 1]))):\n",
    "        if i != j and value >= eps - eps / 20:\n",
    "            print(i, j, value)\n",
    "            base = min(new_labels[i], new_labels[j])\n",
    "            new_labels[j] = base\n",
    "            new_labels = [base if label == new_labels[i] else label for label in new_labels]\n",
    "print(new_labels)\n",
    "\n",
    "grouped_labels = numpy.zeros(len_corpus)\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    grouped_labels[i] = new_labels[label]\n",
    "new_result = generate_result(grouped_labels, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Widget:\n",
    "    def __init__(self, result, column_name):\n",
    "        self.result = result\n",
    "        self.column_name = column_name\n",
    "        \n",
    "        label_count = numpy.unique(result['predicted_label'])\n",
    "        self.widget = widgets.ToggleButtons(\n",
    "            options=[int(num) for num in label_count],\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "        \n",
    "        self.widget.observe(self.on_click, names='index')\n",
    "        self.on_click({'new' : 0})\n",
    "        \n",
    "    def on_click(self, change):\n",
    "        clear_output()\n",
    "        display(self.widget)\n",
    "        new = self.widget.options[change['new']]\n",
    "        for index, value in self.result[self.result['predicted_label'] == new].iterrows():\n",
    "            if value['marks'] == 0:\n",
    "                print(\"@\", end=\"\")\n",
    "            elif value['marks'] == 1:\n",
    "                print(\"*\", end=\"\")\n",
    "            print(index, value[self.column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "result.to_csv('../data/results/em/' + file_name + '.csv')\n",
    "\n",
    "# result = pandas.read_csv('../data/results/2/' + file_name + '.csv')\n",
    "\n",
    "count = 0\n",
    "for index, value in result.iterrows():\n",
    "    if value['marks'] == -1:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61058cf79bec49f78d3a9ace84afdc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 เรางง สั่งคาปูชิโนหวานน้อย ทำไม่ได้ค่ะ ถ้าลูกค้าอยากได้หวานน้อยต้องสั่งลาเต้ กูงงงงงงง\n",
      "4 ซื้อลิขสิทธิ์ UCL มาด้วยนะ จ่ายรายเดือนไป แต่ไม่มีให้ดู\n",
      "12 ผมว่า ผมชอบรสชาติของกาแฟคุณ หลายๆแก้วประทับใจกว่า แบรนด์นางเงือกแต่ว่า แพงไปหว่ะ ไม่เหมาะกะคนไทย (ถึงแม้จะใช้ส่วนลด ลูกค้าทรู แล้วก้ตาม)สังเกตได้ว่า น้อยสาขา ที่คนจะแน่น หรือต้องต่อคิวสัีงกาแฟ เหมือนแบรนด์นางเงือก…See more\n",
      "17 Wi-Fi ทรูที่บ้าน ใช้ได้แต่กลางคืน กลางวันใช้ไม่ได้ แต่จ่ายพันกว่าบาททุกเดือน ควรปรับปรุง ฝากทรูคอฟฟี่ไปบอกด้วย\n",
      "21 ทำไมเอาเมนูทวิซที่ปั่นใส่ขนมช็อกโกแลตทวิซคาลาเมลออก 💥🔥🔥🔥โกรธหนักค่ะ เพราะชอบมากกกกกกกก\n",
      "24 ขนาดตอนเป็นลูกค้าทรูมูฟ ยังไม่กิน ตอนนี้ย้ายค่ายแล้ว อย่าหวังจะได้แดก\n",
      "28 ขอทำขนมไปส่งได้ไหมคะ นักเรียนบ้านยากจนจะสอนนักเรียนแต่ไม่มีที่ให้โอกาสนักเรียนคะ แต่รับรองขนมอร่อยคะ\n",
      "42 ตัวอักษรใหญ่ๆ มองเมนูไม่ชัด ไม่กล้าสั่ง กลัวสั่งผิดละเขินน 555+\n",
      "45 บอกพนักงงานชงกาแฟทำหน้าตาดีๆ เลิกกระแทกแก้วสักที เวลาส่งคืนลูกค้าเยอะๆอ่ะ ละก็ช่วยทำคอมเม้นลูกค้าให้มันเป็นตามที่ขอด้วยอิเวน\n",
      "48 ราคาที่ตั้งนี่ เพื่ิอ ขายนิล อาร์มสตรองที่ดวงจันทร์ หรือ นักบินอวกาศที่กระสวยนาซ่าเหรอ  ....\n",
      "52 ตอนปรับโปรใหม่เดือนเเรกเนตอย่างเเรง ผ่านไปสักพัก ช้าโคตร ส่วนกาเเฟงั้นๆ ราคาเเพงไป\n",
      "54 พนง.น่ารักๆ เยอะนะ แต่เพราะชื่อ ทรู นี่แหละตรูเลยไม่เข้า 5555\n",
      "57 เป็นร้านที่แดกกาแฟร้อนทีไร แม่งต้องล้วกลิ้นไหม้เป็นแผลตลอด น้ำร้อนเอ็งจะร้อนไปไหนวะ สงสัยแม่งเอามาจากบ่อในนรกชัวร์ๆ ถ้าเอ็งร้อนแบบนี้\n",
      "68 โปรมีให้ลูกค้าน้อยนิด กดไม่เคยทันคนอื่น บริษัทก็ใหญ่โต ใจกว้างๆคืนกำไรให้ลูกค้าบ้างเถิดค่ะ\n",
      "70 Iced Black Tea Latte หาชงอร่อยยากมาก อยากกินทีต้องลุ้นทีว่ามือชงจะทำได้แค่ไหน เข้าร้านทรูเพื่อการนี้โดยเฉพาะ เมื่อไม่เจอมือเซียนชง ก็ไม่เข้าไปเหยียบร้านค่ะ\n",
      "71 อยากให้ปรับปรุงเรื่องเครือข่ายคับ บางที่ไม่มีสัญญานเลยครับ เนตข้ามาก\n",
      "75 พนักงานคุยกันไม่สนใจ ลูกค้า ก้มหน้าก้มตาคุยกัน จนต้องบอกสนใจรับออเดอร์หน่อย\n",
      "83 ใช้ค่ายโทรศัพ์ ใช้ wifi โปรแบบแพงๆ มีสิทแบคการ์ดถือไว้หลายปีสิทธ์เต็มตลอดทั้งดูหนังทั้งเครื่องดื่ม สุดท้ายย้ายทิ้งหมดเพราะเรื่องสิทธ์ดูหนังลดราคาเครื่องดื่มนี่แหละ\n",
      "91 ช่วยแจ้งโปรที่ใช้ได้จริง พอไป อะไรก็เต็ม “หวานน้อย “ ของแต่ละสาขาความหวานไม่เท่ากัน หวานน้อยคือ หวาน และ ว๊านหวาน , บอกไม่หวานคือน้ำล้างแก้ว แม้แต่กลิ่นกาแฟยังจาง...\n",
      "93 เนตช้ามากเลยค่ะเดี๋ยวนี้ ขนาดรอบบิลเพิ่งตัด เนต20GB หมุนแล้วหมุนอีก\n",
      "98 คำว่าหวานน้อยไม่มีอยู่จริง\n",
      "100 ปรับปรุงสัญญานมือถือก่อน\n",
      "104 อยากให้ทำโปรเยอะกว่านี้ 🙏\n",
      "105 ทุกสูตรที่มี!!\n",
      "107 เนทกาก ฝนตกปรอยๆทีวีบ้านกุดูไม่ได้ละ Platinum นะเห้ย\n",
      "108 ราคาสูง แต่สิ่งที่ได้กลับมาอิดอก !! ชาพะยอม\n",
      "112 สัญญาณไม่ลื่นเลย เล่น ROV กระตุก\n",
      "117 แพงเกอน แต่รสชาติเอิ่มมมม\n",
      "118 คุณเมิงจะครอบคลุมทุกอย่างเลยแม่นบ่? 555 😂\n",
      "121 ความเป็นออริจินอล\n",
      "126 พนง.หน้าตูด (สาขาเซ้นทรัลพระราม2)\n",
      "131 ไม่เคยแดก\n",
      "134 เครือข่าย ละกัน 555\n",
      "137 สาขาเวสเกต พนักงานโครตแหล่ม\n",
      "146 americano จางมากกกกก ไม่โอเครเลย\n",
      "151 ไม่เคยทาน\n",
      "160 ไปเรียนมาใหม่\n",
      "161 อันนี้ก็ไม่ต่างจากสตาร์บั๊ค\n",
      "165 เราลูกค้าทรู แต่เราไม่เคยได้ใช้สิทธิทรู ในการซื้อทรูคอฟฟี่ เลยยยยยย\n",
      "169 ไม่เคยแดก\n",
      "173 การแต่งตัวพนักงานผู้หญิงสวยมากครับ​ ไม่รู้ว่ากระโปงจะบานไปไหน\n",
      "179 ไม่ค่อยมีคลื่น\n",
      "180 ไม่รู้จะให้ปรับปรุง​อะไร เพราะไม่เคยแดก555\n",
      "183 แย่มาก. ถึงมากที่สุดพนักงานก็แย่. ในบ้างคน\n",
      "185 รสชาด\n",
      "191 สำหรับผม โฮจิชา ขอให้มีขายตลอดปีพอ ปลื้มมาก\n",
      "198 ไม่ต้องปรับปรุงอะไรเลยครับ มึงควรปิดกิจการไปเลย\n",
      "199 รสชาด\n",
      "200 ปรับปรุงเรื่อง อินเตอร์เน็ต ด้วย\n",
      "206 ลาเต้เย็นควรใส่นมเยอะๆนะค่ะ นี่ขมอย่ากะอเมริกาโน่\n",
      "211 พนักงาน\n",
      "216 ปรับปรุงเรื่องราคา กับ ฝีมือการชงของ พนง. ค่ะ 😂\n",
      "218 รรสชาติโอเคแต่มันดูไม่สมราคาเท่าไหร่\n",
      "226 แพงเกิ้นนน\n",
      "230 ไล่ลูกค้านั่งติวนาน(สาด) ที่พาราก้อน\n",
      "231 ไม่กล้าเข้า เราใช้ AIS. 😂\n",
      "235 เหลือแก้วละ50-60จะเป็นช้อยแรกเลยตัด starbuck ทิ้งไป\n",
      "240 เมื่อไหร่จะมี​  dtaccoffee  aiscoffee\n",
      "250 ราคาสูงไปค่ะ\n",
      "255 wifi บ้าน กากมาก หมุนติ้วๆๆ\n",
      "257 รสชาดห่วยสุดในสามโลก\n",
      "263 ครั้งเดียวก็เกินจะพอ\n",
      "264 อันนี้มาแนวเดียวกะแมคคาเฟ่\n",
      "274 ไม่ได้เรื่อง\n",
      "277 ชามะนาว ชาเฝื่อนมากเลยงะ\n",
      "287 ทำไมรู้สึกว่าชาเย็นที่นี่จะออกเค็มๆหน่อย\n",
      "291 โลโก้\n",
      "293 ราคาเกินรสชาต\n",
      "296 ราคาค่ะ หรือมีโปรโมชั่น\n",
      "303 กระจอกมาก\n",
      "307 คนขายไม่สวย\n",
      "311 ลดราคา\n",
      "312 ราคาและรสชาดสวนทางกัน\n",
      "316 สาขาน้อย\n",
      "318 เปลี่ยนชื่อเมนู จากชานมเย็น เป็น ชาไทย\n",
      "331 +1\n",
      "335 ราคา\n",
      "337 ชอบ\n",
      "338 ราคา\n",
      "341 ราคา\n",
      "345 หมาไม่ดืมว่ะ !\n",
      "347 สัญญาณอินเตอร์เน็ตครับ\n"
     ]
    }
   ],
   "source": [
    "w1 = Widget(new_result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08801395c8384c84ae6ca5e9859ec681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=8, options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*18 เครื่องดื่มโบราณ อัดน้ำแข็งแน่นมาก ดูด 3-4 ทีหมด น้ำแข็งยังแน่นแก้วอยู่เลย\n",
      "*29 ซื้อเพราะหาที่นั่งล้วนๆ รสชาติไม่ต้องพูดถึง ปล่อยน้ำแข็งละลายทุกที5555\n",
      "*65 ของปั่น ตระกูลเบอรี่ เปรี้ยวและหวานมากกก รอน้ำแข็งละลายก็ไม่หายเปรี้ยววว\n",
      "*77 พนักงานไม่ควรยื้อเวลาในการทำรายการนานๆเพื่อให้ลูกค้านั่งรอนานจนหิวน้ำนะคะ\n",
      "*78 โกโก้เย็น ตอนชง ถ้ามึงจะละลายน้ำแล้วเป็นตะกอนนอนเล่นก้นแก้วกูว่ามึงเอาไปต้มล่วงหน้าแล้วคนให้มันละลายกับน้ำ ใส่หม้อไว้รินหรือรอตักใส่แก้วเหอะ ดูดที ผงโกโก้ตะกอนจะติดคอตาย สำลักแล้วสำลักอีก\n",
      "*116 น้ำแข็งเค็ม\n",
      "*155 ชาไทยที่เป็นน้ำแข็งป่นน้ำน้อยไป ดูดแปปเดียวหมด\n",
      "*172 น้ำแข็งลดๆลงบ้างนะคะทั้งเมนูแบบปั่นและแบบเย็น\n",
      "*195 อยากลดน้ำแข็งชาไทยหน่อยค่ะ มันแน่นเกิ๊น\n",
      "*220 น้ำแข็งที่ใส่ นึกว่าสั่งชาเย็นพิเศษน้ำแข็ง เยอะกว่าน้ำที้างอีก\n",
      "*224 รสชาติ น้ำแข็งจะอัดเยอะไปไหน\n",
      "*343 รสชาติห่วยทุกน้ำเลย ใช้แต้มทรูแลกเพิ่มตังยังผิดหวัง\n"
     ]
    }
   ],
   "source": [
    "w2 = Widget(result, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98b29aef43f4dc08bdf3a53526e2a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(index=6, options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), value=6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*10 ['ไทย', 'น้ำ', 'แข็ง', 'แน่นแก้ว', 'ขนาด', 'กิน', 'แป๊บ', 'แก้ว', 'น้ำ', 'แข็ง', 'แก้ว']\n",
      "*32 ['แทบ', 'รสชาติ', 'ราคา', 'ซื้อ', 'กิน', 'แก้ว', 'เอิ่ม', 'แย่']\n",
      "*35 ['สิทธิ์', 'ทรู', 'แถม', 'ชาไทย', 'ไหม', 'จ่าย', 'แก้ว', 'แพง', 'แก้ว', 'สอง', 'ชา', 'ไทย', 'ไม']\n",
      "*85 ['รสชาติ', 'แย่', 'จืด', 'แก้ว', 'โอเค']\n",
      "*95 ['พนักงาน', 'หนา', 'ผม', 'หน้า', 'แก้ว', 'นม']\n",
      "*171 ['แก้ว']\n",
      "*176 ['แก้ว', 'สวย', 'รู้สึก', 'แปลก', 'ดี', 'สี', 'เครื่อง', 'ดื่ม', 'ร้าน']\n",
      "*241 ['เหมือน', 'น้ำ', 'ล้าง', 'แก้ว', 'แถม']\n"
     ]
    }
   ],
   "source": [
    "w3 = Widget(result, 'tokenized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "compare = 0\n",
    "\n",
    "a = numpy.array(onehot_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(onehot_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "\n",
    "print(idx_corpus[seed])\n",
    "print(corpus[seed])\n",
    "print(idx_corpus[compare])\n",
    "print(corpus[compare])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
