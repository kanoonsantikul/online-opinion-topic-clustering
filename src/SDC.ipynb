{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import pythainlp\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from data_tokenizer import load_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents 269\n",
      "1 clusters\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ผู้บริโภค - TescoLotus.txt'\n",
    "\n",
    "corpus, labels = load_corpus('../data/facebook/' + file_name)\n",
    "\n",
    "len_corpus = len(corpus)\n",
    "print('Total documents', len_corpus)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "print(len(clusters), 'clusters')\n",
    "\n",
    "f = open('../data/facebook/tokenized/tokenized_' + file_name)\n",
    "tokenized_corpus = eval(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 1313 words\n",
      "filter frequent words: 540 words\n",
      "filter letter words: 539 words\n",
      "filter stop words: 352 words\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_corpus)\n",
    "print('origin:', len(dictionary), 'words')\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7, keep_n=len(dictionary))\n",
    "print('filter frequent words:', len(dictionary), 'words')\n",
    "\n",
    "letter_words = [id for id in range(len(dictionary)) if len(dictionary[id]) <= 1] \n",
    "dictionary.filter_tokens(bad_ids=letter_words)\n",
    "print('filter letter words:', len(dictionary), 'words')\n",
    "\n",
    "stopwords = pythainlp.corpus.stopwords.words('thai')\n",
    "stopwords.append('นี้')\n",
    "dictionary.add_documents([stopwords])\n",
    "stopwords = [dictionary.token2id[word] for word in stopwords]\n",
    "dictionary.filter_tokens(bad_ids=stopwords)\n",
    "print('filter stop words:', len(dictionary), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "idx_corpus = [dictionary.doc2idx(doc) for doc in tokenized_corpus]\n",
    "\n",
    "temp_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    temp_corpus.append([dictionary[id] for id in doc if id >= 0])\n",
    "idx_corpus = temp_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_doc_size = 0\n",
    "for doc in idx_corpus:\n",
    "    average_doc_size += len(doc)\n",
    "average_doc_size /= len(idx_corpus)\n",
    "average_doc_size = math.ceil(average_doc_size)\n",
    "average_doc_size\n",
    "\n",
    "df = dictionary.dfs\n",
    "filtered_corpus = []\n",
    "for doc in idx_corpus:\n",
    "    new_doc = [(word, df[dictionary.token2id[word]]) for word in doc]\n",
    "    new_doc.sort(reverse=True, key=lambda x: x[1])\n",
    "    new_doc = new_doc[:average_doc_size]\n",
    "    filtered_corpus.append([word for word, df in new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(corpus):\n",
    "    new_dict = Dictionary(corpus)\n",
    "\n",
    "    # new_dict.filter_extremes(no_below=2, no_above=1, keep_n=len(new_dict))\n",
    "    # print(len(new_dict))\n",
    "\n",
    "    unique_words = [new_dict[id] for id in range(len(new_dict))]\n",
    "    array = numpy.zeros((len_corpus, len(unique_words)), dtype=float)\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        for word in doc:\n",
    "            array[i, new_dict.token2id[word]] += 1\n",
    "\n",
    "        ## normalization\n",
    "        if len(doc) != 0:\n",
    "            array[i] = numpy.divide(array[i], len(idx_corpus[i]))\n",
    "\n",
    "    return pandas.DataFrame(array, columns=unique_words, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(bow_corpus, min_samples, eps):\n",
    "    delta_eps = eps / 10\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    cluster_num = 0\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            cluster_num += 1\n",
    "            for p in eps_neighbors:\n",
    "                labels[p] = cluster_num\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            points.remove(seed)\n",
    "\n",
    "    while cluster_num != 0:\n",
    "        cluster = [numpy.array(bow_corpus.iloc[i]) for i, label in enumerate(labels) if label == cluster_num]\n",
    "        eps_temp = eps\n",
    "        \n",
    "        while True:\n",
    "            centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "            eps_temp -= delta_eps\n",
    "            \n",
    "            count = 0\n",
    "            for i, label in enumerate(labels):\n",
    "                point = numpy.array(bow_corpus.iloc[i]).reshape(1, -1)\n",
    "                if label == 0 and cosine_similarity(centroid, point) >= eps_temp:\n",
    "                    cluster.append(point[0])\n",
    "                    labels[i] = cluster_num\n",
    "                    count += 1\n",
    "            if count == 0:\n",
    "                break\n",
    "        \n",
    "        cluster_num -= 1\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_sdc(bow_corpus, min_samples, eps):\n",
    "    labels = [-1 for i in range(len(bow_corpus))]\n",
    "    initials = []\n",
    "    delta_eps = eps / 10\n",
    "    sims = cosine_similarity(bow_corpus)\n",
    "    \n",
    "    points = [i for i in range(len(bow_corpus))]\n",
    "    clusters = []\n",
    "    cluster_num = 0\n",
    "    clusters.append([])\n",
    "    while len(points) > 0:\n",
    "        seed = random.choice(points)\n",
    "        eps_neighbors = [i for i, sim in enumerate(sims[seed]) if sim >= eps and labels[i] == -1]\n",
    "        if len(eps_neighbors) >= min_samples:\n",
    "            print(seed)\n",
    "            cluster_num += 1\n",
    "            clusters.append([])\n",
    "            for p in eps_neighbors:\n",
    "                initials.append(p)\n",
    "                labels[p] = cluster_num\n",
    "                clusters[cluster_num].append(numpy.array(bow_corpus.iloc[p]))\n",
    "            points = [i for i in points if i not in eps_neighbors]\n",
    "        else:\n",
    "            labels[seed] = 0\n",
    "            clusters[0].append((seed, numpy.array(bow_corpus.iloc[seed])))\n",
    "            points.remove(seed)\n",
    "            \n",
    "    expandable = numpy.zeros(cluster_num + 1)    \n",
    "    while numpy.sum(expandable) != -cluster_num - 1:\n",
    "        eps -= delta_eps\n",
    "        count = numpy.zeros(cluster_num + 1)\n",
    "        for point in clusters[0]:\n",
    "            if labels[point[0]] != 0:\n",
    "                continue\n",
    "\n",
    "            num = point[0]\n",
    "            p = point[1].reshape(1, -1)\n",
    "            sim = 0\n",
    "            for c, cluster in enumerate(clusters[1:]):\n",
    "                if expandable[c + 1] == -1:\n",
    "                    continue\n",
    "                centroid = numpy.mean(cluster, axis=0).reshape(1, -1)\n",
    "                if cosine_similarity(centroid, p) >= sim:\n",
    "                    sim = cosine_similarity(centroid, p)\n",
    "                    if sim >= eps:\n",
    "                        labels[num] = c + 1\n",
    "\n",
    "            if labels[num] != 0:\n",
    "                count[labels[num]] += 1\n",
    "                clusters[labels[num]].append(point[1])\n",
    "\n",
    "        for i, num in enumerate(count):\n",
    "            if num == 0:\n",
    "                expandable[i] = -1\n",
    "        \n",
    "    return labels, initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster(bow_corpus, min_samples, eps):\n",
    "    predicted_labels, initials = upgrade_sdc(bow_corpus, min_samples, eps)\n",
    "\n",
    "    result = pandas.DataFrame()\n",
    "    result['comment'] = corpus\n",
    "    result['tokenized_comment'] = filtered_corpus\n",
    "    result['label'] = labels\n",
    "    result['predicted_label'] = predicted_labels\n",
    "    \n",
    "    return result, initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "52\n",
      "30\n",
      "227\n",
      "48\n",
      "9\n",
      "187\n",
      "27\n",
      "13\n",
      "83\n",
      "213\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ทำ</th>\n",
       "      <th>พนักงาน</th>\n",
       "      <th>ข้าว</th>\n",
       "      <th>ดี</th>\n",
       "      <th>น้ำ</th>\n",
       "      <th>อี</th>\n",
       "      <th>เครื่อง</th>\n",
       "      <th>เลิก</th>\n",
       "      <th>กก</th>\n",
       "      <th>งง</th>\n",
       "      <th>...</th>\n",
       "      <th>พลังงาน</th>\n",
       "      <th>คอย</th>\n",
       "      <th>แอร์ไม่</th>\n",
       "      <th>สลิป</th>\n",
       "      <th>คู</th>\n",
       "      <th>ปอง</th>\n",
       "      <th>นวมินทร์</th>\n",
       "      <th>สิบ</th>\n",
       "      <th>ห้า</th>\n",
       "      <th>แจก</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ทำ   พนักงาน      ข้าว        ดี       น้ำ        อี   เครื่อง  \\\n",
       "0  0.333333  0.666667  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.166667  0.166667  0.166667  0.166667  0.166667   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.111111  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.111111  0.111111  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       เลิก        กก        งง ...   พลังงาน  คอย  แอร์ไม่  สลิป   คู  ปอง  \\\n",
       "0  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "1  0.166667  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "2  0.000000  0.111111  0.111111 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "3  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "4  0.000000  0.000000  0.000000 ...       0.0  0.0      0.0   0.0  0.0  0.0   \n",
       "\n",
       "   นวมินทร์  สิบ  ห้า  แจก  \n",
       "0       0.0  0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow_corpus = get_bow(idx_corpus)\n",
    "bow_corpus = get_bow(filtered_corpus)\n",
    "result, initials = predict_cluster(bow_corpus, 7, 0.3)\n",
    "bow_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([49, 20, 41, 11, 14, 28, 23, 29, 17, 18, 12,  7]))\n",
      "\t\tpercent\n",
      "0  |\t49\t1.0\n"
     ]
    }
   ],
   "source": [
    "label_count = numpy.unique(result['predicted_label'], return_counts=True) \n",
    "num_cluster = label_count[0][-1] + 1\n",
    "print(label_count)\n",
    "\n",
    "clusters = list(set(labels))\n",
    "for cluster in clusters:\n",
    "    print('\\t' + cluster, end='')\n",
    "print('\\tpercent')\n",
    "\n",
    "for label in range(len(clusters)):\n",
    "    print(str(label) + '  |', end='')\n",
    "    \n",
    "    num_max = 0\n",
    "    for cluster in clusters:\n",
    "        loc = result[(result['label'] == cluster) & (result['predicted_label'] == label)]\n",
    "        if len(loc) > num_max:\n",
    "            num_max = len(loc)\n",
    "        print('\\t' + str(len(loc)), end='')\n",
    "    \n",
    "    print('\\t' + str(num_max / label_count[1][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIntra cluster sim\tInter cluster sim\tIntra / Inter\n",
      "0\t0.18976292231398123\t0.3077350697865628\t0.6166437983347038\n",
      "\n",
      "1\t0.5719088054130818\t0.4661529459794041\t1.2268694434859375\n",
      "\n",
      "2\t0.5998779276320713\t0.7565704836939566\t0.7928910003244725\n",
      "\n",
      "3\t0.6091824129700055\t0.42081422393142054\t1.4476279040161035\n",
      "\n",
      "4\t0.599985469966392\t0.2571322815767566\t2.3333727927401067\n",
      "\n",
      "5\t0.47285481351699\t0.6572732240362573\t0.719418951244096\n",
      "\n",
      "6\t0.536163626756667\t0.6819793568043518\t0.7861874724024573\n",
      "\n",
      "7\t0.6420237248737604\t0.4398735723635048\t1.4595642139264549\n",
      "\n",
      "8\t0.5558531295636603\t0.33108369796106113\t1.6788900600869643\n",
      "\n",
      "9\t0.45710217840787926\t0.4585892053968852\t0.996757387719759\n",
      "\n",
      "10\t0.5577752498950477\t0.66456528593189\t0.8393084347054097\n",
      "\n",
      "11\t0.5767160558023446\t0.4410188201198887\t1.3076903512769984\n",
      "\n",
      "Summation 133.03040147526033\n"
     ]
    }
   ],
   "source": [
    "clusters = [[] for i in range(num_cluster)]\n",
    "corpus_centroid = []\n",
    "for i, label in result['predicted_label'].iteritems():\n",
    "    clusters[label].append(numpy.array(bow_corpus.iloc[i]))\n",
    "    corpus_centroid.append(numpy.array(bow_corpus.iloc[i]))\n",
    "corpus_centroid = numpy.mean(corpus_centroid, axis=0).reshape(1, -1)   \n",
    "\n",
    "print('\\tIntra cluster sim\\tInter cluster sim\\tIntra / Inter')\n",
    "summation = 0\n",
    "for i in range(num_cluster):\n",
    "    size = len(clusters[i])\n",
    "    if size != 0:\n",
    "        print(i, end='\\t')\n",
    "        centroid = numpy.mean(clusters[i], axis=0).reshape(1, -1)\n",
    "        similarities = cosine_similarity(centroid, clusters[i])\n",
    "        pairwises = cosine_similarity(clusters[i])\n",
    "        intra = numpy.sum(similarities) / size\n",
    "        inter = cosine_similarity(centroid, corpus_centroid)[0][0]\n",
    "        print(intra, end='\\t')\n",
    "        print(inter, end='\\t')\n",
    "        print(intra / inter)\n",
    "        summation += numpy.sum(similarities)\n",
    "    print()\n",
    "print('Summation', summation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeccb0a46a504be985443f1bf52ecfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 เลิกเปิดเพลง ข้าวแสนดี กับอีเครื่องกรองน้ำเพียว ได้แล้ว\n",
      "5 อย่าบังคับน้องเข้าประชุมเชียร์ อย่าลงโทษโดยเหตุผลงี่เง่าๆ เกิดก่อนไม่กี่ปีเอง\n",
      "20 เช็คเอาท์  หน้าจะงอไปไหนสวัสดีค่ะมีอะไรให้ช่วยมั้ยคะ\n",
      "26 โลตลาด พนงปากตลาด นินทาลคเผาขน ตะโกนโหวกเหวกข้ามหัวลค วันหวยออกสนั่นเป็นพิเศษ เห็นพฤติกรรมแล้วแย่ ปรับปรุงภาพลักษณ์เถอะ\n",
      "32 สาขายโสธรเปิดใหม่ พนงแอบโกง   ลูกค้ากุเอง จ่ายหลายใบแล้วทำเนียนว่ามีใบนึงชำรุดใช้ไม่ได้แล้วพอขอคืนก็สลับใบที่รูดใช้แล้วมาแทน แจ้งผู้จัดการขอดูกล้องวงจรปิดไม่ยอมให้ดูเลยต้องขู่ว่าจะแจ้งความถึงยอมคืนให้ แต่ไม่มีคำขอโทษใดๆ จาก พนงติด\n",
      "57 ของป้ายเหลืองพวกผัก เน่าจนไม่รู้จะเน่ายังไง คือทิ้งๆไปก็ได้มั่ง\n",
      "59 กลิ่นของห้างค่ะ ที่สาสขาบางซื่อ คือ มันทั้งอาหาร ความชื้น ฝุ่น  แอร์สกปรกๆๆรวมกัน\n",
      "79 เลิกก๊อปปี้สินค้าแบรนด์ แล้วทำคุณภาพห่วยๆ แม่งเล่นเลย์ซะเหมือนเลย สาดดด\n",
      "91 โลตัส รัตนาธิเบศร์ แอร์เหม็นมาก เหม็นสาบหนูสุดๆ ตรงโซนอาหารแห้ง บะหมี่กึ่งสำเร็จรูป เดินผ่านก็รู้เลยมีหนู ตรงโซนข้าวสารอีก\n",
      "102 ไม่รุ้ทำไมเป้นแบบนี้แย่เรื่องมารยาทจริงๆไม่ได้เท่าขี้เล็บ7เลย\n",
      "104 ของเซลคือเซลเเต่หมดอายุมาหลายเดือนเเล้วยังเซลได้ด้วยหรอของกินไงบางที\n",
      "112 วันใหนมาตั้งใจมาชื้อหมู หมูหมดเหลือไก่  วันใหนชื้อไก่ไก่หมดได้หมู  ตั้งใจอย่างได้อีกอย่าง\n",
      "123 ถ้ามึงไม่อยากไหว้ ไม่ต้องยกมือไหว้ก้อได้ กูรู้มึงโดนบังคับ ให้ไหว้คนที่ไม่ใช่พ่อใช่แม่ เยสเข้\n",
      "131 เอ็กเพรสนี้แลกของแล้วไม่เคยจะโทรบอกเลยหรืออะไรเลยต้องได้ทวง ชอบไม่ให้แสตมป์ เป็นไรมากไหมนี้\n",
      "133 พื้นที่โซนอาหารสดถูบ่อยๆ เถอะค่ะ พื้นเปียกมันทำให้ลื่นนะคะ แล้วมันมีรอยเท้ารอยโคลนลากไปลากมา ดูสกปรกแท้\n",
      "140 เครื่องที่ให้แสกนของเองน่ะ อยากให้ปรับตำแหน่งให้มันแสกนจากมือถือได้ด้วยค่ะ บัตรหายไปแล้วแสกนจากมือถือไม่ได้มันติด\n",
      "156 เทสโก้ โลตัส สุขุมวิท50 เมื่อไม่กี่วันก่อน หน้าเชสเตอร์ชั้น1 น้ำรั่วลงมาจากช่องแอร์ เหม็นมากแอร์ก็ไม่เย็น\n",
      "163 ไปทีไรไม่เห็นมีดอกบัวขาย\n",
      "164 อาหารฟู้ดคอร์ทรสชาติแย่มาก แถมแพงด้วย\n",
      "169 ใช้เครื่องรูดไม่เป็น ไม่รู้จัก\n",
      "171 คืออ่านจนเหนื่อยค่ะ\n",
      "177 อาหารไม่ร่อย\n",
      "178 ไม่จำเป็นไม่เข้าจริงๆ\n",
      "181 เลิกเก็บกระเป๋าตูสักที5555555\n",
      "182 แมลงสาปให้มันน้อยๆหน่อย\n",
      "192 เส้นสีเขียวยังใช้อยู่ไหม\n",
      "194 เคยใช้บริการที่โลตัสอรัญประเทศ โดยรวมแล้วดีครับ\n",
      "201 แผนกอาหารสด กลิ่นแรงมากๆ แทบไม่อยากเดินผ่านเลย\n",
      "205 กลิ่น\n",
      "207 อยากให้คิดตัวเร็วๆหน่อยต่อคิวรอจ่ายตังกันยาวชิยหาย\n",
      "219 หนูวิ่งเป็นของสด\n",
      "223 โปรโมชั่นกำกวมมากกกก\n",
      "224 มาตรฐานพลังงานต่ำมากถ้าเทียบกับเซเว่นและบิ๊กซี\n",
      "225 อยากให้ห้องนํ้ามีนํ้า\n",
      "234 แอร์ไม่เย็น\n",
      "239 คูปองไม่ต้องส่งมาที่บ้าน อยากได้ในบิลเลย\n",
      "250 แล้วแต่ที่จริงๆ\n",
      "252 เอกเพรส ซทานสัมฤทธิ์ หน้าหงิกเป็นเล็บขบ\n",
      "255 แอร์ไม่เปิด\n",
      "267 มารยาทของแคชเชียร์\n"
     ]
    }
   ],
   "source": [
    "comment_widget = widgets.ToggleButtons(\n",
    "    options=[num for num in range(num_cluster)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_comment_widget_click(change):\n",
    "    clear_output()\n",
    "    display(comment_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['comment'].iteritems():\n",
    "        if index in initials:\n",
    "            print(\"*\", end=\"\")\n",
    "        print(index, value)\n",
    "\n",
    "comment_widget.observe(on_comment_widget_click, names='index')\n",
    "on_comment_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f31c26aafab4af3b7dd81fac463e8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['ดี', 'เลิก', 'น้ำ', 'เครื่อง', 'ข้าว', 'อี']\n",
      "5 ['อย่า', 'อย่า', 'กี่', 'ปี', 'บังคับ', 'น้อง', 'เชียร์', 'งี่', 'เง่า']\n",
      "20 ['หน้า', 'เช็ค', 'งอ', 'สวัสดี']\n",
      "26 ['ปรับปรุง', 'พนง', 'แย่', 'ตะโกน', 'ข้าม', 'หวย', 'พิเศษ', 'พฤติกรรม']\n",
      "32 ['ลูกค้า', 'จ่าย', 'พนง', 'พนง', 'ใบ', 'ใบ', 'ใบ', 'ติด', 'นึง']\n",
      "57 ['รู้', 'ผัก', 'เน่า', 'เน่า', 'ทิ้ง', 'ป้ายเหลือง']\n",
      "59 ['ห้าง', 'อาหาร', 'กลิ่น', 'ฝุ่น']\n",
      "79 ['ทำ', 'เหมือน', 'สินค้า', 'เลิก', 'แม่ง', 'ห่วย', 'คุณภาพ']\n",
      "91 ['เดิน', 'รู้', 'อาหาร', 'เหม็น', 'ข้าว', 'หนู', 'หนู', 'โซน', 'แห้ง']\n",
      "102 ['เรื่อง', 'มารยาท', 'แย่', 'ขี้', 'เล็บ']\n",
      "104 ['กิน', 'เดือน', 'หรอ', 'เเต่', 'อายุ', 'เเล้ว']\n",
      "112 ['หมู', 'หมู', 'หมู', 'ไก่', 'ไก่', 'ไก่', 'ใหน', 'ตั้งใจ', 'ชื้อ']\n",
      "123 ['คน', 'รู้', 'โดน', 'มือ', 'ก้อ', 'บังคับ', 'ไหว้', 'ไหว้', 'ไหว้']\n",
      "131 ['ชอบ', 'เอ็กเพรส', 'แลก', 'ไหม', 'แสตมป์', 'โทร']\n",
      "133 ['ทำ', 'อาหาร', 'ดู', 'สด', 'สกปรก', 'พื้น', 'พื้นที่', 'โซน', 'ลาก']\n",
      "140 ['ติด', 'บัตร', 'เครื่อง', 'หาย']\n",
      "156 ['หน้า', 'น้ำ', 'เย็น', 'กี่', 'เหม็น', 'ชั้น', 'แอร์']\n",
      "163 ['ขาย', 'ดอก']\n",
      "164 ['แย่', 'อาหาร', 'แถม']\n",
      "169 []\n",
      "171 ['เหนื่อย']\n",
      "177 ['อาหาร']\n",
      "178 []\n",
      "181 ['เลิก', 'สัก', 'ตู']\n",
      "182 []\n",
      "192 ['ไหม', 'สี', 'เขียว']\n",
      "194 ['ดี', 'บริการ']\n",
      "201 ['เดิน', 'อาหาร', 'แทบ', 'สด', 'กลิ่น', 'แรง', 'แผนก']\n",
      "205 ['กลิ่น']\n",
      "207 ['จ่าย', 'รอ', 'ตัว', 'คิว', 'ตัง']\n",
      "219 ['สด', 'วิ่ง', 'หนู']\n",
      "223 ['โปรโมชั่น', 'กกก']\n",
      "224 ['เซเว่น', 'พลังงาน', 'ต่ำ']\n",
      "225 []\n",
      "234 ['เย็น', 'แอร์ไม่']\n",
      "239 ['บ้าน', 'บิล', 'คู', 'ปอง']\n",
      "250 []\n",
      "252 ['หน้า', 'หงิก', 'เอกเพรส', 'เล็บ']\n",
      "255 ['แอร์ไม่']\n",
      "267 ['มารยาท', 'แคชเชียร์']\n"
     ]
    }
   ],
   "source": [
    "token_widget = widgets.ToggleButtons(\n",
    "    options=[num for num in range(num_cluster)],\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    ")\n",
    "\n",
    "def on_token_widget_click(change):\n",
    "    clear_output()\n",
    "    display(token_widget)\n",
    "    for index, value in result[result['predicted_label'] == change['new']]['tokenized_comment'].iteritems():\n",
    "        print(index, value)\n",
    "\n",
    "token_widget.observe(on_token_widget_click, names='index')\n",
    "on_token_widget_click({'new' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15811388]]\n",
      "['สาขา', 'ลูกค้า', 'โลตัส', 'ถุง', 'นวมินทร์']\n",
      "['ถุง', 'ซื้อ', 'เดิน', 'ซ้อน', 'ผ้า', 'นุ่ม', 'เชียร์', 'ถุงขาด']\n"
     ]
    }
   ],
   "source": [
    "seed = 244\n",
    "compare = 89\n",
    "\n",
    "a = numpy.array(bow_corpus.iloc[seed]).reshape(1, -1)\n",
    "b = numpy.array(bow_corpus.iloc[compare]).reshape(1, -1)\n",
    "print(cosine_similarity(a,b))\n",
    "print(filtered_corpus[seed])\n",
    "print(filtered_corpus[compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior-project",
   "language": "python",
   "name": "senior-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
